Model,Domain,Task,Organization,Authors,Publication date,Reference,Link,Citations,Notability criteria,Notability criteria notes,Parameters,Parameters notes,Training compute (FLOP),Training compute notes,Training dataset,Training dataset notes,Training dataset size (datapoints),Dataset size notes,Epochs,Training time (hours),Training time notes,Training hardware,Hardware quantity,Compute cost notes,Compute sponsor categorization,Abstract,Model accessibility,Hardware utilization,Confidence,Country (from Organization),Base model,Finetune compute (FLOP),Finetune compute notes,Batch size,Batch size notes,Organization categorization,Training code accessibility,Accessibility notes,Organization categorization (from Organization),Possibly over 1e23 FLOP,Frontier model,Training power draw (W),Training compute estimation method
DNCON2,Biology,"Proteins,Protein folding prediction",University of Missouri,"Badri Adhikari, Jie Hou, Jianlin Cheng",2018-05-01,DNCON2: improved protein contact prediction using two-level deep convolutional neural networks,https://academic.oup.com/bioinformatics/article/34/9/1466/4708303?login=false,173.00,,,,,95000000000000000.00,"""Our training was conducted on Tesla K20 Nvidia GPUs each having 5 GB of GPU memory, on which, training one model took around 12 h.""

""We train each CNN for a total of 1600 epochs with each epoch of training taking around 2 min.""

Assumptions:
peakFLOP rate 3.52e12FLOP/s (from: https://www.techpowerup.com/gpu-specs/tesla-k20c.c564)
30% utilization rate
1 GPU

Estimate 1: ""training one model took around 12h"" => unclear how many GPUs
(12 *3600) s * 3.52e12 FLOP/s * 0.3 = 4.5e16 FLOP

Estimate 2: ""We train each CNN for a total of 1600 epochs with each epoch of training taking around 2 min.""
(1600 epochs * 2 min/epoch * 60 s/min) * 3.52e12 FLOP/s * 0.3 =  2e17 FLOP

Geometric mean: 9.5e16",PDB (Protein Data Bank),"""We used the original DNCON dataset consisting of 1426 proteins having length between 30 and 300 residues curated before the CASP10 experiment to train and test DNCON2. The protein structures in the dataset were obtained from the Protein Data Bank (PDB)""",1426,"""Our raw feature files for all 1426 training proteins""",1600.00,12.0,"""training one model took around 12 h""",,,,,"Motivation
Significant improvements in the prediction of protein residue–residue contacts are observed in the recent years. These contacts, predicted using a variety of coevolution-based and machine learning methods, are the key contributors to the recent progress in ab initio protein structure prediction, as demonstrated in the recent CASP experiments. Continuing the development of new methods to reliably predict contact maps is essential to further improve ab initio structure prediction.

Results
In this paper we discuss DNCON2, an improved protein contact map predictor based on two-level deep convolutional neural networks. It consists of six convolutional neural networks—the first five predict contacts at 6, 7.5, 8, 8.5 and 10 Å distance thresholds, and the last one uses these five predictions as additional features to predict final contact maps. On the free-modeling datasets in CASP10, 11 and 12 experiments, DNCON2 achieves mean precisions of 35, 50 and 53.4%, respectively, higher than 30.6% by MetaPSICOV on CASP10 dataset, 34% by MetaPSICOV on CASP11 dataset and 46.3% by Raptor-X on CASP12 dataset, when top L/5 long-range contacts are evaluated. We attribute the improved performance of DNCON2 to the inclusion of short- and medium-range contacts into training, two-level approach to prediction, use of the state-of-the-art optimization and activation functions, and a novel deep learning architecture that allows each filter in a convolutional layer to access all the input features of a protein of arbitrary length.",Open weights (unrestricted),,Likely,United States of America,,,,,,Academia,Open source,license: https://github.com/multicom-toolbox/DNCON2?tab=GPL-3.0-1-ov-file#readme,Academia,,,,Hardware
DNABERT,Biology,Protein or nucleotide language model (pLM/nLM),Northeastern University,"Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri",2021-08-15,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,https://academic.oup.com/bioinformatics/article/37/15/2112/6128680,479.00,SOTA improvement,"""We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data."" [Abstract] - SOTA improvement on very specific task",110000000.00,"""We used the same model architecture as the BERT base, which consists of 12 Transformer layers with 768 hidden units and 12 attention heads in each layer, and the same parameter setting across all the four DNABERT models during pre-training""

Known to have 110 million parameters as reported in: https://arxiv.org/pdf/1810.04805v2.pdf
""We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) [...]""",107000000000000020000.00,"""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP

Alternatively:
""DNABERT takes a sequence with a max length of 512 as input... We pre-trained DNABERT for 120k steps with a batch size of 2000""
6 * 512 * 2000 * 120k * 110M = 8.11e19

Geometric mean: 1.07e20",Human Reference Genome (GRCh38/hg38),"""We generated training data from human genome [...]"" [2.2.2 Pre-training]. ",3000000000,"The human genome is around 3 billion base pairs (https://useast.ensembl.org/Homo_sapiens/Info/Annotation).
The authors use both non-overlapping sampling and random sampling from a human genome, though the source is unspecified.",4.04,600.0,"""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""",NVIDIA GeForce RTX 2080 Ti,,,,"Motivation
Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.

Results
To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.",Open weights (unrestricted),,Confident,United States of America,,,,,,Academia,Open source,"Apache 2.0, code and weights: https://github.com/jerryji1993/DNABERT",Academia,,,,"Hardware,Operation counting"
ProteinBERT,Biology,"Proteins,Protein generation","Hebrew University of Jerusalem,Ben-Gurion University of the Negev,Deep Trading","Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, Michal Linial",2022-02-10,ProteinBERT: a universal deep-learning model of protein sequence and function,https://academic.oup.com/bioinformatics/article/38/8/2102/6502274,386.00,SOTA improvement,"""ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes)""",16000000.00,"""Altogether, it includes ∼16M trainable parameters, making it substantially smaller than other protein language models""",65000000000000000000.00,"""Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28 days over ∼670M records""

28 * 24 * 3600 * 89 TFLOP/s * 0.3 (assumed utilization) = 6.5e19
https://www.wolframalpha.com/input?i=28+days+*+89+TFLOP%2Fs+*+0.3",UniRef90,,32000000001,"Number of proteins: 106,000,000
Average protein length: 300 amino acids

Total unique tokens = 106,000,000 × 300 = 31,800,000,000 ≈ 3.2e10 tokens",6.40,672.0,28 days,NVIDIA Quadro RTX 5000,1,,,"Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes), despite using a far smaller and faster model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data.",,,Confident,"Israel,United States of America,United States of America",,,,26008,"Supplementary materials: ""During pretraining we used batch sizes of 128, 64 or 32 for episodes of 128, 512 or 1,024 tokens, respectively"" Since they seem to be used in equal parts, taking geometric mean: ((128*128)*(64*512)*(32*1024))**(1/3) = 26,008","Academia,Academia,Industry",,,"Academia,Academia,Industry",,,511.39675520829087,Hardware
DistilProtBert,Biology,"Proteins,Protein folding prediction",Bar-Ilan University,"Yaron Geffen, Yanay Ofran, Ron Unger",2022-09-18,DistilProtBert: a distilled protein language model used to distinguish between real proteins and their randomly shuffled counterparts,https://academic.oup.com/bioinformatics/article/38/Supplement_2/ii95/6701995,23.00,,,230000000.00,"""we were able to reduce the number of DistilProtBert parameters by almost half, to 230 M""",190000000000000030000.00,"""Pretraining was done on five v100 32-GB Nvidia GPUs from a DGX
cluster with a local batch size of 16 examples... The model was trained for three epochs using mixed precision and dynamic padding. Every epoch run took approximately 4 days, resulting in total pretraining time of 12 days""

5 * 125 teraFLOP/s * 12 * 24 * 3600 * 0.3 (assumed utilization) = 1.9e20",UniRef50,"""DistilProtBert was pretrained on 43 M sequences from UniRef50 with length ranging from 20 to 512 amino acids""",13000000001,"43,000,000 sequences × 300 amino acids = 12,900,000,000 (1.29e10) data points

Calculation:
43,000,000 × 300 = 12,900,000,000",3.00,288.0,12 days,NVIDIA Tesla V100 DGXS 32 GB,,,Academia,"Recently, deep learning models, initially developed in the field of natural language processing (NLP), were applied successfully to analyze protein sequences. A major drawback of these models is their size in terms of the number of parameters needed to be fitted and the amount of computational resources they require. Recently, 'distilled' models using the concept of student and teacher networks have been widely used in NLP. Here, we adapted this concept to the problem of protein sequence analysis, by developing DistilProtBert, a distilled version of the successful ProtBert model. Implementing this approach, we reduced the size of the network and the running time by 50%, and the computational resources needed for pretraining by 98% relative to ProtBert model. Using two published tasks, we showed that the performance of the distilled model approaches that of the full model. We next tested the ability of DistilProtBert to distinguish between real and random protein sequences. The task is highly challenging if the composition is maintained on the level of singlet, doublet and triplet amino acids. Indeed, traditional machine-learning algorithms have difficulties with this task. Here, we show that DistilProtBert preforms very well on singlet, doublet and even triplet-shuffled versions of the human proteome, with AUC of 0.92, 0.91 and 0.87, respectively. Finally, we suggest that by examining the small number of false-positive classifications (i.e. shuffled sequences classified as proteins by DistilProtBert), we may be able to identify de novo potential natural-like proteins based on random shuffling of amino acid sequences.",,,Confident,Israel,,,,,,Academia,,,Academia,,,,Hardware
BERT-RBP,Biology,"Proteins,Protein interaction prediction",Waseda University,"Keisuke Yamada, Michiaki Hamada",2022-04-07,Prediction of RNA–protein interactions using a nucleotide language model,https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac023/6564689,36.00,SOTA improvement,"""Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs"" [Abstract] - SOTA improvement on a very specific task",110000000.00,"Base model is BERT base (110M parameters), pre-trained on human reference genome (DNABert: https://academic.oup.com/bioinformatics/article/37/15/2112/6128680)",140000000000000000000.00,"See DNABert entry:

""Since the pre-training of DNABERT model is resource-intensive (about 25 days on 8 NVIDIA 2080Ti GPUs)""

Assuming FP16 and 30% utilization

Calculation = (25 * 24 *3600) s * 2.7e13 FLOP/s per GPU * 8 GPUs * 0.3 utilization = 1.4e20 FLOP",RBPSuite,"See DNABert entry: ""We generated training data from human genome [...]"" [2.2.2 Pre-training]

""An eCLIP-seq dataset previously generated from the ENCODE3 database by Pan et al. (2020) was used. The original dataset consisted of 154 RBP sets with up to 60 000 positive RNA sequences that bind to the corresponding RBP and the same number of negative sequences."" [2.2 Data preparation]",78000001,"RBPs: 154
Sequences per RBP: 15,000
Total sequences = 154 × 15,000 = 2,310,000
Tokens per sequence = 34
Total tokens = 2,310,000 × 34 = 78,540,000
Final estimate: 7.8 × 10⁷ tokens",3.00,,,,,,,"Motivation
The accumulation of sequencing data has enabled researchers to predict the interactions between RNA sequences and RNA-binding proteins (RBPs) using novel machine learning techniques. However, existing models are often difficult to interpret and require additional information to sequences. Bidirectional encoder representations from transformer (BERT) is a language-based deep learning model that is highly interpretable. Therefore, a model based on BERT architecture can potentially overcome such limitations.

Results
Here, we propose BERT-RBP as a model to predict RNA–RBP interactions by adapting the BERT architecture pretrained on a human reference genome. Our model outperformed state-of-the-art prediction models using the eCLIP-seq data of 154 RBPs. The detailed analysis further revealed that BERT-RBP could recognize both the transcript region type and RNA secondary structure only based on sequence information. Overall, the results provide insights into the fine-tuning mechanism of BERT in biological contexts and provide evidence of the applicability of the model to other RNA-related problems.",Open weights (non-commercial),,Confident,Japan,DNABERT,22000000000000000,"""The models were trained on four NVIDIA Tesla V100 GPUs (128
GB memory). The training of one RBP model using 19 200 samples
took <10 min.""

Calculation assuming FP16 and 30% utlization and NVIDIA Tesla V100 SMX2 model: 
10 min * 60 sec/min * 3.1e13 FLOP/s * 4 GPU * 0.3 utilization = 2.2e16",,,Academia,Open (non-commercial),"No clear license: https://github.com/kkyamada/bert-rbp

train script: https://github.com/kkyamada/bert-rbp/blob/master/examples/run_finetune.py 

data also doesn't have a clear license: http://www.csbio.sjtu.edu.cn/bioinf/RBPsuite/dataset_new.html",Academia,,,,Hardware
AbLang (heavy sequences),Biology,Proteins,University of Oxford,"Tobias H Olsen, Iain H Moal, Charlotte M Deane",2022-01-22,AbLang: an antibody language model for completing antibody sequences,https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac046/6609807,99.00,SOTA improvement,"""AbLang restores residues more accurately and faster than a current state-of-the-art protein language model ESM-1b, emphasizing the benefits and potential of an antibody specific language model"" - SOTA improvement for a very specific task",355000000.00,"""The hyperparameters were selected to be similar to those used
in the RoBERTa paper (Liu et al., 2019).""

Liu et al., 2019 link: https://arxiv.org/pdf/1907.11692.pdf
""We begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters)""",,,Observed Antibody Space (OAS) database,,2290000001,"Heavy Chain: 14,126,724 sequences × 160 residues = 2,260,275,840 datapoints
Light Chain: 187,068 sequences × 160 residues = 29,930,880 datapoints
Total: 2,260,275,840 + 29,930,880 = 2,290,206,720 datapoints (2.29B)",20.00,,,,,,,"Motivation
General protein language models have been shown to summarize the semantics of protein sequences into representations that are useful for state-of-the-art predictive methods. However, for antibody specific problems, such as restoring residues lost due to sequencing errors, a model trained solely on antibodies may be more powerful. Antibodies are one of the few protein types where the volume of sequence data needed for such language models is available, e.g. in the Observed Antibody Space (OAS) database.

Results
Here, we introduce AbLang, a language model trained on the antibody sequences in the OAS database. We demonstrate the power of AbLang by using it to restore missing residues in antibody sequence data, a key issue with B-cell receptor repertoire sequencing, e.g. over 40% of OAS sequences are missing the first 15 amino acids. AbLang restores the missing residues of antibody sequences better than using IMGT germlines or the general protein language model ESM-1b. Further, AbLang does not require knowledge of the germline of the antibody and is seven times faster than ESM-1b.",,,Confident,United Kingdom of Great Britain and Northern Ireland,,,,,,Academia,,,Academia,,,,
Evo,Biology,Protein or nucleotide language model (pLM/nLM),"Stanford University,University of California (UC) Berkeley,Together","Eric Nguyen, Michael Poli, Matthew G. Durrant, Armin W. Thomas, Brian Kang, Jeremy Sullivan, Madelena Y. Ng, Ashley Lewis, Aman Patel, Aaron Lou, Stefano Ermon, Stephen A. Baccus, Tina Hernandez-Boussard, Christopher Ré, Patrick D. Hsu, Brian L. Hie",2024-02-27,Sequence modeling and design from molecular to genome scale with Evo,"https://arcinstitute.org/news/blog/evo
https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1",44.00,,Competitive with SOTA protein language models,7000000000.00,"Based on a StripedHyena architecture, context length of 131 kilobases",2.0000000001e+22,"""In total, Evo was trained on approximately 340B tokens, using approximately 2e22 FLOPS""",OpenGenome,"""We're also open-sourcing a large 300B token training dataset we compiled, which we call OpenGenome, consisting of 2.7M publicly available prokaryotic and phage genomes""",300000000000,"300 billion nucleotides. Since these are tokenized at the single-nucleotide level, 300B tokens.",1.00,2968.0,"Trained for first stage on 64 H100s, then for second stage on 128 A100s. Trained primarily with BF16 precision; FP32 used for ""long convolutional parameters"". I use BF16 performance for simplicity.

This gives a plausible range for completing 2e22 FLOPs.

If all 2e22 FLOPs were performed on H100s:
2e22 FLOP / (64 GPUs * 1.3e14 FLOP/GPU-sec) * 1/0.3 efficiency *  1/3600 hour/sec  = 2226 hours

If all 2e22 FLOPs were performed on A100s:
2e22 FLOP / (128 GPUs * 3.9e13 FLOP/GPU-sec) * 1/0.3 efficiency * 1/3600 hour/sec = 3710 hours

Assume each stage is roughly half the total time, so:
(2226+3710)/2 = 2968 hours","NVIDIA A100,NVIDIA H100 SXM5 80GB",,"""We trained Evo in stage 1 on 64 Nvidia H100 GPUs and on 128 Nvidia A100 GPUs in stage 2. In total, Evo was trained on approximately 340B tokens, using approximately 2e22 FLOPS""

This gives a plausible range for completing 2e22 FLOPs.

If all 2e22 FLOPs were performed on H100s:
2e22 FLOP / (1.3e14 FLOP/GPU-sec) * 1/0.3 efficiency *  1/3600 hour/sec * $1.69/GPU-hour = $240,741

If all 2e22 FLOPs were performed on A100s:
2e22 FLOP / (3.9e13 FLOP/GPU-sec) * 1/0.3 efficiency * 1/3600 hour/sec * $0.93/GPU-hour = $441,595

Assume each stage is roughly half the total compute, so:
(240741+441595)/2 = $341,168",Industry,"The genome is a sequence that completely encodes the DNA, RNA, and proteins that orchestrate the function of a whole organism. Advances in machine learning combined with massive datasets of whole genomes could enable a biological foundation model that accelerates the mechanistic understanding and generative design of complex molecular interactions. We report Evo, a genomic foundation model that enables prediction and generation tasks from the molecular to genome scale. Using an architecture based on advances in deep signal processing, we scale Evo to 7 billion parameters with a context length of 131 kilobases (kb) at single-nucleotide, byte resolution. Trained on whole prokaryotic genomes, Evo can generalize across the three fundamental modalities of the central dogma of molecular biology to perform zero-shot function prediction that is competitive with, or outperforms, leading domain-specific language models. Evo also excels at multielement generation tasks, which we demonstrate by generating synthetic CRISPR-Cas molecular complexes and entire transposable systems for the first time. Using information learned over whole genomes, Evo can also predict gene essentiality at nucleotide resolution and can generate coding-rich sequences up to 650 kb in length, orders of magnitude longer than previous methods. Advances in multi-modal and multi-scale learning with Evo provides a promising path toward improving our understanding and control of biology across multiple levels of complexity.",Open weights (unrestricted),,Confident,"United States of America,United States of America,United States of America",,,,,,"Academia,Academia,Industry",,Apache License 2.0,"Academia,Academia,Industry",,,,Reported
TAPE Transformer,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","University of California (UC) Berkeley,Covariant,Google,Chan Zuckerberg Initiative","Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi Chen, John Canny, Pieter Abbeel, Yun S. Song",2019-06-19,Evaluating Protein Transfer Learning with TAPE,https://arxiv.org/abs/1906.08230,697.00,,,38000000.00,"""We use a 12-layer Transformer with a hidden size of 512 units and 8 attention heads, leading to a 38M-parameter model""",30000000000000000000.00,"""All self-supervised models are trained on four NVIDIA V100 GPUs for one week""

(7 * 24 * 3600) s * 4 GPUs * 3.1e13 FLOP/s * 0.4 (utilization assumption) = 3e19",Pfam,"""We use Pfam [33], a database of thirty-one million protein domains used extensively in bioinformatics, as the pretraining corpus for TAPE""",9660000001,"N = 32.2 million sequences
L = 300 residues per sequence
Total datapoints = N × L = 32,200,000 × 300 = 9.66e9 residues",,168.0,,NVIDIA V100,4,,,"Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.",Open weights (unrestricted),,Confident,"United States of America,United States of America,United States of America,United States of America",,,,,,"Academia,Industry,Industry,Research collective",Open source,"BSD license. code, data, and weights:
https://github.com/songlab-cal/tape?tab=readme-ov-file#data","Academia,Industry,Industry,Research collective",,,2695.9079931482615,Hardware
Hopfield Networks (2020),"Biology,Vision,Language,Medicine","Drug discovery,Language modeling,Object recognition","Johannes Kepler University Linz,Institute of Advanced Research in Artificial Intelligence,University of Oslo","Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter",2020-07-16,Hopfield Networks is All You Need,https://arxiv.org/abs/2008.02217,345.00,SOTA improvement,"""Hopfield layers yielded a new state-ofthe-art when compared to different machine learning methods. Finally, Hopfield
layers achieved state-of-the-art on two drug design datasets""",,,,,"BACE,SIDER","""We test the Hopfield layer HopfieldLayer, on four drug
design datasets. These datasets represent four main areas of modeling tasks in drug design, concretely
to develop accurate models for predicting a) new anti-virals (HIV) by the Drug Therapeutics Program
(DTP) AIDS Antiviral Screen, b) new protein inhibitors, concretely human β-secretase (BACE) inhibitors by Subramanian et al. (2016), c) metabolic effects as blood-brain barrier permeability (BBBP)
(Martins et al., 2012) and d) side effects of a chemical compound from the Side Effect Resource
(SIDER) Kuhn et al. (2016). """,,,,,,,,,Academia,"We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: this https URL",Open weights (unrestricted),,Unknown,"Austria,Austria,Norway",,,,,,"Academia,Academia,Academia",Unreleased,"copyleft-like license, derivative works must retain this license. code here:
https://github.com/ml-jku/hopfield-layers/blob/master/LICENSE","Academia,Academia,Academia",,,,
Fold2Seq,Biology,"Proteins,Protein generation,Protein inverse folding","IBM,Texas A&M","Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin-Yu Chen, Igor Melnyk, Yang Shen",2021-06-24,Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design,https://arxiv.org/abs/2106.13058,44.00,SOTA improvement,"""On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared to existing state-of-the-art methods that include data-driven deep generative models and physics-based RosettaDesign."" [Abstract]",12427904.00,"Three components, a sequence encoder, fold encoder, and sequence decoder. (1) and (3) have similar architectures, a sequence embedder + transformer block (the encoder uses a transformer encoder, the decoder uses a decoder block). 

The fold encoder adds 6 residual conv blocks and has a 3d positional encoder rather than sequence embedder. Each residual block has two 3D-convolutional layers (3×3×3) and batch normalization layers.

Each transformer block has 4 layers and d = 256 latent dimensions.

Calculations here: https://docs.google.com/document/d/1luTCTQLmfaBfmnjbsBpWuG51x_IwuiN7mqNoOXtoi8Y/edit?usp=sharing",140000000000000000.00,"""We train our model on 2 Tesla K80 GPUs, with batch size 128. In every training stage we train up to 200 epochs with an early stopping strategy based on the validation loss""
See calculations here: https://docs.google.com/document/d/1luTCTQLmfaBfmnjbsBpWuG51x_IwuiN7mqNoOXtoi8Y/edit?usp=sharing

Block 1:
CNN 1: 2*20*20*20*3*3*3*4*8=13824000
CNN 2: 2*20*20*20*3*3*3*8*8=27648000
Block 2:
CNN 1: 2*20*20*20*3*3*3*8*16=55296000
CNN 2: 2*20*20*20*3*3*3*16*16=110592000
Block 3:
CNN 1: 2*20*20*20*3*3*3*16*32=221184000
CNN 2: 2*20*20*20*3*3*3*32*32=442368000
Block 4:
CNN 1: 2*10*10*10*3*3*3*32*64=110592000
CNN 2: 2*10*10*10*3*3*3*64*64=221184000
Block 5:
CNN 1: 2*10*10*10*3*3*3*64*128=442368000
CNN 2: 2*10*10*10*3*3*3*128*128=884736000
Block 6:
CNN 1: 2*10*10*10*3*3*3*128*256=1769472000
CNN 2: 2*10*10*10*3*3*3*256*256=3538944000
CNN forward FLOP: 13824000+27648000+55296000+110592000+221184000+442368000+110592000+221184000+442368000+884736000+1769472000+3538944000=7838208000
Training FLOP: 45995*200*(3299020800+4074393600+7838208000)=1.4e17


Note I make several assumptions about the CNN architecture which could change this value.",CATH,"""We used protein structure data from CATH 4.2""",45995,"""Training set includes 45995 proteins belonging to a total of 971 folds""",200.00,,,NVIDIA Tesla K80,2,,,"Designing novel protein sequences for a desired 3D topological fold is a fundamental yet nontrivial task in protein engineering. Challenges exist due to the complex sequence–fold relationship, as well as the difficulties to capture the diversity of the sequences (therefore structures and functions) within a fold. To overcome these challenges, we propose Fold2Seq, a novel transformer-based
generative framework for designing protein sequences conditioned on a specific target fold. To model the complex sequence–structure relationship, Fold2Seq jointly learns a sequence embedding using a transformer and a fold embedding from the density of secondary structural elements in 3D voxels. On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared to existing state-of-the-art methods that include datadriven deep generative models and physics-based RosettaDesign. The unique advantages of foldbased Fold2Seq, in comparison to a structurebased deep model and RosettaDesign, become more evident on three additional real-world challenges originating from low-quality, incomplete, or ambiguous input structures. Source code and data are available at https://github.com/IBM/fold2seq.",,,Unverified,"United States of America,United States of America",,,,128,,"Industry,Academia",,,"Industry,Academia",,,1337.253704239513,Operation counting
ProteinLM,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Tsinghua University,Beijing Academy of Artificial Intelligence / BAAI,Tencent","Yijia Xiao, Jiezhong Qiu, Ziang Li, Chang-Yu Hsieh, Jie Tang
",2021-08-17,Modeling Protein Using Large-scale Pretrain Language Model,https://arxiv.org/abs/2108.07435,24.00,,,3000000000.00,"""We have trained multiple largescale models on the PFAM[7] dataset, the largest with 3 billion parameters""",1.5999999999999998e+22,"""We pretrained two large models on a 480 GPUs (TeslaV100-32GB) cluster for about three weeks""

21 * 24* 3600 * 480 * 125 teraFLOP/s * 0.3 (utilization) * 0.5 (two models) = 1.6e22",Pfam,"""PFAM[7] is a widely-used database consisting of more than 32 million protein sequences""",9000000001,"Total sequences: 32,000,000
Testing (1%): 32,000,000 × 0.01 = 320,000
Remaining: 32,000,000 - 320,000 = 31,680,000
Training (95%): 31,680,000 × 0.95 = 30,096,000
Final calculation: 30,096,000 sequences × 300 tokens/sequence = 9,028,800,000 tokens",,252.0,,NVIDIA V100,48,,Industry,"Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token- level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",Open weights (unrestricted),,Confident,"China,China,China",,,,,,"Academia,Academia,Industry",Open source,"apache 2.0
https://github.com/THUDM/ProteinLM","Academia,Academia,Industry",,,32076.092304184695,Hardware
EquiDock,Biology,Proteins,"Massachusetts Institute of Technology (MIT),ETH Zurich,Tencent","Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Yatao Bian, Regina Barzilay, Tommi Jaakkola, Andreas Krause",2021-11-15,Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking,https://arxiv.org/abs/2111.07786,133.00,,"""Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates.""

did not have best results, per Table 1",,,10800000000000000000.00,"Training details here:

https://docs.nvidia.com/bionemo-framework/latest/models/equidock.html

32 A100s can do 30 epochs per hour on the DIPS dataset. Equidock was trained on 30 epochs on DIPS and 150 epochs on DB5.5. DIPS is about 100x bigger, so the large majority of compute was DIPS.

32 A100-hours = 312 teraflops * 32 * 3600 * 0.3 
~= 1.08e19","DIPS,DB5.5","""We leverage the following datasets: Docking Benchmark 5.5 (DB5.5) (Vreven et al.,
2015) and Database of Interacting Protein Structures (DIPS) (Townshend et al., 2019). DB5.5 is
a gold standard dataset in terms of data quality, but contains only 253 structures. DIPS is a larger
protein complex structures dataset mined from the Protein Data Bank (Berman et al., 2000) and
tailored for rigid body docking. Datasets information is given in Appendix D""",39938,"DIPS Dataset:
39,937 protein pairs

Total data points = 39,937 = 3.9937e4",30.00,,,,,,,"Protein complex formation is a central problem in biology, being involved in most of the cell's processes, and essential for applications, e.g. drug design or protein engineering. We tackle rigid body protein-protein docking, i.e., computationally predicting the 3D structure of a protein-protein complex from the individual unbound structures, assuming no conformational change within the proteins happens during binding. We design a novel pairwise-independent SE(3)-equivariant graph matching network to predict the rotation and translation to place one of the proteins at the right docked position relative to the second protein. We mathematically guarantee a basic principle: the predicted complex is always identical regardless of the initial locations and orientations of the two structures. Our model, named EquiDock, approximates the binding pockets and predicts the docking poses using keypoint matching and alignment, achieved through optimal transport and a differentiable Kabsch algorithm. Empirically, we achieve significant running time improvements and often outperform existing docking software despite not relying on heavy candidate sampling, structure refinement, or templates.",Open weights (unrestricted),,Likely,"United States of America,Switzerland,China",,,,,,"Academia,Academia,Industry",,,"Academia,Academia,Industry",,,,Hardware
Rita-XLarge,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","LightOn,Harvard University,University of Oxford","Daniel Hesslow, Niccolo Zanichelli, Pascal Notin, Iacopo Poli, Debora Marks ",2022-07-14,RITA: a Study on Scaling Up Generative Protein Sequence Models,https://arxiv.org/abs/2205.05789,70.00,,,1200000000.00,"""with up to 1.2 billion parameters""",864000000000000000000.00,"""The models were trained for a total training time of over 25 thousand Nvidia-V100 GPU hours""

125 teraFLOP/s (uncertain which V100 model, tensor performance varies from 112-130tFLOP/s) * 25000 * 3600 * 0.3 (utilization) = 3.4e+21"" <- the total compute for several models

For the biggest (XL) from Figure 1:
10 PF-days = 10*10^15*24*3600 FLOPs = 8.64e+20 FLOPs <- total compute just for the XL model","UniRef100,MGnify,Metaclust","""We focus on three different pre-training corpora: UniRef100 (The UniProt Consortium, 2020), MGnify (Mitchell et al., 2020) and Metaclust (Steinegger & Soding ¨ , 2018), each providing a sufficient amount of tokens for model pretraining without having to repeat the data.""",300000000001,"150 billion amino acids × 2 (primary + reversed sequences) = 300 billion datapoints
Final estimate: 3.0e11 datapoints",,,,NVIDIA V100,,,,"In this work we introduce RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database. Such generative models hold the promise of greatly accelerating protein design. We conduct the first systematic study of how capabilities evolve with model size for autoregressive transformers in the protein domain: we evaluate RITA models in next amino acid pre- diction, zero-shot fitness, and enzyme function prediction, showing benefits from increased scale. We release the RITA models openly, to the benefit of the research community.",Open weights (unrestricted),,Confident,"France,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,"Industry,Academia,Academia",Unreleased,"MIT license
https://github.com/lightonai/RITA","Industry,Academia,Academia",,,,Reported
Tranception,Biology,"Proteins,Protein pathogenicity prediction","University of Oxford,Harvard Medical School,Cohere","Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena-Hurtado, Aidan Gomez, Debora S. Marks, Yarin Gal",2022-05-27,Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval,https://arxiv.org/abs/2205.13760,139.00,SOTA improvement,"""We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches.""",700000000.00,"""Our largest transformer model, Tranception L, has 700M parameters and is trained on UniRef100 (Suzek et al., 2014)""",7.240000000000001e+21,"Trained using 64 A100 GPUs for two weeks.
64 * 312 teraFLOP/s * 14 days * 24 hours/day * 3600 seconds/hour * 0.3 utilization (assumption)
= 7.24e21",UniRef100,"""We therefore train our final model (700M parameters) on UniRef100""",75000000001,"Total tokens = Number of Sequences × Average Sequence Length
249,000,000 × 300 = 74,700,000,000 ≈ 7.5 × 10¹⁰ tokens",,336.0,2 weeks,NVIDIA A100,64,,,"The ability to accurately model the fitness landscape of protein sequences is critical to a wide range of applications, from quantifying the effects of human variants on disease likelihood, to predicting immune-escape mutations in viruses and designing novel biotherapeutic proteins. Deep generative models of protein sequences trained on multiple sequence alignments have been the most successful approaches so far to address these tasks. The performance of these methods is however contingent on the availability of sufficiently deep and diverse alignments for reliable training. Their potential scope is thus limited by the fact many protein families are hard, if not impossible, to align. Large language models trained on massive quantities of non-aligned protein sequences from diverse families address these problems and show potential to eventually bridge the performance gap. We introduce Tranception, a novel transformer architecture leveraging autoregressive predictions and retrieval of homologous sequences at inference to achieve state-of-the-art fitness prediction performance. Given its markedly higher performance on multiple mutants, robustness to shallow alignments and ability to score indels, our approach offers significant gain of scope over existing approaches. To enable more rigorous model testing across a broader range of protein families, we develop ProteinGym -- an extensive set of multiplexed assays of variant effects, substantially increasing both the number and diversity of assays compared to existing benchmarks.",Open weights (unrestricted),,Confident,"United Kingdom of Great Britain and Northern Ireland,United States of America,Canada",,,,,,"Academia,Academia,Industry",,MIT,"Academia,Academia,Industry",,,56859.714898306775,Hardware
ProGen2-xlarge,Biology,"Proteins,Protein generation,Protein or nucleotide language model (pLM/nLM)","Salesforce Research,Columbia University,Johns Hopkins University","Erik Nijkamp, Jeffrey Ruffolo, Eli N. Weinstein, Nikhil Naik, Ali Madani
",2022-06-27,ProGen2: Exploring the Boundaries of Protein Language Models,https://arxiv.org/abs/2206.13517,199.00,SOTA improvement,"""ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and pre- dicting protein fitness without additional finetuning.""",6400000000.00,"""We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters""",1.35e+22,"Estimate 1:
""350,000 steps x 1m batch size x 6.4 B “connections” x 6"" - Arb Research (https://arbresearch.com/files/gen_bio.pdf)
Steps and batches from Table 1. 
FLOP estimate: 1.3e22

Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf
FLOP estimate: 1.4e22

Geometric mean = 1.35e22 FLOP","UniRef90,BFD30","""The standard PROGEN2 models are pretrained on a mixture of Uniref90 (Suzek et al., 2015) and BFD30 (Steinegger & Söding, 2018) databases""",350000000000,350B from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1,,,,Google TPU v3,,,Industry,"Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence- driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.",Open weights (unrestricted),,Confident,"United States of America,United States of America,United States of America",,,,,,"Industry,Academia,Academia",Unreleased,"BSD license (permissive)
https://github.com/salesforce/progen?tab=BSD-3-Clause-1-ov-file#readme","Industry,Academia,Academia",,,,"Hardware,Third-party estimation"
DiffDock,Biology,Proteins,Massachusetts Institute of Technology (MIT),"Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi Jaakkola",2022-10-04,"DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking","https://arxiv.org/abs/2210.01776, https://docs.nvidia.com/bionemo-framework/latest/models/diffdock.html",313.00,SOTA improvement,"""DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods""",20240000.00,"""For determining the hyperparameters of DIFFDOCK’s score model, we trained
smaller models (3.97 million parameters) that fit into 48GB of GPU RAM before scaling it up to the final model (20.24 million parameters) that was trained on four 48GB GPUs""

There's a separate 4.77M ""confidence model"" that helps make predictions along with the score model",72000000000000000000.00,"""We trained our final score model on four 48GB RTX A6000 GPUs for 850 epochs (around 18 days).""

4 * 38.7 teraflops * 18 days * 24 * 3600 * 0.3 = 7.2e19

https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",PDB (Protein Data Bank),"""We evaluate our method on the complexes from PDBBind [Liu et al., 2017], a large collection of protein-ligand structures collected from PDB [Berman et al., 2003], which was used with time-based splits to benchmark many previous works""
",17000,"""We employ the time-split of PDBBind proposed by Stark et al. [2022] with 17k complexes from 2018 or earlier for training/validation and 363 test structures from 2019 with no ligand overlap with the training complexes""",850.00,432.0,18 days,NVIDIA RTX A6000,,,,"Predicting the binding structure of a small molecule ligand to a protein -- a task known as molecular docking -- is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD<2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy.",Open weights (unrestricted),,Likely,United States of America,,,,,,Academia,,,Academia,,,,Hardware
Galactica,"Language,Biology","Language modeling,Question answering",Meta AI,"Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",2022-11-16,Galactica: A Large Language Model for Science,https://arxiv.org/abs/2211.09085,599.00,SOTA improvement,"""We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH""",120000000000.00,"""The largest 120B model we train runs on a single NVIDIA A100 node""",3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",Galactica Corpus,"""Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process LATEX where we can capture it, and also include academic code to capture computational science""",106000000000,"""Total dataset size = 106 billion tokens""",4.00,,,NVIDIA A100 SXM4 80 GB,128,,,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",Open weights (non-commercial),,Likely,United States of America,,,,2000000,"Table 1: batch size 2M, warmup 1.1B (out of 450B tokens)",Industry,Unreleased,"cc-by-nc (non-commercial): https://huggingface.co/facebook/galactica-120b 

repo but no training code: https://github.com/paperswithcode/galai/blob/main/README.md ",Industry,,,113523.59989094557,Operation counting
Ankh_large,Biology,"Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,Columbia University","Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost",2023-01-16,Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling,https://arxiv.org/abs/2301.06568,59.00,SOTA improvement,"""On average, Ankh improved the PLM SOTA performance by 4.8%""",1900000000.00,"Figure 1 indicates 1.15B parameters, but both the huggingface model and a replication (https://huggingface.co/ElnaggarLab/ankh-large and https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf) indicate 1.9B parameters.
Notebook for counting params: https://colab.research.google.com/drive/1EGI5_vDl4pOBUukJexMHQR16BFKJe4a5?usp=sharing",6.5e+21,"Table 9 from here: https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1.full.pdf

Can also be manually estimated based on the details in Table 11 and 4.6.1 Exp 4. 14B residues * 68 epochs = 952B tokens seen in forward passes. However, only 20% of tokens are masked as individual targets; other tokens in consecutive spans are collapsed into single-token targets to reduce computations. For masking rate of 20%, the average sequence will have 36% as many targets as input tokens under this strategy. This is the relevant number of backward passes:
(2 * 952B * 19B) + (4 * 952B * 0.36 * 19B) = 6.22e22

36% figure verified here: https://colab.research.google.com/drive/1ETsmp_KRMK8kIRA5kdfcO9QiPK28cBQ6?usp=sharing ",UniRef50,"""We build upon the same results by pre-training our baseline on UniRef50.""",14000000000,"Pretrained over UniRef50; 45M proteins and 14B amino acids, per Table 2

952B tokens from Table 9 at:
https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1
(This is total tokens over multiple epochs)",68.00,,,Google TPU v4,64,,,"As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google’s TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.",Open weights (non-commercial),,Confident,"Germany,United States of America",,,,524288,Table 11,"Academia,Academia",,"cc non-commercial:
https://github.com/agemagician/Ankh/blob/main/LICENSE.md","Academia,Academia",,,27229.314562763117,"Operation counting,Third-party estimation"
ProteinDT,"Biology,Language",Proteins,"University of California (UC) Berkeley,California Institute of Technology,University of Toronto,University of Wisconsin Madison,Texas A&M,NVIDIA,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)","Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, Jian Tang, Hongyu Guo, and Anima Anandkumar",2023-02-09,A Text-guided Protein Design Framework,https://arxiv.org/abs/2302.04611,,SOTA improvement,"""Compared to six state-of-the-art protein sequence representation methods, ProteinDT can obtain consistently superior performance on four of six benchmark tasks.""",,,,,UniProtKB,They extract a subset of 441K protein-text pairs,197000001,"Total amino acids: 197,000,000 residues

Final calculation: 1.97 × 10⁸ datapoints

Value = 197,000,000 = 1.97e8",,,,,,,,"Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins’ high-level functionalities. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP which aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that creates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks.",Unreleased,,Unknown,"United States of America,United States of America,Canada,United States of America,United States of America,United States of America,Canada",SciBERT,,,,,"Academia,Academia,Academia,Academia,Academia,Industry,Academia",,,"Academia,Academia,Academia,Academia,Academia,Industry,Academia",,,,
LLaMA-7B (protein-oriented instruction-tuned),"Language,Biology","Protein folding prediction,Protein generation,Other Biological Modeling",Zhejiang University,"Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen",2023-10-02,Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models,https://arxiv.org/abs/2306.08018,54.00,,,7000000000.00,In the name,2.78e+22,"Estimate 1: 1T tokens * 6.7B parameters * 6 FLOP/token/parameter = 4e22 FLOP

from paper, Llama-7B took 82,432 GPU hours using A100s
Estimate 2: 312 trillion FLOP/s * (82,432 * 3600) s * 0.3 = 2.78e22 FLOP",Mol instructions,"""we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions""",204000001,"Data estimate summary:
* Instructions: 2,043,587
* Tokens per instruction: 100 
* Total = 2,043,587 × 100 = 2.04e8 tokens",,,,,,,,"Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce MolInstructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models’ performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability.",Open weights (unrestricted),,Confident,China,LLaMA-7B,,,,,Academia,Unreleased,"MIT. includes data
https://github.com/zjunlp/Mol-Instructions",Academia,,,,"Hardware,Operation counting"
HyenaDNA,Biology,Protein or nucleotide language model (pLM/nLM),"Stanford University,Harvard University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal","Eric Nguyen, Michael Poli, Marjan Faizi, Armin W. Thomas, Callum Birch Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, Christopher Ré",2023-06-27,HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution,https://arxiv.org/abs/2306.15794,123.00,SOTA improvement,"""On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data.1 On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points, and by as much as +20 accuracy points on enhancer identification.""",6600000.00,"Table A.1 shows details, largest experiment is on far right.",1.811e+21,"8 Nvidia A100 (80GB) GPUs, ~4 weeks
(4 * 7 * 24 * 3600) seconds * (8 * 3.12e14) FLOP/sec * 0.3 (utilization) = 1.811e21",Human Reference Genome (GRCh38/hg38),"""For pretraining, we use a single human reference genome [...] For the test set, we use chromosomes 14 and X, exclusively"" 
Human genome is ~3.2B nucleotide pairs, 14 and X are ~101M and 154M respectively.",2945000000,"Human genome is ~3.2B nucleotide pairs, 14 and X are ~101M and 154M respectively. Largest run sees 2T tokens, which implies ~679 epochs.",679.12,672.0,"""For example, the largest model with context length 1M was trained on 2T tokens over 4 weeks.""",NVIDIA A100,8,,,"Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points.",,,Confident,"United States of America,United States of America,Canada,Canada",,,,64000000,"Table A.1 indicates largest model saw sequence length of 1M, and that batch sizes range from 64-1024. In section 3.2: ""Our sequence length schedule starts at L1 = 64, then doubles the window at each stage while keeping the global batch size constant."" I assume smallest batch size was used for largest sequence length.","Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia",,,7079.804571805214,Hardware
PeptideBERT,Biology,Proteins,Carnegie Mellon University (CMU),"Chakradhar Guntuboina, Adrita Das, Parisa Mollaei, Seongwon Kim, and Amir Barati Farimani",2023-08-28,PeptideBERT: A language Model based on Transformers for Peptide Property Prediction,https://arxiv.org/abs/2309.03099,,SOTA improvement,"""Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide’s potential to induce red blood cell lysis.""",,,49000000000000000.00,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",,,21700000001,"Pretraining:
217,000,000 sequences × 100 residues = 2.17 × 10¹⁰ tokens

Fine-tuning sequences:
9,316 + 29,892 + 17,185 = 56,393 sequences
56,393 × 100 residues = 5.64 × 10⁶ tokens

Total tokens:
2.17 × 10¹⁰ + 5.64 × 10⁶ ≈ 2.17 × 10¹⁰",30.00,4.1,244 minues from Table 1,NVIDIA GeForce GTX 1080 Ti,1,,,"Recent advances in Language Models have enabled the protein modeling community with a powerful tool since protein sequences can be represented as text. Specifically, by taking advantage of Transformers, sequence-to-property prediction will be amenable without the need for explicit structural data. In this work, inspired by recent progress in Large Language Models (LLMs), we introduce PeptideBERT, a protein language model for predicting three key properties of peptides (hemolysis, solubility, and non- fouling). The PeptideBert utilizes the ProtBERT pretrained transformer model with 12 attention heads and 12 hidden layers. We then finetuned the pretrained model for the three downstream tasks. Our model has achieved state of the art (SOTA) for predicting Hemolysis, which is a task for determining peptide’s potential to induce red blood cell lysis. Our PeptideBert non-fouling model also achieved remarkable accuracy in predicting peptide’s capacity to resist non-specific interactions. This model, trained predominantly on shorter sequences, benefits from the dataset where negative examples are largely associated with insoluble peptides. Codes, models, and data used in this study are freely available at: https://github.com/ChakradharG/PeptideBERT",Open weights (unrestricted),,Confident,United States of America,ProtBERT-UniRef,49805280000000000,"""Compute for fine-tuning ProtBERT: 1 NVidia GeForce GTX 1080Ti, 30 epochs, batch size 32, model trained for individual tasks with training time ranging from 58-116 minutes, assuming 
from Table 1 we have 244 minutes
11.34e12 FLOPs and 0.3 utilization rate FLOP = 244 min * 60 sec/min * 11.34e12 FLOP/sec *0.3 = 4.9e16 FLOP,",,,Academia,Open source,"MIT (models, training, inference): https://github.com/ChakradharG/PeptideBERT ",Academia,,,552.7802604940294,Hardware
EXAONE 2.0,"Multimodal,Image generation,Language,Biology,Vision","Language modeling,Image generation,Visual question answering",LG AI Research,,2023-07-19,LG AI Research Develops Foundation Model Using Amazon SageMaker,https://aws.amazon.com/solutions/case-studies/lg-ai-research-case-study/,,,,300000000000.00,300 billion,,,Unspecified unreleased,"From KoreaTimes (https://www.koreatimes.co.kr/www/tech/2023/12/129_355258.html)

""EXAONE 2.0 studied about 45 million specialized documents and 350 million images, including patents and papers secured through partnerships.

Considering that much of the existing expertise data is in English, EXAONE 2.0 is developed as a bilingual model that can understand and answer both in Korean and English at the same time. It also learns over four times more data than the previous model.""",,,,,,,,,,,Unreleased,,Speculative,Korea (Republic of),,,,,,Industry,,,Industry,checked,,,
SeqVec,Biology,Proteins,Technical University of Munich,"Michael Heinzinger, Ahmed Elnaggar, Yu Wang, Christian Dallago, Dmitrii Nechaev, Florian Matthes & Burkhard Rost ",2019-12-17,Modeling aspects of the language of life through transfer-learning protein sequences,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3220-8,,,,93000000.00,"""The model had about 93 M (mega/million) free parameters""",41000000000000000000.00,"3 weeks, 5 NVIDIA Titan GPUs (Assuming NVIDIA Titan V and 30% utilization rate for calculation) with 12 GB memory, ",UniRef50,UniRef50,9500000000,"""We found UniRef50 to contain almost ten times more tokens (9.5 billion amino acids) than the largest existing NLP corpus (1 billion words)""
Elsewhere notes 9.6B, possibly one figure is rounded.",,508.0,,NVIDIA Titan V,5,,,"Background
Predicting protein function and structure from sequence is one important challenge for computational biology. For 26 years, most state-of-the-art approaches combined machine learning and evolutionary information. However, for some applications retrieving related proteins is becoming too time-consuming. Additionally, evolutionary information is less powerful for small families, e.g. for proteins from the Dark Proteome. Both these problems are addressed by the new methodology introduced here.
Results
We introduced a novel way to represent protein sequences as continuous vectors (embeddings) by using the language model ELMo taken from natural language processing. By modeling protein sequences, ELMo effectively captured the biophysical properties of the language of life from unlabeled big data (UniRef50). We refer to these new embeddings as SeqVec (Sequence-to-Vector) and demonstrate their effectiveness by training simple neural networks for two different tasks. At the per-residue level, secondary structure (Q3 = 79% ± 1, Q8 = 68% ± 1) and regions with intrinsic disorder (MCC = 0.59 ± 0.03) were predicted significantly better than through one-hot encoding or through Word2vec-like approaches. At the per-protein level, subcellular localization was predicted in ten classes (Q10 = 68% ± 1) and membrane-bound were distinguished from water-soluble proteins (Q2 = 87% ± 1). Although SeqVec embeddings generated the best predictions from single sequences, no solution improved over the best existing method using evolutionary information. Nevertheless, our approach improved over some popular methods using evolutionary information and for some proteins even did beat the best. Thus, they prove to condense the underlying principles of protein sequences. Overall, the important novelty is speed: where the lightning-fast HHblits needed on average about two minutes to generate the evolutionary information for a target protein, SeqVec created embeddings on average in 0.03 s. As this speed-up is independent of the size of growing sequence databases, SeqVec provides a highly scalable approach for the analysis of big data in proteomics, i.e. microbiome or metaproteome analysis.
Conclusion
Transfer-learning succeeded to extract information from unlabeled sequence databases relevant for various protein prediction tasks. SeqVec modeled the language of life, namely the principles underlying protein sequences better than any features suggested by textbooks and prediction methods. The exception is evolutionary information, however, that information is not available on the level of a single sequence.",Open weights (unrestricted),,Confident,Germany,,,,,,Academia,Unreleased,"MIT license. doesn't look like it has training code
https://github.com/rostlab/SeqVec",Academia,,,2802.578432203454,Hardware
ProBERTa,Biology,Proteins,"University of Illinois Urbana-Champaign (UIUC),Reed College","Ananthan Nambiar, Maeve Heflin, Simon Liu, Sergei Maslov, Mark Hopkins, Anna Ritz",2020-09-01,"Transforming the Language of Life: Transformer Neural
Networks for Protein Prediction Tasks",https://dl.acm.org/doi/10.1145/3388440.3412467,97.00,SOTA improvement,"""Furthermore, we used embeddings from PRoBERTa for a fundamentally different problem, PPI prediction, using two different
datasets generated from the HIPPIE database and found that with
sufficient data, it substantially outperforms the current state-of-theart method in the conservative scenario.""",44000000.00,"""In total, our model has approximately 44M trainable parameters.""",9720000000000000000.00,"""we pre-train PRoBERTa on 4 NVIDIA V100 GPUs in 18 hours""
4 * 125 tFLOP/s * 18 * 3600 * 0.3 (assumed utilization) = 9.72e18",UniProtKB/Swiss-Prot,"""Pre-training data: We use UniProtKB/Swiss-Prot (450K unique sequences with a mean tokenized length of 129.6 tokens), a collection of experimentally annotated and reviewed amino acid sequences""

Fine tuning uses a subset of 313,214 sequences which have annotated labels.",58320000,"450k sequences * 129.6 tokens per sequence = 58,320,000 tokens",,18.0,,NVIDIA V100,4,,Academia,"The scientific community is rapidly generating protein sequence information, but only a fraction of these proteins can be experimentally characterized. While promising deep learning approaches for protein prediction tasks have emerged, they have computational limitations or are designed to solve a specific task. We present a Transformer neural network that pre-trains task-agnostic sequence representations. This model is fine-tuned to solve two different protein prediction tasks: protein family classification and protein interaction prediction. Our method is comparable to existing state-of-the-art approaches for protein family classification while being much more general than other architectures. Further, our method outperforms all other approaches for protein interaction prediction. These results offer a promising framework for fine-tuning the pre-trained sequence representations for other protein prediction tasks.",,,Confident,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,2682.896227030356,Hardware
MegaMolBART,Biology,Drug discovery,NVIDIA,,2021-09-14,MegaMolBART,"https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html, https://github.com/NVIDIA/MegaMolBART",,,,45000000.00,"""MegaMolBART has eight layers, four attention heads, a hidden space dimension of 256, and contains 45M parameters""",720000000000000000000.00,"""MegaMolBART was trained with data parallelism on 64 V100 32 GB GPUs (4 nodes x 16 GPUs) for 8 epochs (approximately 160k iterations or ~80 wall-clock hours) using a batch size of 32 molecules per GPU (micro batch)""

https://docs.nvidia.com/bionemo-framework/0.4.0/models/megamolbart.html

64 * 130 teraflops * 80 * 3600 * 0.3 = 7.2e20",ZINC 15,"""The ZINC-15 database was used for training [Sterling and Irwin, 2015]. Approximately 1.54 Billion molecules (SMILES strings) were selected from tranches meeting the following constraints: molecular weight <= 500 Daltons, LogP <= 5, reactivity level was “reactive,” and purchasability was “annotated.” The compounds were filtered to ensure a maximum length of 512 characters.""",,"1.54B molecules, maximum length of 512 characters. Perhaps ~100B tokens",8.00,80.0,,NVIDIA V100,,,,"MegaMolBART is a model that understands chemistry and can be used for a variety of cheminformatics applications in drug discovery. The embeddings from its encoder can be used as features for predictive models. Alternatively, the encoder and decoder can be used together to generate novel molecules by sampling the model’s embedding space.",Open weights (unrestricted),,Likely,United States of America,,,,,,Industry,Open source,"apache, inference and training code: https://github.com/NVIDIA/MegaMolBART/blob/dev/LICENSE/license.txt",Industry,,,,Hardware
DNA Fine-Tuned Language Model (DFLM),Biology,Protein or nucleotide language model (pLM/nLM),Tongji University,"Ying He , Qinhu Zhang , Siguo Wang , Zhanheng Chen , Zhen Cui ,
Zhen-Hao Guo, and De-Shuang Huang",2023-01-02,Predicting the Sequence Specificities of DNA-Binding Proteins by DNA Fine-Tuned Language Model With Decaying Learning Rates,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751352,4.00,,,,,3500000000000000000.00,"""The pre-training of Human genome language model is
resource-intensive (about 7-10 days on 2 NVIDIA TITAN X GPU).""

Assuming FP32 and 30% utilization

Estimate: (10*24*3600) s * 6.7e12 FLOP/s * 2 * 0.3 = 3.5e18",Human Reference Genome (GRCh38/hg38),"""DFLM mainly used two types of data sets, the unlabeled large-scale Genome Reference Consortium Human Build 38(GRCh38/hg38) [1] and about 500 labeled ChIP-seq data set [2]. The unlabeled large-scale genome data set hg38 was used to train a general Human Genome language model. The ChIP-seq data sets were used to train the DBP fine-tuning language model and classification model.""",,,,,,NVIDIA TITAN Xp,,,,"DNA-binding proteins (DBPs) play vital roles in the regulation of biological systems. Although there are already many deep learning methods for predicting the sequence specificities of DBPs, they face two challenges as follows. Classic deep learning methods for DBPs prediction usually fail to capture the dependencies between genomic sequences since their commonly used one-hot codes are mutually orthogonal. Besides, these methods usually perform poorly when samples are inadequate. To address these two challenges, we developed a novel language model for mining DBPs using human genomic data and ChIP-seq datasets with decaying learning rates, named DNA Fine-tuned Language Model (DFLM). It can capture the dependencies between genome sequences based on the context of human genomic data and then fine-tune the features of DBPs tasks using different ChIP-seq datasets. First, we compared DFLM with the existing widely used methods on 69 datasets and we achieved excellent performance. Moreover, we conducted comparative experiments on complex DBPs and small datasets. The results show that DFLM still achieved a significant improvement. Finally, through visualization analysis of one-hot encoding and DFLM, we found that one-hot encoding completely cut off the dependencies of DNA sequences themselves, while DFLM using language models can well represent the dependency of DNA sequences. Source code are available at: https://github.com/Deep-Bioinfo/DFLM",Unreleased,,Confident,China,,,,,,Academia,Open source,"MIT for code. not sure there are weights here?
https://github.com/Deep-Bioinfo/DFLM",Academia,,,,Hardware
Vespa,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)",Technical University of Munich,"Céline Marquet, Michael Heinzinger, Tobias Olenyi, Christian Dallago, Kyra Erckert, Michael Bernhofer, Dmitrii Nechaev & Burkhard Rost ",2021-12-30,Embeddings from protein language models predict conservation and variant effects,https://link.springer.com/article/10.1007/s00439-021-02411-y,,,,231000.00,"""(3) standard convolutional neural network (CNN; with two convolutional layers with a window size of 7, connected through ReLU activations; dropout rate of 0.25; 231 k free parameters)""",,,"ConSurf10k,Eff10k,PMD4k,DMS4,DMS39",ConSurf10k data set contained about 2.7 million samples,2800001,"ConSurf10k: 2.7 x 10^6 samples
Eff10k: 1.00737 x 10^5 samples
Total = 2.7 x 10^6 + 1.00737 x 10^5 = 2.800737 x 10^6 ≈ 2.8 x 10^6 datapoints",,,,,,,,"The emergence of SARS-CoV-2 variants stressed the demand for tools allowing to interpret the effect of single amino acid variants (SAVs) on protein function. While Deep Mutational Scanning (DMS) sets continue to expand our understanding of the mutational landscape of single proteins, the results continue to challenge analyses. Protein Language Models (pLMs) use the latest deep learning (DL) algorithms to leverage growing databases of protein sequences. These methods learn to predict missing or masked amino acids from the context of entire sequence regions. Here, we used pLM representations (embeddings) to predict sequence conservation and SAV effects without multiple sequence alignments (MSAs). Embeddings alone predicted residue conservation almost as accurately from single sequences as ConSeq using MSAs (two-state Mat- thews Correlation Coefficient—MCC—for ProtT5 embeddings of 0.596 ± 0.006 vs. 0.608 ± 0.006 for ConSeq). Inputting the conservation prediction along with BLOSUM62 substitution scores and pLM mask reconstruction probabilities into a simplistic logistic regression (LR) ensemble for Variant Effect Score Prediction without Alignments (VESPA) predicted SAV effect magnitude without any optimization on DMS data. Comparing predictions for a standard set of 39 DMS experiments to other methods (incl. ESM-1v, DeepSequence, and GEMME) revealed our approach as competitive with the state-of-the-art (SOTA) methods using MSA input. No method outperformed all others, neither consistently nor statistically significantly, independently of the performance measure applied (Spearman and Pearson correlation). Finally, we investigated binary effect predictions on DMS experiments for four human proteins. Overall, embedding-based methods have become competi- tive with methods relying on MSAs for SAV effect prediction at a fraction of the costs in computing/energy. Our method predicted SAV effects for the entire human proteome (~ 20 k proteins) within 40 min on one Nvidia Quadro RTX 8000. All methods and data sets are freely available for local and online execution through bioembeddings.com, https://github.com/ Rostlab/VESPA, and PredictProtein.",,,Confident,Germany,ProteinBERT,,,,,Academia,,"All methods and data sets are freely available for local and online execution through bioembeddings.com, https://github.com/Rostlab/VESPA, and PredictProtein.",Academia,,,,
SPIDER2,Biology,"Protein folding prediction,Proteins","Griffith University,University of Iowa,Dezhou University","Yuedong Yang, Rhys Heffernan, Kuldip Paliwal, James Lyons, Abdollah Dehzangi, Alok Sharma, Jihua Wang, Abdul Sattar, and Yaoqi Zhou",2016-10-28,"SPIDER2: A Package to Predict Secondary Structure, Accessible Surface Area, and Main-Chain Torsional Angles by Deep Neural Networks",https://link.springer.com/protocol/10.1007/978-1-4939-6406-2_6,,SOTA improvement,"The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. ",409536.00,"Three networks, each three layers. First takes in 459 inputs and outputs 12, second and third take in 459 + (12*17) = 663 inputs.

Network 1: (459 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 116,112
Networks 2 and 3: (663 * 150 + 150) + (150 * 150 + 150) + (150 * 150 + 150) + (150 * 12 + 12) = 146,712
Total: 116,112 + (2 * 146,712) = 409,536",18220000000000000.00,"120 epochs, dataset 5789 proteins. There are about 300 residues per protein (115,479 residues / 418 proteins) according to https://www.ncbi.nlm.nih.gov/pmc/articles/PMC22960/. 
First network gets 27 features per residue, second and third get 39.
FLOPs from first: 6 * 116112 * (27 * 300 * 5789 * 120) = 3.92e15
FLOPs from 2nd and 3rd: 2 *6 * 146712 * (39 * 300 * 5789 * 120) = 1.43e16
Total: 1.822E16",Unspecified,,13893600,"5,789 nonredundant, high resolution structure.
Assuming ~200 residues per protein, 5,789 * 200 = 1,157,800 residues. Each residue has 12 associated features being predicted on.
1,157,800 * 12 = 13,893,600",120.00,,"The authors had a website where sequences could be submitted for processing through the model: ""Each prediction is usually completed within 10 min, but may take up to a few hours depending on how busy the server is and how long the protein chain is [...] Using an external PSSM file can skip the most time consuming step of generating the evolution profile by PSIBLAST, and the executive time reduce to a few seconds"" 

Rough estimate:
It looks like the PSIBLAST step only needs doing once per input, and this takes the majority of the time. If the inference server uses the same hardware that was used for training, 10 mins * 5789 sequences =  965 hours for PSIBLAST calculation. Then assume training on a sequence takes 3x as long as inference (forward + backward pass uses 6 FLOPs per parameter, vs 2 for forward only), so 120 epochs would take:
3 seconds * 3 * 5789 * 120 = 1,737 hours
Total: around 2,702 hours
(This seems on the long side – probably they had better hardware for training, or else there's an incorrect assumption here)",,,,,"Predicting one-dimensional structure properties has played an important role to improve prediction of protein three-dimensional structures and functions. The most commonly predicted properties are secondary structure and accessible surface area (ASA) representing local and nonlocal structural characteristics, respectively. Secondary structure prediction is further complemented by prediction of continuous main-chain torsional angles. Here we describe a newly developed method SPIDER2 that utilizes three iterations of deep learning neural networks to improve the prediction accuracy of several structural properties simultaneously. For an independent test set of 1199 proteins SPIDER2 achieves 82 % accuracy for secondary structure prediction, 0.76 for the correlation coefficient between predicted and actual solvent accessible surface area, 19° and 30° for mean absolute errors of backbone φ and ψ angles, respectively, and 8° and 32° for mean absolute errors of Cα-based θ and τ angles, respectively. The method provides state-of-the-art, all-in-one accurate prediction of local structure and solvent accessible surface area. The method is implemented, as a webserver along with a standalone package that are available in our website: http://sparks-lab.org.",Open weights (non-commercial),,Likely,"Australia,United States of America,China",,,,,,"Academia,Academia,Academia",,"some kind of download, unclear license

http://zhouyq-lab.szbl.ac.cn/download/","Academia,Academia,Academia",,,,Operation counting
Wu Dao - Wen Su,Biology,Proteins,Beijing Academy of Artificial Intelligence / BAAI,,2021-03-01,China's GPT-3? BAAI Introduces Superscale Intelligence Model 'Wu Dao 1.0',https://medium.com/syncedreview/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0-98a573fc4d70,,,,,,,,,,,,,,,,,,Industry,,,,Unknown,China,,,,,,Academia,,,Academia,,,,
TripletRes,Biology,"Proteins,Protein folding prediction",University of Michigan,"Yang Li, Chengxin Zhang, Eric W. Bell, Dong-Jun Yu, Yang Zhang",2019-08-13,Ensembling multiple raw coevolutionary features with deep residual neural networks for contact-map prediction in CASP13,https://onlinelibrary.wiley.com/doi/10.1002/prot.25798,,,,,,,"4 GPUs, 50 epochs, batch size between 1-4 depending on sequence length",SCOPe 2.07,"""7,671 non-redundant domains, sequence length 30-400, with sequences for which PDB structure resolution is good""",,,50.00,,,,,,,"We report the results of residue-residue contact prediction of a new pipeline built purely on the learning of coevolutionary features in the CASP13 experiment. For a query sequence, the pipeline starts with the collection of multiple sequence alignments (MSAs) from multiple genome and metagenome sequence databases using two complementary Hidden Markov Model (HMM)-based searching tools. Three profile matrices, built on covariance, precision, and pseudolikelihood maximization respectively, are then created from the MSAs, which are used as the input features of a deep residual convolutional neural network architecture for contact-map training and prediction. Two ensembling strategies have been proposed to integrate the matrix features through end-to-end training and stacking, resulting in two complementary programs called TripletRes and ResTriplet, respectively. For the 31 free-modeling domains that do not have homologous templates in the PDB, TripletRes and ResTriplet generated comparable results with an average accuracy of 0.640 and 0.646, respectively, for the top L/5 long-range predictions, where 71% and 74% of the cases have an accuracy above 0.5. Detailed data analyses showed that the strength of the pipeline is due to the sensitive MSA construction and the advanced strategies for coevolutionary feature ensembling. Domain splitting was also found to help enhance the contact prediction performance. Nevertheless, contact models for tail regions, which often involve a high number of alignment gaps, and for targets with few homologous sequences are still suboptimal. Development of new approaches where the model is specifically trained on these regions and targets might help address these problems.",Unreleased,,Unknown,United States of America,,,,,,Academia,,,Academia,,,,
OntoProtein,"Biology,Language","Proteins,Protein or nucleotide language model (pLM/nLM)",Zhejiang University,"Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Siyuan Cheng, Shumin Deng, Qiang Zhang, Jiazhang Lian, Huajun Chen, Haosen Hong",2022-01-23,ONTOPROTEIN: PROTEIN PRETRAINING WITH GENE ONTOLOGY EMBEDDING,https://openreview.net/pdf?id=yfe1VMYAXa4,,SOTA improvement,Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1.,420000000.00,"""For the protein encoder, we use the pre-trained ProtBert from Elnaggar et al. (2020).""",,,ProteinKG25,"""the ProteinKG25 dataset used for pre-training contains about 612,483 entities and 4,990,097 triples, aligned with GO annotations and including protein sequences.""",160000001,"Data Summary:
- Protein tokens: 488,000 × 300 = 1.464 × 10^8
- GO term tokens: 612,483 × 20 = 1.224966 × 10^7
- Knowledge graph triples: 4,990,097
Total: (1.464 × 10^8 + 1.224966 × 10^7 + 4.99 × 10^6) ≈ 1.626 × 10^8 data points",,,,,,,,"Self-supervised protein language models have proved their effectiveness in learn- ing the proteins representations. With the increasing computational power, cur- rent protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve re- markable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowl- edge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction1.",,,,China,ProtBERT-BFD,,,,,Academia,,,Academia,,,,
MSA Transformer,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Facebook AI Research,University of California (UC) Berkeley,New York University (NYU)","Roshan Rao, Jason Liu, Robert Verkuil, Joshua Meier, John F. Canny, Pieter Abbeel, Tom Sercu, Alexander Rives",2021-02-13,MSA Transformer,https://proceedings.mlr.press/v139/rao21a/rao21a.pdf,433.00,SOTA improvement,"""The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models""",100000000.00,"""We train an MSA Transformer model with 100M parameters..."" ",5.49e+21,"Based on: https://docs.google.com/spreadsheets/d/1enan21dFx03TkwufHgOwTVNBtuYlqNY9uurjIK6YS-8/edit#gid=0

Number of steps 4.5e5, batch size (tokens) 6.1e7, parameters 1e8

Calculation = 4e8 FLOP/bp * 4.5e5 bp + 2e8 FLOP/fp * 2.75e13 fp

Batch size: 512
Seq length: 100 * 1192 tokens
All models are trained on 32 V100 GPUs for 100k updates. The four models with best contact precision are then further trained to 150k updates. Finally, the best model at 150k updates is trained to 450k updates.

450k * 512 * 100 * 1192 * 100M * 6 = 1.65e22","UniRef50,UniRef30 (FKA UniClust30)","""Models are trained on a dataset of 26 million MSAs. An MSA is generated for each UniRef50 sequence by searching UniClost30 with HHblits.""",9297600000000,"""We train an MSA Transformer model with 100M parameters on a large dataset (4.3 TB) of 26 million MSAs, with an average of 1192 sequences per MSA.""
Average sequence is ~300 amino acids/tokens long.
26 million * 1192 * 300 = 9.3T tokens",,,,NVIDIA Tesla V100 DGXS 32 GB,32,,,"Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models. ",Open weights (unrestricted),,Likely,"United States of America,United States of America,United States of America",,,,,,"Industry,Academia,Academia",Unreleased,"MIT: https://github.com/facebookresearch/esm

looks like no training code","Industry,Academia,Academia",,,17854.564630974684,Operation counting
SACHS,Biology,Other Biological Modeling,"Massachusetts Institute of Technology (MIT),Stanford University","K. Sachs, O. Perez, D. Pe'er, D. A. Lauffenburger and G. P. Nolan",2005-04-22,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data.,https://science.sciencemag.org/content/308/5721/523.long,1682.00,Highly cited,,178.00,From https://www.bnlearn.com/bnrepository/,,,,,5400,"I think? 

"" The truncated singlecell data set (420 data points) shows a large
(11-arc) decline in accuracy, missing more connections and reporting more unexplained arcs than its larger (5400 data points) counterpart (fig. S4B). ""

Seems potentially wrong by maybe 20%. Might need to add 1200.",,,,,,,Academia,,,,,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
ProGen,Biology,"Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM)","Salesforce Research,Stanford University","Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R. Eguchi,  View ORCID ProfilePo-Ssu Huang, Richard Socher",2020-03-13,ProGen: Language Modeling for Protein Generation,https://www.biorxiv.org/content/10.1101/2020.03.07.982272v2,241.00,,,1200000000.00,"""We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences""",370000000000000000000.00,"Our model was implemented in TensorFlow (Abadi et al.,
2016) and trained with a global batch size of 64 distributed
across 256 cores of a Cloud TPU v3 Pod for 1M iterations. Training took approximately two weeks using Adagrad (Duchi et al., 2011)

4.00E+12*256*60**2*24*14*0.3 = 3.7e20

7.6E+21 FLOPs from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1",,,1049000000000," 1,049B from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1",5.00,,,,,,Industry,"Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ∼280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",,,Likely,"United States of America,United States of America",,,,,,"Industry,Academia",,,"Industry,Academia",,,,"Hardware,Third-party estimation"
ProtT5-XXL,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University","A Elnaggar, M Heinzinger, C Dallago, G Rihawi",2021-05-04,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",396.00,SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art
without using evolutionary information""",11000000000.00,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb",7.370000000000001e+22,"7.37e22 from:
source: https://lair.lighton.ai/akronomicon/
archived: https://github.com/lightonai/akronomicon/blob/main/akrodb/Technical%20University%20of%20Munich/ProtT5-XXL.json

3.7E+22 from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1

Manual calculation: forward passes on 512 * (4096*920k + 2048*343k) = 2.3T tokens
Backward passes on 15% of those, 2.3T * 0.15 = 343B tokens.
Total FLOPs: (2 * 11B * 2.3T) + (4 * 11B * 343B) = 6.57e22","BFD (Big Fantastic Dataset),UniRef50","First, T5-XL and T5-XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on
UniRef50 for 991k and 343k steps respectively (ProtT5-XLU50, ProtT5-XXL-U50).
Table 1 and 2 give enough info to calculate epochs:
BFD: 512 (seq len) * 4096 (global batch) * 920k (steps) / 393B = 4.9 epochs
UniRef50: 512 (seq len) * 2048 (global batch) * 343k (steps) / 14B = 25.7 epochs",407000000000,"Table 2. ProtT5-XXL uses BFD100 and UniRef50, which sum to 407 billion amino acids.",,,,Google TPU v3,512,,Industry,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",Open weights (unrestricted),,Confident,"Germany,China,United States of America,United States of America,United States of America,Korea (Republic of)",,,,,,"Academia,Industry,Government,Industry,Academia",Unreleased,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans","Academia,Industry,Government,Industry,Academia",,checked,513779.0240754994,"Third-party estimation,Operation counting"
ProtT5-XXL-BFD,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,Med AI Technology,NVIDIA,Oak Ridge National Laboratory,Google,Seoul National University","Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, Burkhard Rost",2021-05-04,ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",,SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.""",11000000000.00,Table 2,3.7e+22,"FLOP = 11B*2*(920k*512*4096) +  11B*4*(920k*512*4096), 920k steps using seq length 512 batch size 4096, ",BFD (Big Fantastic Dataset),"First, T5-XL and T5-XXL were trained on BFD for 1.2M and 920k steps respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second step, ProtT5-XL-BFD and ProtT5-XXL-BFD were fine-tuned on
UniRef50 for 991k and 343k steps respectively (ProtT5-XLU50, ProtT5-XXL-U50).",,"Table 1: 2122M proteins, 393B amino acids, 572 GB",5.00,,,Google TPU v3,512,,,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.

Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",Open weights (unrestricted),,Confident,"Germany,China,United States of America,United States of America,United States of America,Korea (Republic of)",,,,,,"Academia,Industry,Government,Industry,Academia",Unreleased,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans","Academia,Industry,Government,Industry,Academia",,,513779.0240754994,Operation counting
ProtBERT-BFD,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,NVIDIA,Seoul National University,Google,Oak Ridge National Laboratory,Med AI Technology","Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,  Debsindhu Bhowmik, Burkhard Rost",2021-05-04,ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",,SOTA improvement,"""For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches.""",420000000.00,Table 2,3.9e+22,"FLOP = 420M * 6 * (800k*512*32k + 200k*2048*6k) 
1M steps total split into two phases, (1) 800k steps, seq length 512 (batch size 32k) and (2) 200k steps, seq length 2048 (batch size 6k)
single TPU Pod V3-1024 (64 nodes and 1024 TPUs) info from paper and https://huggingface.co/Rostlab/prot_bert_bfd",BFD (Big Fantastic Dataset),"ProtBert: BERT2 was trained using both UniRef100
and BFD-100 datasets (referred to as ProtBert and ProtBertBFD, respectively; Table 2)",8900000000000,"""ProtBERT-BFD (420M parameters) saw around 27B proteins during pre-training"" 

Table 1: BFD has 2122M proteins, 393B amino acids, 572 GB
Suggests average amino acid length of 185

Implies 27B * 185 = 5T amino acids seen in training

However, Table 2 suggests number of tokens (amino acids) seen in training was:
(512*32768*800k) + (2048*6144*200k) = 15.9T amino acids in training

Geometric mean = 8.9T",,,"figure 3 shows 19 hours per epoch, though this was on a different GPU setup than the one used for training.",Google TPU v3,1024,,,"Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores.

Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The first was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81%-87%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble (2-state accuracy Q2=91%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.",Open weights (unrestricted),,Confident,"Germany,United States of America,Korea (Republic of),United States of America,United States of America,China",,,,,,"Academia,Industry,Academia,Industry,Government",Unreleased,"Licensed under the Academic Free License version 3.0

The ProtTrans project is a open source project supported by various partner companies and research institutions. We are committed to share all our pre-trained models and knowledge. 
https://github.com/agemagician/ProtTrans","Academia,Industry,Academia,Industry,Government",,,1027558.0481509988,Operation counting
AlphaFold-Multimer,Biology,"Protein folding prediction,Proteins","Google DeepMind,DeepMind","Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin Žídek, Russ Bates, Sam Blackwell, Jason Yim, Olaf Ronneberger, Sebastian Bodenstein, Michal Zielinski, Alex Bridgland, Anna Potapenko, Andrew Cowie, Kathryn Tunyasuvunakool, Rishub Jain, Ellen Clancy, Pushmeet Kohli, John Jumper and Demis Hassabis",2021-10-04,Protein complex prediction with AlphaFold-Multimer,https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1,1762.00,"Highly cited,SOTA improvement","""On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] ≥ 0.49) on 14 targets and high accuracy (DockQ ≥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2])""

""For heteromeric interfaces we successfully predict the interface (DockQ ≥ 0.23) in 67% of cases, and produce high accuracy predictions (DockQ ≥ 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively""

""For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances""",,"""Multiple changes to the AlphaFold system were made to adapt it to training on protein complexes, which are detailed below. Summarizing briefly, we [...] make various small adjustments to the structure losses and the model architecture."" [2. Methods]

Hence, this will have approximately the same amount of parameters as AlphaFold2",4.3499999999999995e+21,"Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""

Assuming: FP16 and utilization 0.4

Calculation: (14+2) days * 24 hours/day * 60 min/hour * 60 sec/min * (128 TPU cores/2 cores per chip) * 1.23e14 FLOP/s per chip * 0.4 utilization = 4.35e21 FLOPs",PDB (Protein Data Bank),"""The training dataset comprised structures from the Protein Data Bank (PDB) [13] with a maximum release date of 2018-04-30"" [2.5. Training Regimen]",147328,"See: https://www.rcsb.org/stats/growth/growth-released-structures for 2018

""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores with a batch size of 1 per TPU core. Then we halve the learning rate and double the number of sequences fed into the MSA stack before running two separate fine-tuning stages (one further day of training each)""

10000000/147328 ~ 68 epochs",68.00,384.0,"Section: 2.5. Training Regimen
""We train the model to convergence (approximately 10M samples, for 2 weeks) across 128 TPUv3 cores [...]. Then we [...] run two separate fine-tuning stages (one further day of training each)""",Google TPU v3,64,,,"While the vast majority of well-structured single protein chains can now be predicted to high accuracy due to the recent AlphaFold [1] model, the prediction of multi-chain protein complexes remains a challenge in many cases. In this work, we demonstrate that an AlphaFold model trained specifically for multimeric inputs of known stoichiometry, which we call AlphaFold-Multimer, significantly increases accuracy of predicted multimeric interfaces over input-adapted single-chain AlphaFold while maintaining high intra-chain accuracy. On a benchmark dataset of 17 heterodimer proteins without templates (introduced in [2]) we achieve at least medium accuracy (DockQ [3] ≥ 0.49) on 14 targets and high accuracy (DockQ ≥ 0.8) on 6 targets, compared to 9 targets of at least medium accuracy and 4 of high accuracy for the previous state of the art system (an AlphaFold-based system from [2]). We also predict structures for a large dataset of 4,433 recent protein complexes, from which we score all non-redundant interfaces with low template identity. For heteromeric interfaces we successfully predict the interface (DockQ ≥ 0.23) in 67% of cases, and produce high accuracy predictions (DockQ ≥ 0.8) in 23% of cases, an improvement of +25 and +11 percentage points over the flexible linker modification of AlphaFold [4] respectively. For homomeric interfaces we successfully predict the interface in 69% of cases, and produce high accuracy predictions in 34% of cases, an improvement of +5 percentage points in both instances.",Open weights (unrestricted),,Confident,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United Kingdom of Great Britain and Northern Ireland",AlphaFold 2,,,,,"Industry,Industry",Unreleased,"While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license

https://github.com/google-deepmind/alphafold","Industry,Industry",,,64120.378469547315,Hardware
DARK,Biology,"Protein generation,Proteins",University College London (UCL),"Lewis Moffat, Shaun M. Kandathil, David T. Jones",2022-01-28,Design in the DARK: Learning Deep Generative Models for De Novo Protein Design,https://www.biorxiv.org/content/10.1101/2022.01.27.478087v1.full,25.00,,,,"""DARK3, uses a Transformer decoder with 12 layers, 12 heads, and a feedforward dimension of 768.""",9700000000000000000.00,"""Rounding up to the nearest day, if we were to re-perform DARK from nothing to having a trained DARK3 it would take 12 days when parallelized across ten V100 GPUS. Of that time, model training constitutes just over 3 days and only requires 1 GPU""

3 * 24 * 3600 * 125 teraFLOP/s * 0.3 (utilization) = 9.7e18",,"Iteratively trained by generating synthetic data: ""To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences""",50000001,"500,000 sequences × 100 amino acids = 50,000,000 data points",,,,NVIDIA V100,,,,"The design of novel protein sequences is providing paths towards the development of novel therapeutics and materials. At the forefront is the challenging field of de novo protein design, which looks to design protein sequences unlike those found in nature using general design methodologies. In this work, we develop a tool for de novo design, based on a deep generative sequence model, that rapidly samples novel protein sequences with diverse and ordered structures. To build this tool we develop a framework, called DARK, that trains the underlying generative model on an iteratively expanding set of synthetic sequences. The resulting model generalizes where models trained on natural sequences struggle and greatly improves on the efficiency of comparable sampling-based approaches. We further show how it can generate high quality candidates for de novo design problems and aid in the development of further novel design methods, in all, providing another step, amongst others, towards truly automated and intelligent protein design.",,,Speculative,United Kingdom of Great Britain and Northern Ireland,,,,,,Academia,,,Academia,,,,Hardware
OmegaPLM,Biology,"Proteins,Protein folding prediction","Massachusetts Institute of Technology (MIT),Westlake University","Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, Jian Peng",2022-07-22,High-resolution de novo structure prediction from primary sequence,https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1,268.00,Historical significance,"""Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures""",670000000.00,"""Our model contains 66 layers with around 670 million parameters without sharing parameters, which doubles the layer count of ESM-1b but roughly retains the parameter count.""",1.03514112e+22,"""OmegaPLM is implemented in PyTorch (44) and trained for 2,560 GPU Nvidia A100 80G days."" 
""Default precision format in Nvidia A100 GPUs is set to TensorFloat-32 for matrix operations.""

Assume 0.3 utilization for language model

Estimate: (2560 * 24 * 3600) s * 156e12 FLOP/s * 0.3 * = 1.04e22",UniRef50,"""After pretraining on sequences in UniRef50 (dated at 2021/04)""",8960000001,"Number of sequences: 35 x 10^6
Sequence length: 512
Total data points: 35 x 10^6 x 512 = 1.792 x 10^10 tokens

First stage crop size: 256
First stage data points: 35 x 10^6 x 256 = 8.96 x 10^9 tokens

Additional structural data: ~110,000 sequences = 7.68 x 10^7 tokens

Final estimate: 8.96 x 10^9 tokens",,,"2,560 GPU Nvidia A100 80G days",NVIDIA A100 SXM4 80 GB,,,,"Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a protein’s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures. OmegaFold enables accurate predictions on orphan proteins that do not belong to any functionally characterized protein family and antibodies that tend to have noisy MSAs due to fast evolution. Our study fills a much-encountered gap in structure prediction and brings us a step closer to understanding protein folding in nature.",,,Confident,"United States of America,China",,,,2097152,"""[...] each batch contains 4,096 sequences and each sequence is padded or cropped to 512 residues"" 4096 * 512 = 2097152","Academia,Academia",,,"Academia,Academia",,,,Hardware
TranceptEve,Biology,"Proteins,Protein pathogenicity prediction","University of Oxford,Harvard Medical School","Pascal Notin, Lood Van Niekerk, Aaron W Kollasch, Daniel Ritter, Yarin Gal, Debora S. Marks",2022-12-10,TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction,https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1,,SOTA improvement,"""Besides its broader application scope, it achieves state-of- the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.""",,,,,ProteinGym,,,,,,,,,,,"Modeling the fitness landscape of protein sequences has historically relied on training models on family-specific sets of homologous sequences called Multiple Sequence Alignments. Many proteins are however difficult to align or have shallow alignments which limits the potential scope of alignment-based methods. Not subject to these limitations, large protein language models trained on non-aligned sequences across protein families have achieved increasingly high predictive performance – but have not yet fully bridged the gap with their alignment-based counterparts. In this work, we introduce TranceptEVE – a hybrid method between family-specific and family-agnostic models that seeks to build on the relative strengths from each approach. Our method gracefully adapts to the depth of the alignment, fully relying on its autoregressive transformer when dealing with shallow alignments and leaning more heavily on the family-specifc models for proteins with deeper alignments. Besides its broader application scope, it achieves state-of-the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.",,,Unknown,"United Kingdom of Great Britain and Northern Ireland,United States of America",Tranception,,,,,"Academia,Academia",,,"Academia,Academia",,,,
CaLM,Biology,Protein or nucleotide language model (pLM/nLM),University of Oxford,"Carlos Outeiral, Charlotte M. Deane",2022-12-19,Codon language embeddings provide strong signals for protein engineering,https://www.biorxiv.org/content/10.1101/2022.12.15.519894v1.full.pdf,8.00,SOTA improvement,"""We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks. In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters"" [Abstract]",86000000.00,"""We trained a large language model with 86M parameters""",29000000000000000000.00,"""4 NVIDIA Quadro RTX4000 GPUs for 40 days""

Calculation assuming FP32, utilization 30%:
= (40 * 24 * 3600) s * 7.1e12 FLOP/s * 0.3 * 4 GPU = 2.999808e+19

alternative calculation:
""Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. ""
""(66,000 gradient steps, 14 full epochs)""

256000*66000*14*86000000*6=1.220567e+20",European Nucleotide Archive (ENA),"""The training set was constructed from the European Nucleotide Archive [39], with significant preprocessing to limit redundancy and save computational cost.""",2304000000,"""a dataset of 9M non-redundant and diverse cDNA sequences identified from whole-genome sequencing""

""Gradients were accumulated to an effective batch size of 1,000 examples, or approximately 256,000 tokens. ""

9000000*256000/1000=2304000000 tokens",14.00,960.0,"""The model reported in this work was trained on 4 NVIDIA Quadro
RTX4000 GPUs for 40 days (66,000 gradient steps, 14 full epochs)""",NVIDIA Quadro RTX 4000,4,,,"Protein representations from deep language models have yielded state-of-the-art performance across many tasks in computational protein engineering. In recent years, progress has primarily focused on parameter count, with recent models’ capacities surpassing the size of the very datasets they were trained on. Here, we propose an alternative direction. We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks.
In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters. These results suggest that, in addition to commonly studied scale and model complexity, the information content of biological data provides an orthogonal direction to improve the power of machine learning in biology.",,,Likely,United Kingdom of Great Britain and Northern Ireland,,,,,,Academia,,,Academia,,,1418.583589472499,"Hardware,Operation counting"
Nucleotide Transformer,Biology,Protein or nucleotide language model (pLM/nLM),"NVIDIA,Technical University of Munich","Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, Marcin Skwark, Karim Beguir,
Marie Lopez, Thomas Pierrot",2023-01-15,"The Nucleotide Transformer: Building and Evaluating Robust
Foundation Models for Human Genomics",https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf,22.00,SOTA improvement,"""We show that the representations alone match or outperform specialized
methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning.""",2500000000.00,"""We built four distinct foundation language models of different sizes, ranging from 500M up to 2.5B parameters""",8.08e+21,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""

In repo, they default to jnp.float32, but recommend fp16 or bp16 for activations in the docstring. JAX defaults to TF32 so they should be utilizing tensor cores.

Assuming 1.56e14 FLOP/s for 32-bit calculations and 0.3 utilization rate

Estimate: 1.56e14 FLOP/s * 128 GPUs * 28 days * 24 h/day * 3600 s/h * 0.3 utilization rate = 1.45e22

Or, with 6ND:
""the model processed a total of 300B tokens during training""
6 * 300B * 2.5B = 4.5e21

Geometric mean: sqrt(1.45e22 * 4.5e21) = 8.08e21","Human Reference Genome (GRCh38/hg38),1000 Genomes Project","""pre-trained them on three different datasets encompassing the Human
reference genome, a collection of 3,200 diverse human genomes, and 850 genomes from several species""
",300000000000,"Largest dataset is the 1000 Genome dataset, with 3202 genomes for a total of 20.5 trillion nucleotides. However, for each dataset training was run until the model saw 300B tokens.",1.00,672.0,"""Training the largest parameter model required a total of 128 GPUs across 16 compute nodes for 28 days""",NVIDIA A100,128,,,"Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.
",,,Likely,"United States of America,Germany",,935000000000000000,"""All fine-tuning runs were performed on a single node with eight A100 GPUs. [...] On average, a fine-tuning run lasted 20 minutes for the 500M parameter models, and 50 minutes for the 2.5B parameter models.""

Estimate: 78e12 FLOP/s * 8 GPUs * 50 min * 60 seconds * 0.5 utilization rate = 9.36e17",,,"Industry,Academia",,,"Industry,Academia",,,113456.5902448678,"Operation counting,Hardware"
LEP-AD,Biology,"Proteins,Protein interaction prediction","King Abdullah University of Science and Technology (KAUST),Karolinska Institute","Anuj Daga, Sumeer Ahmad Khan, David Gomez Cabrero, Robert Hoehndorf, Narsis A. Kiani, Jesper Tegner",2023-03-15,LEP-AD: Language Embedding of Proteins and Attention to Drugs predicts Drug Target Interactions,https://www.biorxiv.org/content/10.1101/2023.03.14.532563v1.full.pdf,1.00,SOTA improvement,"""We report new best-in-class state-of-the-art results compared
to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA,
and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast,
and STITCH. Finally, we find that a pre-trained model with embedding of proteins
(the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold)""",3007381000.00,"Uses ESM-2 3B. Table 2 gives details on the non-ESM layers. The GCN appears to have about 3.31M parameters and the linear layers should have 771k and 3.3M, respectively. So total is ~3.007B",,No indication of the training used here. ESM-2 3B used 3e22.,,Table 1,1244420,"Largest dataset appears to be STITCH, at 1244420 drug-target pairs.",,,,,,,,"Predicting drug-target interactions is a tremendous challenge for drug development and lead optimization. Recent advances include training algorithms to learn drug-target interactions from data and molecular simulations. Here we utilize Evolutionary Scale Modeling (ESM-2) models to establish a Transformer protein language model for drug-target interaction predictions. Our architecture, LEPAD, combines pre-trained ESM-2 and Transformer-GCN models predicting binding affinity values. We report new best-in-class state-of-the-art results compared to competing methods such as SimBoost, DeepCPI, Attention-DTA, GraphDTA, and more using multiple datasets, including Davis, KIBA, DTC, Metz, ToxCast, and STITCH. Finally, we find that a pre-trained model with embedding of proteins (the LED-AD) outperforms a model using an explicit alpha-fold 3D representation of proteins (e.g., LEP-AD supervised by Alphafold). The LEP-AD model
scales favorably in performance with the size of training data. Code available at https://github.com/adaga06/LEP-AD",Unreleased,,Confident,"Saudi Arabia,Sweden",ESM2-3B,,,,,"Academia,Academia",Open (non-commercial),https://github.com/adaga06/LEP-AD unclear license,"Academia,Academia",,,,
gLM,Biology,Protein or nucleotide language model (pLM/nLM),Harvard University,"Yunha Hwang, Andre L. Cornman, Sergey Ovchinnikov, Peter R. Girguis",2023-04-08,Deep learning of genomic contexts predicts protein co-regulation and function,"https://www.biorxiv.org/content/10.1101/2023.04.07.536042v1.full
https://github.com/y-hwang/gLM",,,,1000000000.00,"""Our model consists of ~1B parameters which is at least a magnitude smaller  compared to state-of-the-art pLMs.""",226437000000000000000.00,"""The training stage takes several weeks on four NVIDIA A100 GPUs.""

Assumption: 3 weeks, 40% utilization rate, 78 TFLOP peak rate

Estimate: (3*7*24*3600) s * 78e12 FLOP/s *4 GPU * 0.4 = 2.3e20",MGnify,"""seven million metagenomic contig fragments consisting of 15 to 30 genes from the MGnify database""",30800001,"Number of datapoints = 30.8 × 10^6

Calculations:
30.8 × 10^6 = 30,800,000 unique datapoints/tokens in first epoch",,,,NVIDIA A100,,,,"Deciphering the relationship between a gene and its genomic context is fundamental to understanding and engineering biological systems. Machine learning has shown promise in learning latent relationships underlying the sequence-structure-function paradigm from massive protein sequence datasets. However, to date, limited attempts have been made in extending this continuum to include higher order genomic context information. Evolutionary processes dictate the specificity of genomic contexts in which a gene is found across phylogenetic distances, and these emergent genomic patterns can be leveraged to uncover functional relationships between gene products. Here, we trained a genomic language model (gLM) on millions of metagenomic scaffolds to learn the latent functional and regulatory relationships between genes. gLM learns contextualized protein embeddings that capture the genomic context as well as the protein sequence itself, and appears to encode biologically meaningful and functionally relevant information (e.g. phylogeny, enzymatic function). Our analysis of the attention patterns demonstrates that gLM is learning co-regulated functional modules (i.e. operons). Our findings illustrate that gLM’s unsupervised deep learning of the metagenomic corpus is an effective approach to encode functional semantics and regulatory syntax of genes in their genomic contexts, providing a promising avenue for uncovering complex relationships between  genes in a genomic region.",Open weights (non-commercial),,Likely,United States of America,ESM2-650M,,,,,Academia,Open (non-commercial),"non-commercial license
train code:
https://github.com/y-hwang/gLM?tab=License-1-ov-file",Academia,,,,Hardware
xTrimoPGLM -100B,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Tsinghua University,BioMap Research","Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song",2023-07-06,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,65.00,"SOTA improvement,Training cost","""Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories)""",100000000000.00,"Abstract: ""training xTrimoPGLM at an unprecedented scale of 100 billion
parameters and 1 trillion training tokens""",6.2e+23,"""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1 trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date, xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""

6 * 100 billion params * 1T tokens = 6e23

8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24

directly given in the paper (Table 9, or Table 4 in new version): 6.2E+23 ",UniRef50,,,~24M protein sequences,,3912.0,163 days,NVIDIA A100 SXM4 40 GB,768,,,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.",Unreleased,,Confident,"China,China",,,,2097152,"""We employ batches of 2,048 sequences, each 1,024 tokens in length""","Academia,Industry",Unreleased,,"Academia,Industry",checked,,679602.2897189565,"Reported,Operation counting,Hardware"
RoseTTAFold All-Atom (RFAA),Biology,"Protein folding prediction,Proteins","University of Washington,Seoul National University,University of Sheffield","Rohith Krishna, Jue Wang, Woody Ahern, Pascal Sturmfels, Preetham Venkatesh, Indrek Kalvet, Gyu Rie Lee, Felix S Morey-Burrows, Ivan Anishchenko, Ian R Humphreys, Ryan McHugh, Dionne Vafeados, Xinting Li, George A Sutherland, Andrew Hitchcock, C Neil Hunter, Minkyung Baek, Frank DiMaio, David Baker",2023-10-09,Generalized Biomolecular Modeling and Design with RoseTTAFold All-Atom,https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1,,,,,,214000000000000030000.00,"Supplementary material: ""This took 8 days on 8 NVIDIA A6000 GPUs.""
8*3.87E+13*60*60*24*8","PDB (Protein Data Bank),Cambridge Structural Dataset","More than a PLM. Models other kinds of molecules too. 

""From the PDB, we curated a protein-biomolecule dataset including protein-small molecule, protein-metal, and covalently modified protein complexes, filtering out common solvents and crystallization additives. Following clustering (30% sequence identity) to avoid bias towards overrepresented structures, we obtained 121,800 protein-small molecule structures in 5,662
clusters, 112,546 protein-metal complexes in 5,662 clusters, and 12,689 structures with covalently modified amino acids in 1,099 clusters for training. To help the network learn the general properties of small molecules rather than features specific to the molecules in the PDB, we supplemented the training set with small molecule crystal structures from the Cambridge
Structural Database(12).""",110000001,"Datasets:
121,800 + 112,546 + 12,689 + 21,000 + 6,016 + 25,000 = 298,051 ≈ 300,000 structures

Tokens per structure:
300 residues + 30 atoms = 330 tokens (used 375 tokens as upper estimate)

Final calculation:
300,000 structures × 375 tokens/structure = 1.125 × 10⁸ tokens ≈ 1.1 × 10⁸ tokens",,,,,,,,"Although AlphaFold2 (AF2) and RoseTTAFold (RF) have transformed structural biology by enabling high-accuracy protein structure modeling, they are unable to model covalent modifications or interactions with small molecules and other non-protein molecules that can play key roles in biological function. Here, we describe RoseTTAFold All-Atom (RFAA), a deep network capable of modeling full biological assemblies containing proteins, nucleic acids, small molecules, metals, and covalent modifications given the sequences of the polymers and the atomic bonded geometry of the small molecules and covalent modifications. Following training on structures of full biological assemblies in the Protein Data Bank (PDB), RFAA has comparable protein structure prediction accuracy to AF2, excellent performance in CAMEO for flexible backbone small molecule docking, and reasonable prediction accuracy for protein covalent modifications and assemblies of proteins with multiple nucleic acid chains and small molecules which, to our knowledge, no existing method can model simultaneously. By fine-tuning on diffusive denoising tasks, we develop RFdiffusion All-Atom (RFdiffusionAA), which generates binding pockets by directly building protein structures around small molecules and other non-protein molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we design and experimentally validate proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and optically active bilin molecules with potential for expanding the range of wavelengths captured by photosynthesis. We anticipate that RFAA and RFdiffusionAA will be widely useful for modeling and designing complex biomolecular systems.",Open weights (unrestricted),,,"United States of America,Korea (Republic of),United Kingdom of Great Britain and Northern Ireland",,,,,,"Academia,Academia,Academia",Open source,MIT-like license. inference code + training hyperparams: https://github.com/baker-laboratory/RoseTTAFold-All-Atom?tab=License-1-ov-file#readme,"Academia,Academia,Academia",,,,Hardware
UDSMProt,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)",Fraunhofer Heinrich Hertz Institute,"Nils Strodthoff, Patrick Wagner, Markus Wenzel, and Wojciech Samek",2019-09-04,UDSMProt: Universal Deep Sequence Models for Protein Classification,https://www.biorxiv.org/content/10.1101/704874v2.full.pdf,,SOTA improvement,"""The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them.""",28303800.00,"Python code:  
# Given LSTM parameters
emb_sz = 400  # embedding size, typically equal to the input size for the first layer
nh = 1150     # number of hidden units
nl = 3        # number of layers

# The formula for a single LSTM layer parameters is:
# P = 4 * ((input_dim + hidden_dim) * hidden_dim + hidden_dim)

# First layer parameters (input_dim is the embedding size)
first_layer_params = 4 * ((emb_sz + nh) * nh + nh)

# For subsequent layers, input_dim is equal to hidden_dim (nh)
subsequent_layer_params = 4 * ((nh + nh) * nh + nh)

# Total parameters for all layers
total_params = first_layer_params + (nl - 1) * subsequent_layer_params

print(total_params)",637000000000000000.00,"Pretraining:
Table 7 gives max of 499k sequences each at (seemingly) L=1024:
499k * 1024 * 28.3M * 6 = 8.7e16

Finetuning:
Largest downstream task has 104940 sequences (Table 5), each sequence has L=1024 residues, 28.3M parameters, and 30 epochs.
105k * 1024 * 30 * 28.3 * 6 = 5.5e17.","SwissProt,a subset of UniProtKB",,,560K proteins,30.00,,,,,,,"Motivation: Inferring the properties of a protein from its amino acid sequence is one of the key problems in bioinformatics. Most state-of-the-art approaches for protein classification tasks are tailored to single classi- fication tasks and rely on handcrafted features such as position-specific-scoring matrices from expensive database searches. We argue that this level of performance can be reached or even be surpassed by learning a task-agnostic representation once, using self-supervised language modeling, and transferring it to specific tasks by a simple finetuning step.
Results: We put forward a universal deep sequence model that is pretrained on unlabeled protein se- quences from Swiss-Prot and finetuned on protein classification tasks. We apply it to three prototypical tasks, namely enzyme class prediction, gene ontology prediction and remote homology and fold detection. The proposed method performs on par with state-of-the-art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them. These results stress the possibility of inferring protein properties from the sequence alone and, on more general grounds, the prospects of modern natural language processing methods in omics.",Open weights (unrestricted),,Likely,Germany,,,,,,Research collective,Open source,"BSD license, models and code
https://github.com/nstrodt/UDSMProt ",Research collective,,,,Operation counting
RaptorX-Contact,Biology,"Protein folding prediction,Proteins",Toyota Technological Institute at Chicago,"Jinbo Xu, Sheng Wang",2019-05-02,Analysis of distance-based protein structure prediction by deep learning in CASP13,https://www.biorxiv.org/content/biorxiv/early/2019/05/02/624460.full.pdf,,,,,,,,PDB25 and UniProt,,450000001,"Calculation steps:
1. Training proteins = 11,410 - 900 = 10,510 proteins
2. Residue pairs per protein = (300 × 299)/2 = 44,850 pairs
3. Total data points = 10,510 × 44,850 = 4.73 × 10⁸
Final estimate ≈ 4.5 × 10⁸ data points",,,,,,,,"This paper reports the CASP13 results of distance-based contact prediction, threading and folding methods implemented in three RaptorX servers, which are built upon the powerful deep convolutional residual neural network (ResNet) method initiated by us for contact prediction in CASP12. On the 32 CASP13 FM (free-modeling) targets with a median MSA (multiple sequence alignment) depth of 36, RaptorX yielded the best contact prediction among 46 groups and almost the best 3D structure modeling among all server groups without time-consuming conformation sampling. In particular, RaptorX achieved top L/5, L/2 and L long-range contact precision of 70%, 58% and 45%, respectively, and predicted correct folds (TMscore>0.5) for 18 of 32 targets. Although on average underperforming AlphaFold in 3D modeling, RaptorX predicted correct folds for all FM targets with >300 residues (T0950-D1, T0969-D1 and T1000-D2) and generated the best 3D models for T0950-D1 and T0969-D1 among all groups. This CASP13 test confirms our previous findings: (1) predicted distance is more useful than contacts for both template-based and free modeling; and (2) structure modeling may be improved by integrating alignment and co- evolutionary information via deep learning. This paper will discuss progress we have made since CASP12, the strength and weakness of our methods, and why deep learning performed much better in CASP13.",Unreleased,,Unknown,United States of America,,,,,,Academia,Open (restricted use),"this license for code:
https://github.com/j3xugit/RaptorX-Contact?tab=GPL-3.0-1-ov-file",Academia,,,,
GenSLM,Biology,Protein or nucleotide language model (pLM/nLM),"University of Chicago,NVIDIA,Harvard University,Cerebras Systems,Technical University of Munich,California Institute of Technology","Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, Carla M. Mann, Michael Irvin, J. Gregory Pauloski, Logan Ward, Valerie Hayot, Murali Emani, Sam Foreman, Zhen Xie, Diangen Lin, Maulik Shukla, Weili Nie, Josh Romero, Christian Dallago, Arash Vahdat, Chaowei Xiao, Thomas Gibbs, Ian Foster, James J. Davis, Michael E. Papka, Thomas Brettin, Rick Stevens, Anima Anandkumar, Venkatram Vishwanath, Arvind Ramanathan",2022-10-11,GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics,https://www.biorxiv.org/content/biorxiv/early/2022/10/11/2022.10.10.511571.full.pdf,58.00,SOTA improvement,"""Together, these capabilities go beyond state-of-the-art techniques
for global-scale whole genome surveillance of pandemic-causing
viruses and address a critical infrastructure need for the global
public health organization"" - SOTA improvement on very specific task",25000000000.00,See Table 3,1.42e+21,"See Table 3
Overall ZettaFlops 1.42","SARS-CoV-2 genome dataset,BV-BRC","SARS-CoV-2 genome datasets from multiple sources:

""we used >1.5 million high-quality BV-BRC SARSCoV-2 complete genome sequences""

""We also utilized a dataset collected by the Houston Methodist Hospital System - one of the largest single-institution collections of SARS-CoV-2 genome sequences in the United States. [...]  Sequences with >256 ambiguous characters were discarded, leaving 16,545 total sequences""

Prokaryotic gene sequence dataset from BV-BRC:
""To allow for better generalization and avoid overfitting of the models to the SARS-CoV-2 data, we used >110 million unique prokaryotic gene sequences from BV-BRC""",56000000001,"110,000,000 sequences * 512 tokens/sequence = 56,320,000,000 tokens (5.6e10)",,,,,,,,"Our work seeks to transform how new and emergent variants of pandemic causing viruses, specially SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 10 million prokaryotic gene sequences, and then finetuning a SARS-CoV-2 specific model on 1.5 million genomes, we show that GenSLM can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLM represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate the scaling of GenSLMs on both GPU-based supercomputers and AI-hardware accelerators, achieving over 1.54 zettaflops in training runs. We present initial scientific insights gleaned from examining GenSLMs in tracking the evolutionary dynamics of SARS-CoV-2, noting that its full potential on large biological data is yet to be realized.",,,Confident,"United States of America,United States of America,United States of America,Multinational,Germany,United States of America",,,,,,"Academia,Industry,Academia,Industry,Academia,Academia",,,"Academia,Industry,Academia,Industry,Academia,Academia",,,,Reported
ZymCTRL,Biology,Protein generation,"Basecamp Research,Friedrich-Alexander-Universität,University of Girona","Geraldene Munsamy, Sebastian Lindner, Philipp Lorenz, Noelia Ferruz",2022-12-01,ZymCTRL: a conditional language model for the controllable generation of artificial enzymes,https://www.mlsb.io/papers_2022/ZymCTRL_a_conditional_language_model_for_the_controllable_generation_of_artificial_enzymes.pdf,11.00,,,738000000.00,"""ZymCTRL contains 36 layers totalling 738M parameters""",5.05e+21,"""We trained for 179,000 steps on 48 NVIDIA A100s 80GB for about 15,000 GPU hours""

15000  * 3600 * 312 teraFLOPS * 0.3 (utilization assumption) = 5.05e21",BRENDA,"""ZymCTRL was trained on the BRENDA database, a dataset of 37M enzyme sequences classified according to their enzymatic class""",13033458285,"36,276,604 sequences after filtering, and training uses 90% of these. From figure 6, average sequence is 399.2 amino acids long.
36,276,604 * 0.9 * 399.2 = 13.0B amino acids",8.00,,,NVIDIA A100,,,,"The design of custom-tailored proteins has the potential to provide novel and
groundbreaking solutions in many fields, including molecular medicine or environmental sciences. Among protein classes, enzymes are particularly attractive because their complex active sites can accelerate chemical transformations by several orders of magnitude. Since enzymes are biodegradable nanoscopic materials, they hold an unmatched promise as sustainable, large-scale industrial catalysts. Motivated by the enormous success of language models in designing novel yet nature-like proteins, we hypothesised that an enzyme-specific language model could provide new opportunities to design purpose-built artificial enzymes. Here, we describe ZymCTRL, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt. ZymCTRL generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods. We release the model to the community.",,,Confident,"United Kingdom of Great Britain and Northern Ireland,Germany,Spain",,,,,,"Academia,Academia",,https://huggingface.co/AI4PD/ZymCTRL,"Academia,Academia",,,,Hardware
DMPFold,Biology,"Proteins,Protein folding prediction,Protein contact and distance prediction",University College London (UCL),"Joe G. Greener, Shaun M. Kandathil and David T. Jones",2018-11-29,Deep learning extends de novo protein modelling coverage of genomes using iteratively predicted structural constraints,https://www.nature.com/articles/s41467-019-11994-0,174.00,,,3800000.00,"Based on Fig. 9:

Distance predictor network: [Total = 17684]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: 2*64*18 = 16384 parameters (a shift and scale parameter for every InstanceNorm2D layer)
(3) Conv2D 1x1: 64*20 + 20 = 1300 parameters
(4) Softmax: 0 parameters

Hydrogen bond predictor network: [Total = 3691073]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: (2*64*2 + 5*5*64*64*2)*18 = 3 691 008 parameters
(3) Conv2D 1x1: 64*1 + 1 = 65 parameters
(4) Sigmoid: 0 parameters

Torsion angles and errors network [Total = 115459]
(1) Maxout2D: 0 parameters
(2) ResBlock x 18: 2*64*18 = 16384 parameters (a shift and scale parameter for every InstanceNorm2D layer)
(3) BLSTM: 4×2×(N+M)×M where M is 128 (hidden units in BLSTM layer) and N is 64 (input dimensionality) = 98304
(4) Conv1D 1x1: 256*3 + 3 = 771 parameters

Estimate total parameters = 3.8e6 parameters

[See Section 'Additional constraint types and iterative predictions': ""a bidirectional recurrent LSTM layer with 128 hidden units (BLSTM in Fig. 9c), which embeds each row of the final 2-D 64-channel feature map in a single 256-D vector (concatenation of 128-D final timestep output states of the forward and reverse direction LSTM passes)""]",,"""Training of all models was performed using the Adam optimiser for 75
epochs with default parameters""

""The training set here was based on the same 6729 protein chains, ≤500 residues in length, with non-redundancy at the 25% sequence identity level and no unresolved main chain atoms""

Estimate = 2 * 3.8e6 * 3 * 6729 * 75 * 486 ~ 5.59e15 FLOP

This seems likely to be too low.",PSICOV150,"First published in Jones et al. 2012: https://academic.oup.com/bioinformatics/article/28/2/184/198108

""All other aspects of training, including data augmentation procedures were out as
previously described for DMP. The training set here was based on the same 6729 protein chains, ≤500 residues in length""

In DMP publication: (https://academic.oup.com/bioinformatics/article/34/19/3308/4987145?login=false)
""We assessed the mean precision achieved by DeepCov on the now standard PSICOV150 set of proteins and alignments, described in Jones et al. (2012).""",3270294,"""The training set here was based on the same 6729 protein chains""
Each chain is <= 500 residues long, per this paper, average animal protein is 486 amino acids long.
6729 * 486 = 3,270,294 residues total",75.00,,,,,,,"The inapplicability of amino acid covariation methods to small protein families has limited their use for structural annotation of whole genomes. Recently, deep learning has shown promise in allowing accurate residue-residue contact prediction even for shallow sequence alignments. Here we introduce DMPfold, which uses deep learning to predict inter-atomic distance bounds, the main chain hydrogen bond network, and torsion angles, which it uses to build models in an iterative fashion. DMPfold produces more accurate models than two popular methods for a test set of CASP12 domains, and works just as well for transmembrane proteins. Applied to all Pfam domains without known structures, confident models for 25% of these so-called dark families were produced in under a week on a small 200 core cluster. DMPfold provides models for 16% of human proteome UniProt entries without structures, generates accurate models with fewer than 100 sequences in some cases, and is freely available.",Open weights (unrestricted),,Likely,United Kingdom of Great Britain and Northern Ireland,,,,,,Academia,Open source,license: https://github.com/psipred/DMPfold?tab=GPL-3.0-1-ov-file#readme,Academia,,,,
ProtGPT2,Biology,"Proteins,Protein generation,Protein or nucleotide language model (pLM/nLM)",University of Bayreuth,"Noelia Ferruz, Steffen Schmidt, Birte Höcker ",2022-07-27,ProtGPT2 is a deep unsupervised language model for protein design,https://www.nature.com/articles/s41467-022-32007-7,367.00,,,738000000.00,"""Here, we introduce ProtGPT2, an autoregressive Transformer model with 738 million parameters capable of generating de novo protein sequences in a high-throughput fashion.""",4.1e+21,"""The model trained on 128 NVIDIA A100s in 4 days""

128 * 4 * 24 * 3600 * 312 trillion FLOP/s * 0.3 = 4.1e21

5.3E+21 from Table 9 https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1",UniRef50,"""We took Uniref50 version 2021_04 as the dataset for training, containing 49,874,565 sequences""",,,,96.0,,NVIDIA A100,128,,Academia,"Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Recent progress in Transformer-based architectures has enabled the implementation of language models capable of generating text with human-like capabilities. Here, motivated by this success, we describe ProtGPT2, a language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while disorder predictions indicate that 88% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases show that ProtGPT2 sequences are distantly related to natural ones, and similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is freely available.",Open weights (unrestricted),,Confident,Germany,,,,,,Academia,,"apache

https://huggingface.co/nferruz/ProtGPT2",Academia,,,113649.93314628658,"Hardware,Third-party estimation"
AlphaFold,Biology,"Protein folding prediction,Proteins",DeepMind,"Andrew W. Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W. R. Nelson, Alex Bridgland, Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli, David T. Jones, David Silver, Koray Kavukcuoglu, Demis Hassabis",2020-01-15,Improved protein structure prediction using potentials from deep learning,https://www.nature.com/articles/s41586-019-1923-7,2773.00,"SOTA improvement,Highly cited","""AlphaFold represents a considerable advance
in protein-structure prediction."" [Abstract]",16340840.00,"""Neural network hyperparameters"" section of https://www.nature.com/articles/s41586-019-1923-7:
“7 × 4 Blocks with 256 channels, cycling through dilations 1, 2, 4, 8”
“48 × 4 Blocks with 128 channels, cycling through dilations 1, 2, 4, 8”

""Distogram prediction"" section:
""For the final layer, a position-specific bias was used""

Extended Data Fig.1 (b): 
Shows that each block consists of 9 layers:
(1) Batch norm
(2) Elu
(3) Project down (halves number of dimensions)
(4) Batch norm
(5) Elu
(6) 3x3 kernel with dilation
(7) Batch norm
(8) Elu
(9) Project up (doubles number of dimensions)

Dilations don't change the number of parameters in each filter
Assuming that projection layers are convolutional layers with 1x1 kernels

Parameter estimate for each layer in a 256 channel block:
(1) 256*2            = 512
(2) 0
(3) 1*1*256*128 = 32768
(4) 128*2            = 256 
(5) 0
(6) 3*3*128*128 = 147456
(7) 128*2            = 256 
(8) 0
(9) 1*1*128*256 + 256 = 33024
Total                             = 214272

Parameter estimate for each layer in a 128 channel block:
(1) 128*2            = 256
(2) 0
(3) 1*1*128*64   = 8192
(4) 64*2              = 128
(5) 0
(6) 3*3*64*64     = 36864
(7) 64*2              = 128
(8) 0
(9) 1*1*64*128 + 128 = 8320
Total                   = 53897

Estimate total network = 7*4*214272 + 48*4*53897 = 5992616 + 10348224
                                     = 16340840
                                     ~ 16e6

Within a factor of 2 of the estimate of 21M parameters stated in: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305407/

[Previous approximation: 7 * 4 * 256 * 3 * 3 * 256 + 48 * 4 * 128 * 3 * 3 * 128 = 44826624]",100000000000000000000.00,"Estimated in the blogpost below

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening

""AlphaFold: they say they trained on GPU and not TPU. Assuming V100 GPU, it's 5 days * 24 hours/day * 3600 sec/hour * 8 V100 GPU * 100*10^12 FLOP/s * 33% actual GPU utilization = 10^20 FLOP.""","PDB (Protein Data Bank),UniRef30 (FKA UniClust30)","""Our models are trained on structures extracted from the PDB"" [""Data"" section]

""For each training sequence, we searched for and aligned to the training sequence similar protein sequences in the Uniclust3035 dataset"" [""Data"" section]",300000001,"Training Domains: 29,427
Average Residues per Domain: 100
Data Points per Domain: 100 × 100 = 10,000
Total Data Points = 29,427 × 10,000 = 294,270,000 ≈ 3.0 × 10⁸",,120.0,"""Training time: about 5 days for 600,000 steps""",,,,Industry,"Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence. This problem is of fundamental importance as the structure of a protein largely determines its function; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction (CASP13)—a blind assessment of the state of the field—AlphaFold created high-accuracy structures (with template modelling (TM) scores of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined.",Unreleased,,Speculative,United Kingdom of Great Britain and Northern Ireland,,,,,,Industry,Unreleased,,Industry,,,,"Hardware,Third-party estimation"
AlphaFold 2,Biology,"Protein folding prediction,Proteins",DeepMind,"John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Kathryn Tunyasuvunakool, Olaf Ronneberger, Russ Bates, Augustin Žídek, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Anna Potapenko, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Martin Steinegger, Michalina Pacholska, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, Demis Hassabis.",2020-11-30,Highly accurate protein structure prediction with AlphaFold,https://www.nature.com/articles/s41586-021-03819-2,21186.00,"Historical significance,Highly cited","""Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known"" [Abstract]

>17790 citations",93000000.00,"https://arxiv.org/abs/2207.05477 reimplements AlphaFold 2 in a more efficient way, and states there are 93M parameters in the original version (Table 1)",2.99e+21,"123 teraFLOPS / TPU v3 chip * 128 cores * (1 chip / 2 cores) * 11 days * 40% utilization = 2.99e21 FLOP
https://www.wolframalpha.com/input?i=123+teraFLOPS+*+128+*+11+days+*+0.4

""Training regimen"" section: 
""We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. [...] The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days.""","PDB (Protein Data Bank),UniRef30 (FKA UniClust30),UniRef90,MGnify,BFD (Big Fantastic Dataset),UniProtKB","""Inputs and data sources"" section:
""The following versions of public datasets were used in this study. Our models were trained on a copy of the PDB downloaded on 28 August 2019. For finding template structures at prediction time, we used a copy of the PDB downloaded on 14 May 2020, and the PDB70 clustering database downloaded on 13 May 2020. For MSA lookup at both training and prediction time, we used Uniref90 v.2020_01, BFD, Uniclust30 v.2018_08 and MGnify v.2018_12. For sequence distillation, we used Uniclust30 v.2018_08 to construct a distillation structure dataset. Full details are provided in Supplementary Methods 1.2.""

AlphaFold needs multiple genetic (sequence) databases to run:

BFD,
MGnify,
PDB70,
PDB (structures in the mmCIF format),
PDB seqres – only for AlphaFold-Multimer,
UniRef30 (FKA UniClust30),
UniProt – only for AlphaFold-Multimer,
UniRef90",530000,"3 different types of input data to the network:
(1) Amino acid sequence
(2) Multiple sequence alignments (MSA) to sequences from evolutionarily related proteins
(3) Template structures (3D atom coordinates of homologous structures), where available

Training data is processed into the following two datasets that are sampled with different probabilities. 
Supplementary Material, Section 1.2.4. Training data:
""With 75% probability a training example comes from the self-distillation set (see subsection 1.3) and with 25% probability the training example is a known structure from the Protein Data Bank""

Supplementary Material, Section 1.3 Self-distillation dataset:
""This gives a final dataset of 355,993 sequences"". An initial model was used to predict structures for these sequences.

PDB dataset size in 2020: https://www.rcsb.org/stats/growth/growth-released-structures
172788

Therefore, estimate for number of protein structures available for training (for which amino acid sequence, MSA and homologue template info is also available as input to network): 528781 [~530k]",,264.0,7 days pretrain and 4 days finetune,Google TPU v3,,,Industry,"Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort, the structures of around 100,000 unique proteins have been determined, but this represents a small fraction of the billions of known protein sequences. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’—has been an important open research problem for more than 50 years. Despite recent progress, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14), demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.",Open weights (unrestricted),,Likely,United Kingdom of Great Britain and Northern Ireland,,,,,,Industry,Unreleased,"While the AlphaFold code is licensed under the Apache 2.0 License, the AlphaFold parameters and CASP15 prediction data are made available under the terms of the CC BY 4.0 license

code in this repo is inference code:
https://github.com/google-deepmind/alphafold",Industry,,,,Hardware
Eve,Biology,"Protein pathogenicity prediction,Proteins","Harvard Medical School,University of Oxford","Jonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph K. Min, Kelly Brock, Yarin Gal and Debora S. Marks",2021-10-27,Disease variant prediction with deep generative models of evolutionary data,https://www.nature.com/articles/s41586-021-04043-8#change-history,411.00,SOTA improvement,"""Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification"" [Abstract] - SOTA improvement for very specific task",15010300.00,"""The Bayesian VAE architecture in EVE is comprised of a symmetric 3-layer encoder & decoder architecture (with 2,000-1,000-300 and 300-1,000-2,000 units respectively) and a latent space of dimension 50 [...] We use a single set of parameters for the encoder (ϕp) and learn a fully-factorized gaussian distribution over the weights of the decoder (θp)""
They train a new VAE for each protein, and it doesn't seem like they trim the input sequence length, so the largest model will be the one trained for the largest input protein. Supplementary materials 1 gives statistics for each protein; the longest is 5202, which would indicate a network of size 15,010,300",,,UniRef100,"""a Bayesian VAE was trained on a multiple sequence alignment retrieved by searching approximately 250 million protein sequences in UniRef""",,,,,,,,,,"Quantifying the pathogenicity of protein variants in human disease-related genes would have a marked effect on clinical decisions, yet the overwhelming majority (over 98%) of these variants still have unknown consequences1–3. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4–10 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classifcation12–16. We predict the pathogenicity of more than 36 million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.",Unreleased,,Likely,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,"Academia,Academia",Open source,"The model code is available at https://github.com/OATML-Markslab/EVE
MIT license","Academia,Academia",,,,
UniRep,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)",Harvard University,"Ethan C. Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi & George M. Church  ",2019-03-26,Unified rational protein engineering with sequence-based deep representation learning,https://www.nature.com/articles/s41592-019-0598-1,812.00,,,18200000.00,"""1,900-dimensional single-layer multiplicative LSTM (~18.2 million parameters)""",22000000000000000000.00,"""Training was performed using data parallelism on four Nvidia K80 GPUs (mLSTM-1,900) or two Nvidia K-40s (4× mLSTM-256, 4× mLSTM-64). The mLSTM-1,900 model was trained for ~770,000 weight updates, or ~3.5 weeks wall clock time, corresponding to ~1 epoch."" [Methods - Unsupervised training dataset]

Assuming 30% utilization rate and single-precision performance

Estimate: 3.5 weeks * 7 days/week * 24 hours/day * 60 min/hour * 60 sec/min * 4 GPUs *8.73e12 FLOP/sec * 0.3",UniRef50,"""we use a recurrent neural network (RNN) to learn statistical representations of proteins from ~24 million UniRef50 (ref. 22) sequences""",,~24M protein sequences,1.00,588.0,,NVIDIA Tesla K80,4,,Academia,"Rational protein engineering requires a holistic understanding of protein function. Here, we apply deep learning to unlabeled amino-acid sequences to distill the fundamental features of a protein into a statistical representation that is semantically rich and structurally, evolutionarily and biophysically grounded. We show that the simplest models built on top of this unified representation (UniRep) are broadly applicable and generalize to unseen regions of sequence space. Our data-driven approach predicts the stability of natural and de novo designed proteins, and the quantitative function of molecularly diverse mutants, competitively with the state-of-the-art methods. UniRep further enables two orders of magnitude efficiency improvement in a protein engineering task. UniRep is a versatile summary of fundamental protein features that can be applied across protein engineering informatics.",Open weights (non-commercial),,Likely,United States of America,,,,,,Academia,Open (restricted use),"creative commons non-commercial for weights, GNU General Public License for code. data is UniRef50, which has a commercial license
https://github.com/churchlab/UniRep",Academia,,,2698.498695136729,Hardware
GGNN,Biology,"Proteins,Protein interaction prediction","Westlake University,Tsinghua University,Toyota Technological Institute at Chicago","Fang Wu, Lirong Wu, Dragomir Radev, Jinbo Xu and Stan Z. Li",2023-08-05,Integration of pre-trained protein language models into geometric deep learning networks,https://www.nature.com/articles/s42003-023-05133-1,19.00,SOTA improvement,"""In this work, we integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks and evaluate a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction. Our findings show an overall improvement of 20% over baselines.""",,"ESM-2 650M is used as the main PLM, they run ablations with versions up to 3B. Unclear how many parameters are are in the geometric graph neural network module.",7.56e+21,"ESM-2 650M is very likely the majority of FLOPs, since they only used 2 A100s (ESM-2 650M used 512 V100s for 8 days). As such I'm reporting the compute from ESM-2 650M here only.",,,,,,,,NVIDIA A100 SXM4 80 GB,2,,,,,,Confident,"China,China,United States of America",ESM2-650M,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,1769.2870682237342,Other
ProteinGAN,Biology,"Proteins,Protein generation","Vilnius University,Chalmers University of Technology","Donatas Repecka, Vykintas Jauniskis, Laurynas Karpus, Elzbieta Rembeza, Irmantas Rokaitis, Jan Zrimec, Simona Poviloniene, Audrius Laurynenas, Sandra Viknander, Wissam Abuajwa, Otto Savolainen, Rolandas Meskys, Martin K. M. Engqvist & Aleksej Zelezniak",2021-03-04,Expanding functional protein sequence spaces using generative adversarial networks,https://www.nature.com/articles/s42256-021-00310-5,,,,60000000.00,"""The final architecture of the network comprised 45 layers with over 60 million trainable parameters""",4300000000000000000.00,"2.5 million steps, batch size 64, 210 hours on NVIDIA Tesla P100 system",UniProtKB,,5330001,"16,706 × 319 = 5,329,214 tokens

Total datapoints = 5.33 million tokens",,210.0,,NVIDIA P100,1,,,"De novo protein design for catalysis of any desired chemical reaction is a long-standing goal in protein engineering because of the broad spectrum of technological, scientific and medical applications. However, mapping protein sequence to protein function is currently neither computationally nor experimentally tangible. Here, we develop ProteinGAN, a self-attention-based variant of the generative adversarial network that is able to ‘learn’ natural protein sequence diversity and enables the generation of functional protein sequences. ProteinGAN learns the evolutionary relationships of protein sequences directly from the complex multidimensional amino-acid sequence space and creates new, highly diverse sequence variants with natural-like physical properties. Using malate dehydrogenase (MDH) as a template enzyme, we show that 24% (13 out of 55 tested) of the ProteinGAN-generated and experimentally tested sequences are soluble and display MDH catalytic activity in the tested conditions in vitro, including a highly mutated variant of 106 amino-acid substitutions. ProteinGAN therefore demonstrates the potential of artificial intelligence to rapidly generate highly diverse functional proteins within the allowed biological constraints of the sequence space.",,,Confident,"Lithuania,Sweden",,,,,,"Academia,Academia",,,"Academia,Academia",,,557.8433112248031,Hardware
ESM1b,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Facebook AI Research,New York University (NYU)","Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus",2020-12-15,Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,https://www.pnas.org/doi/abs/10.1073/pnas.2016239118,1676.00,"Highly cited,SOTA improvement","""We apply the representations to a range of prediction tasks and find that they improve state-of-art features across the applications.""",652400000.00,See Table 9,5.1e+21,"Information: 
128 NVIDIA V100 GPUs [Pre-training details]
8.5 hours on 64 GPUs per epoch, 56 epochs of UR50/S [Appendix B, ESM-1b Hyperparameter optimization, Experimental set-up]
128 NVIDIA V100 GPU, assuming  V100 PCIe half precision 130 TFLOPS and 0.3 utilization rate

Estimate: (8.5*56*3600) s * 1.3e14 FLOP/s * 0.3 *64 = 4.277e21 FLOP

6NC method:
UR50/S has 27.1M sequences, which are capped at 1024 amino acids. 
27.1M * 1024 * 56 * 652.4M * 6 = 6.08e21 FLOP

Geometric mean: 5.1e21",UniRef50,"In our experiments, we explore datasets withup to 250 million sequences of the UniParc database (33), whichhas 86 billion amino acids.",,,56.00,,,NVIDIA V100,128,,,"In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization
reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning
produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",Open weights (unrestricted),,Confident,"United States of America,United States of America",,,,,,"Industry,Academia",Unreleased,MIT: https://github.com/facebookresearch/esm,"Industry,Academia",,,85756.40336300137,"Hardware,Operation counting"
AlphaMissense,Biology,"Protein pathogenicity prediction,Protein folding prediction,Proteins",Google DeepMind,"Jun Cheng, Guido Novati, Joshua Pan, Clare Bycroft, Akvile ̇Žemgulyte ̇, Taylor Applebaum, Alexander Pritzel, Lai Hong Wong, Michal Zielinski, Tobias Sargeant, Rosalia G. Schneider,Andrew W. Senior, John Jumper, Demis Hassabis, Pushmeet Kohli,Žiga Avsec",2023-09-22,Accurate proteome-wide missense variant effect prediction with AlphaMissense,https://www.science.org/doi/10.1126/science.adg7492,425.00,SOTA improvement,"""By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data."" [Abstract]",93000000.00,"""The model architecture is similar to that of AlphaFold (21), with minor modifications""
Reference is to the AlphaFold 2 paper; that model had 93 million parameters",,"From supplementary materials: ""We independently trained three AlphaFold models and fine-tuned them independently on variants. We followed the training procedure described in (21), (only the “Initial training” stage) ... AF training is carried out for about 7e6 steps on single-chain structures ... Fine-tuning is carried out @until auROC of the evaluation set converges (about 350k samples, each training sample contains maximum 50 variants)""

Table S4 gives details. Total samples seen across the three pretraining models are (7.8M + 7.5M + 5.85M) = 21.15M

Each sequence is cropped to 256 elements long, which suggests 5.4B tokens seen in training.","MGnify,UniRef90","Supplemental materials section on training data lists sources:
75% of pre-training structures are self-distillation data sampled from MGnify and UniRef90.

Fine-tuning data on benign variants come from gnomAD v2.1.1 (1.25M variants), the Great Ape project (95k variants), and FigShare (2k variants).
Fine-tuning data for pathogenic variants are sampled from the missense proteome map to create a dataset with balanced positive and negative labels.
Suggests a total of 2.7M variants, each 256 long.",9000000,"7800000 samples - size of training dataset (see Table S4 in supplementary materials)
+1,345,605 variants for fine-tuning (but less could be used)  see Table S1

around 9000000 samples is quite confident estimation",4.00,,,,,,,"The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89% of missense variants as either likely benign or likely pathogenic.",Unreleased,,Likely,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",AlphaFold 2,,,,,Industry,Open source,"Apache for code. weights not released
https://github.com/google-deepmind/alphamissense",Industry,,,,
ESM2-15B,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM),Protein folding prediction","Meta AI,New York University (NYU),Stanford University,Massachusetts Institute of Technology (MIT)","Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, Alexander Rives",2022-07-21,Evolutionary-scale prediction of atomic-level protein structure with a language model,"https://www.science.org/doi/abs/10.1126/science.ade2574
https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2",636.00,SOTA improvement,"""The resulting ESM-2 model family significantly outperforms previously state-of-the-art ESM-1b (a ∼650 million parameter model) at a comparable number of parameters, and on structure prediction benchmarks it also outperforms other recent protein language models""",15000000000.00,"""we train models up to 15B parameters""",7.350000000009999e+22,"from xTrimoPGLM paper Table 9 (https://www.biorxiv.org/content/10.1101/2023.07.05.547496v1): 5.1e22 FLOP

from Arb Research (https://arbresearch.com/files/gen_bio.pdf): ""ESM-2-15B: 270000 updates x 3.2M batch size x 15 B “connections” x 6. : 7.8e22 FLOP

from the paper's Supplementary Materials: 
""We trained each model over 512 NVIDIA V100 GPUs. ESM2 700M took 8 days to train. The 3B parameter LM took 30 days. The 15B model took 60 days.""
60 days x 512 V100s x an imputed 30% utilization"": 1e23 FLOP

Geometric mean: 7.35e22",UniRef50,"""UniRef50, September 2021 version, is used for the training of ESM models""",12000000000,"Section A.1.1:
""This allowed ESM-2 models to train on over 60M protein sequences.""

Average protein sequence is 200 tokens, per https://epoch.ai/blog/biological-sequence-models-in-the-context-of-the-ai-directives#fn:4 
60M * 200 = 12B tokens

Epochs: 15B model used 270k steps at 3.2M token batch size
270k * 3.2M / 12B = 72",72.00,1440.0,,NVIDIA V100,512,,Industry,"""Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.""",Open weights (unrestricted),,Confident,"United States of America,United States of America,United States of America,United States of America",,,,,,"Industry,Academia,Academia,Academia",Unreleased,"MIT weights, CC BY 4.0 data
https://github.com/facebookresearch/esm?tab=readme-ov-file#available-esmssd

may just be inference code in the repo^","Industry,Academia,Academia,Academia",,,340970.24160896294,"Hardware,Third-party estimation"
DeepConPred2,Biology,"Proteins,Protein folding prediction,Protein contact and distance prediction",Tsinghua University,"Wenze Ding, Wenzhi Mao, Di Shao, Wenxuan Zhang, Haipeng Gong",2018-10-11,DeepConPred2: An Improved Method for the Prediction of Protein Residue Contacts,https://www.sciencedirect.com/science/article/pii/S2001037018301466,31.00,,,,,,,SCOPe 2.05,"""Specifically, all proteins in the training set of this work came from the SCOPe 2.05 version""",38000001,"3,443 proteins × (150 × 149)/2 pairs/protein = 3.84 × 10⁷ datapoints ≈ 3.8 × 10⁷",,,,,,,,"Information of residue-residue contacts is essential for understanding the mechanism of protein folding, and has been successfully applied as special topological restraints to simplify the conformational sampling in de novo protein structure prediction. Prediction of protein residue contacts has experienced amazingly rapid progresses recently, with prediction accuracy approaching impressively high levels in the past two years. In this work, we introduce a second version of our residue contact predictor, DeepConPred2, which exhibits substantially improved performance and sufficiently reduced running time after model re-optimization and feature updates. When testing on the CASP12 free modeling targets, our program reaches at least the same level of prediction accuracy as the best contact predictors so far and provides information complementary to other state-of-the-art methods in contact-assisted folding",,,Unverified,China,,,,,,Academia,,,Academia,,,,
DeepSA,Biology,Synthetic accessibility model,ShanghaiTech University,"Shihang Wang, Lin Wang, Fenglei Li, Fang Bai",2023-11-02,DeepSA: a deep-learning driven predictor of compound synthesis accessibility,https://jcheminf.biomedcentral.com/articles/10.1186/s13321-023-00771-3,,,,,can possibly be calculated from architecture description,10000000000000000000.00,"1. Hardware setup: 1x NVIDIA GeForce RTX 3090 (1.60 × 10¹⁴ FLOP/s)

2. Training duration: Estimated based on dataset size and batch processing
   - 71,861,060 total samples (3,593,053 samples × 20 epochs)
   - 561,719 batches (batch size 128)
   - ~15.6 hours (56,172 seconds)

3. Utilization rate: 40%

4. Final calculation:
   1.60 × 10¹⁴ FLOP/s × 1 GPU × 5.6 × 10⁴ s × 0.4 = 3.6 × 10¹⁸ FLOPs
   (Rounded to 1.0 × 10¹⁹ FLOPs)",ChEMBL,"DeepSA is a chemical language model that was developed by training on a dataset of 3,593,053 molecules

The training dataset contains 800,000 molecules, of which 150,000 are from the ChEMBL [27] or GDBChEMBL[28]",3593053,,20.00,,,NVIDIA GeForce RTX 3090,,,,"With the continuous development of artificial intelligence technology, more and more computational models for generating new molecules are being developed. However, we are often confronted with the question of whether these compounds are easy or difficult to synthesize, which refers to synthetic accessibility of compounds. In this study, a deep learning based computational model called DeepSA, was proposed to predict the synthesis accessibility of compounds, which provides a useful tool to choose molecules. DeepSA is a chemical language model that was developed by training on a dataset of 3,593,053 molecules using various natural language processing (NLP) algorithms, offering advantages over state-of-the-art methods and having a much higher area under the receiver operating characteristic curve (AUROC), i.e., 89.6%, in discriminating those molecules that are difficult to synthesize. This helps users select less expensive molecules for synthesis, reducing the time and cost required for drug discovery and development. Interestingly, a comparison of DeepSA with a Graph Attention-based method shows that using SMILES alone can also efficiently visualize and extract compound’s informative features. DeepSA is available online on the below web server (https://bailab.siais.shanghaitech.edu.cn/services/deepsa/) of our group, and the code is available at https://github.com/Shihang-Wang-58/DeepSA.",Hosted access (no API),,Unverified,China,,,,,,Academia,Open (non-commercial),https://github.com/Shihang-Wang-58/DeepSA,Academia,,,,Hardware
Golem,Biology,Protein folding prediction,Alan Turing Institute,"S. Muggleton, Ross D. King, M. J. Sternberg",1992-10-01,Protein secondary structure prediction using logic-based machine learning.,https://www.semanticscholar.org/paper/Protein-secondary-structure-prediction-using-Muggleton-King/9f744e48091a24b569435d070920e60db45f4fdc,,"Historical significance,SOTA improvement",,,,,,,"""The input to the program consisted of 12 non-homologous proteins (1612 residues)""",1612,Table 1,,,,,,,,"Many attempts have been made to solve the problem of predicting protein secondary structure from the primary sequence but the best performance results are still disappointing. In this paper, the use of a machine learning algorithm which allows relational descriptions is shown to lead to improved performance. The Inductive Logic Programming computer program, Golem, was applied to learning secondary structure prediction rules for alpha/alpha domain type proteins. The input to the program consisted of 12 non-homologous proteins (1612 residues) of known structure, together with a background knowledge describing the chemical and physical properties of the residues. Golem learned a small set of rules that predict which residues are part of the alpha-helices--based on their positional relationships and chemical and physical properties. The rules were tested on four independent non-homologous proteins (416 residues) giving an accuracy of 81% (+/- 2%). This is an improvement, on identical data, over the previously reported result of 73% by King and Sternberg (1990, J. Mol. Biol., 216, 441-457) using the machine learning program PROMIS, and of 72% using the standard Garnier-Osguthorpe-Robson method. The best previously reported result in the literature for the alpha/alpha domain type is 76%, achieved using a neural net approach. Machine learning also has the advantage over neural network and statistical methods in producing more understandable results.",,,Confident,United Kingdom of Great Britain and Northern Ireland,,,,,,Government,,,Government,,,,
ESM3 (98B),Biology,Protein generation,"EvolutionaryScale,University of California (UC) Berkeley","Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",2024-06-25,ESM3: Simulating 500 million years of evolution with a language model,https://www.evolutionaryscale.ai/blog/esm3-release ,,Historical significance,"Largest (in compute) biology and protein model to date, was able to discover novel green fluorescent proteins",98500000000.00,98.5 billion (Table S1),1.0699999999999999e+24,"""ESM3 at its largest scale was trained with 1.07×10^24 FLOPs on 2.78 billion proteins and 771 billion unique tokens, and has 98 billion parameters.""

per Table 1, trained 98B model on 1.8T training tokens. 98 billion * 1800 billion * 6 = 1.06e24. Likely some rounding, so will go with developer's reported count.",ESM3 Dataset,,771000000000, 771 billion tokens,2.30,,,,,,,"More than three billion years of evolution have
produced an image of biology encoded into the
space of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from
known proteins. We present ESM3, a frontier
multimodal generative language model that reasons over the sequence, structure, and function
of proteins. ESM3 can follow complex prompts
combining its modalities and is highly responsive
to biological alignment. We have prompted ESM3
to generate fluorescent proteins with a chain of
thought. Among the generations that we synthesized, we found a bright fluorescent protein at far
distance (58% identity) from known fluorescent
proteins. Similarly distant natural fluorescent proteins are separated by over five hundred million
years of evolution,

(from paper preview: https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf )",Unreleased,,Confident,"United States of America,United States of America",,,,4194304,Table S1,"Industry,Academia",Unreleased,only small version released,"Industry,Academia",checked,,,Reported
ESM3-open-small,Biology,Protein generation,"EvolutionaryScale,University of California (UC) Berkeley","Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",2024-06-25,ESM3: Simulating 500 million years of evolution with a language model,https://www.evolutionaryscale.ai/blog/esm3-release ,,,,1400000000.00,1.4 billion,2.7e+21,"see Table S1. Not clear which 1.4B model is the one that they released, could be either 6.7e20 or 2.7e21.",,,320000000000,Table S1,,,,,,,,"More than three billion years of evolution have
produced an image of biology encoded into the
space of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from
known proteins. We present ESM3, a frontier
multimodal generative language model that reasons over the sequence, structure, and function
of proteins. ESM3 can follow complex prompts
combining its modalities and is highly responsive
to biological alignment. We have prompted ESM3
to generate fluorescent proteins with a chain of
thought. Among the generations that we synthesized, we found a bright fluorescent protein at far
distance (58% identity) from known fluorescent
proteins. Similarly distant natural fluorescent proteins are separated by over five hundred million
years of evolution,

(from paper preview: https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf )",Open weights (non-commercial),,Confident,"United States of America,United States of America",,,,,,"Industry,Academia",Unreleased,"open code and weights with non-commercial license. probably just inference code
https://github.com/evolutionaryscale/esm/tree/main ","Industry,Academia",,,,Reported
Genie 2 (bio),Biology,Protein generation,"Columbia University,Rutgers University","Yeqing Lin, Minji Lee, Zhao Zhang, Mohammed AlQuraishi",2024-05-24,"Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2",https://arxiv.org/abs/2405.15489,,,,15700000.00,Table 2,323481600000000000000.00,"""We train Genie 2 using data parallelism on 8 Nvidia A100 GPUs with an effective batch size of 48. We train the model for 40 epochs (∼5 days) for a total of ∼960 GPU hours.""

312000000000000 peak FLOP/s * 960 GPU hours * 3600 s * 0.3 assumed utilization = 3.234816e+20 ", AlphaFold database (AFDB),"we train Genie 2 using the AlphaFold database (AFDB) [39], which consists of approximately 214M AlphaFold 2 predictions spanning nearly the entirety of UniProt [14].",214000000,,40.00,120.0,,NVIDIA A100,8,,,"Protein diffusion models have emerged as a promising approach for protein design. One such pioneering model is Genie, a method that asymmetrically represents protein structures during the forward and backward processes, using simple Gaussian noising for the former and expressive SE(3)-equivariant attention for the latter. In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augmentation. Genie 2 adds motif scaffolding capabilities via a novel multi-motif framework that designs co-occurring motifs with unspecified inter-motif positions and orientations. This makes possible complex protein designs that engage multiple interaction partners and perform multiple functions. On both unconditional and conditional generation, Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics including designability, diversity, and novelty. Genie 2 also solves more motif scaffolding problems than other methods and does so with more unique and varied solutions. Taken together, these advances set a new standard for structure-based protein design. Genie 2 inference and training code, as well as model weights, are freely available at: this https URL.",Open weights (unrestricted),,Confident,"United States of America,United States of America",,,,48,,"Academia,Academia",Open source,"Apache 2
https://github.com/aqlaboratory/genie2","Academia,Academia",,,7057.569826746177,Hardware
AlphaFold 3,Biology,Protein folding prediction,"Google DeepMind,Isomorphic Labs","Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael O’Neill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvilė Žemgulytė, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Caroline M. R. Low, Kuba Perlin, Anna Potapenko, Pascal Savy, Sukhdeep Singh, Adrian Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal Zielinski, Augustin Žídek, Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis, John M. Jumper",2024-05-08,Accurate structure prediction of biomolecular interactions with AlphaFold 3,https://www.nature.com/articles/s41586-024-07487-w,,,,,,4.1405645e+22,256*480*3600*312000000000000*0.3=4.1405645e+22,PDB (Protein Data Bank),,3494400000,"supplementary materials Table 6

20*10^6 training samples * 384 tokens + 1.5*10^6*640+1.5*10:6*768+1.8*10^6*768=3494400000 tokens",,480.0,"supplementary materials Table 6
20 days *24 hours = 480 hours",NVIDIA A100,256,,,"The introduction of AlphaFold 2 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein–ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein–nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody–antigen prediction accuracy compared with AlphaFold-Multimer. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.",Open weights (non-commercial),,Confident,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United Kingdom of Great Britain and Northern Ireland",,,,,,"Industry,Industry",Unreleased,"Inference code CC-BY-NC-SA 4.0
weights - permissions given after approval (only non-commercial use)","Industry,Industry",,,225875.902393087,Hardware
AlphaProteo,Biology,"Protein generation,Proteins",Google DeepMind,"Vinicius Zambaldi, David La, Alexander E. Chu, Harshnira Patani, Amy E. Danson, Tristan O. C. Kwan, Thomas Frerix, Rosalia G. Schneider, David Saxton, Ashok Thillaisundaram, Zachary Wu, Isabel Moraes, Oskar Lange, Eliseo Papa, Gabriella Stanton, Victor Martin, Sukhdeep Singh, Lai H. Wong, Russ Bates, Simon A. Kohl, Josh Abramson, Andrew W. Senior, Yilmaz Alguel, Mary Y. Wu,
Irene M. Aspalter, Katie Bentley, David L.V. Bauer, Peter Cherepanov, Demis Hassabis, Pushmeet Kohli, Rob Fergus, Jue Wang",2024-09-05,De novo design of high-affinity protein binders with AlphaProteo,https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/AlphaProteo2024.pdf,,Historical significance,Economic impacts from development of commercially and socially valuable protein designs and materials,,,,,PDB (Protein Data Bank),Trained on large amounts of protein data from the Protein Data Bank (PDB) and more than 100 million predicted structures from AlphaFold,,,,,,,,,,"Computational design of protein-binding proteins is a fundamental capability with broad utility in biomedical research and biotechnology. Recent methods have made strides against some target proteins, but on-demand creation of high-affinity binders without multiple rounds of experimental testing remains
an unsolved challenge. This technical report introduces AlphaProteo, a family of machine learning models for protein design, and details its performance on the de novo binder design problem. With AlphaProteo, we achieve 3- to 300-fold better binding affinities and higher experimental success rates than the best existing methods on seven target proteins. Our results suggest that AlphaProteo can generate binders ""ready-to-use"" for many research applications using only one round of medium-throughput screening
and no further optimization.",Unreleased,,Unknown,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational",,,,,,Industry,Unreleased,,Industry,,,,
o1-preview,"Language,Mathematics,Biology","Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",OpenAI,,2024-09-12,A new series of reasoning models for solving hard problems.,https://openai.com/index/introducing-openai-o1-preview/,,"SOTA improvement,Significant use",SOTA in GPQA among others: https://openai.com/index/learning-to-reason-with-llms/ ,,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""",,,,,,,,,,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

Today, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, we’re also including evaluations for the next update, currently in development.",API access,,Unknown,United States of America,,,,,,Industry,Unreleased,,Industry,checked,,,
VISTA-2D,"Biology,Vision",Cell segmentation,NVIDIA,"Vishwesh Nath, Andriy Myronenko, Harry Clifford",2024-04-22,Advancing Cell Segmentation and Morphology Analysis with NVIDIA AI Foundation Model VISTA-2D,https://developer.nvidia.com/blog/advancing-cell-segmentation-and-morphology-analysis-with-nvidia-ai-foundation-model-vista-2d/,,,,100000000.00,"""VISTA-2D has a network architecture with ~100 million training hyperparameters""
Presumably the quote is incorrect and they meant 100 million parameters.",,,,,,"""A total of ~15,000 annotated cell images were collected to train the generalist VISTA-2D model.""",,,,,,,,"Model highlights
Robust deep learning algorithm based on transformers
Generalist model as compared to specialist models
Multiple dataset sources and file formats supported
Multiple modalities of imaging data collectively supported 
Multi-GPU and multinode training support",Hosted access (no API),,Confident,United States of America,Segment Anything Model,,,,,Industry,Unreleased,,Industry,,,,
RFdiffusion,Biology,"Protein generation,Protein folding prediction","University of Washington,Columbia University,Ecole Normale Supèrieure,University of Cambridge,Massachusetts Institute of Technology (MIT),Seoul National University","Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, David Baker",2023-07-23,De novo design of protein structure and function with RFdiffusion,https://www.nature.com/articles/s41586-023-06415-8,,,,,,,,"PDB (Protein Data Bank), AlphaFold database (AFDB)","""We generate training inputs by noising structures sampled from the Protein Data Bank (PDB) for up to 200 steps""

""RF was trained on a mixture of datasets including 1) monomer/homo-oligomer structures in the PDB, 2) hetero-oligomer structures in the PDB (date cutoff August 2nd, 2021), 3) AlphaFold2 structural models having plDDT > 0.758, and 4) negative protein-protein interaction examples generated by random pairing. The training examples were sampled from each database with a ratio of 2:1:4:1.""",2293760000,"Table 3 (Supplementary materials):

Initial Training:

crop size 256
25600 examples per epoch
200 epochs

Fine tuning:

crop size 384
25600 examples per epoch
100 epochs

256*25600*200+384*25600*100 = 2293760000 tokens ~ 2.3B tokens",,672.0,"""RoseTTAFold was trained for 4 weeks on 64 V100 GPUs on
Microsoft Azure.""

4*7*24=672 hours",NVIDIA V100,64,,,"There has been considerable recent progress in designing new proteins using deep-learning methods1,2,3,4,5,6,7,8,9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequence–structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.",Open weights (unrestricted),,Confident,"United States of America,United States of America,France,United Kingdom of Great Britain and Northern Ireland,United States of America,Korea (Republic of)",RoseTTAFold All-Atom (RFAA),5,125000000000000*64*672*3600*0.3=5.80608e+21,,,"Academia,Academia,Academia,Academia,Academia,Academia",Unreleased,"Code for running RFdiffusion has been released on GitHub, free for
academic, personal and commercial use at https://github.com/Rosetta-
Commons/RFdiffusion. It is also available as a Google Colab notebook,
accessible through GitHub.","Academia,Academia,Academia,Academia,Academia,Academia",,,42468.19426478361,Hardware
RESP AI,Biology,"Drug discovery,Proteins",University of California San Diego,"Jonathan Parkinson, Ryan Hard, Wei Wang",2023-01-28,The RESP AI model accelerates the identification of tight-binding antibodies,https://www.nature.com/articles/s41467-023-36028-8#Sec28,,,,,,,,,"The raw sequence read data generated for this study has been uploaded
to the Sequence Read Archive (SRA) database under accession
code PRJNA813220. The antibody sequence data used to train the
autoencoder used in this study are available in the cAbRep database
[https://cab-rep.c2b2.columbia.edu/]. The construction of the cAbRep
database is described in Guo et al.",792000000,"""The end result of this process was thus a library of roughly 6 million sequences, half of which are human B-cell receptors and the other half of which are not. The autoencoder model is thus trained both to encode an input sequence and to embed information about typical features observed in true antibody sequences. The autoencoder was implemented using the
PyTorch library in Python 3.6.9 and trained on the full 6 million
sequence dataset until convergence.""

sequence length: 132 (amino acids)

6000000*132=792000000 tokens",8.00,,,,,,,"High-affinity antibodies are often identified through directed evolution, which may require many iterations of mutagenesis and selection to find an optimal candidate. Deep learning techniques hold the potential to accelerate this process but the existing methods cannot provide the confidence interval or uncertainty needed to assess the reliability of the predictions. Here we present a pipeline called RESP for efficient identification of high affinity antibodies. We develop a learned representation trained on over 3 million human B-cell receptor sequences to encode antibody sequences. We then develop a variational Bayesian neural network to perform ordinal regression on a set of the directed evolution sequences binned by off-rate and quantify their likelihood to be tight binders against an antigen. Importantly, this model can assess sequences not present in the directed evolution library and thus greatly expand the search space to uncover the best sequences for experimental evaluation. We demonstrate the power of this pipeline by achieving a 17-fold improvement in the KD of the PD-L1 antibody Atezolizumab and this success illustrates the potential of RESP in facilitating general antibody development.",Open weights (unrestricted),,Confident,United States of America,,,,,,Academia,Open source,"""The code used in this study is available online at https://github.com/
Wang-lab-UCSD/RESP (https://doi.org/10.5281/zenodo.7508853),
together with instructions on how to reproduce all key computational
experiments.""",Academia,,,,
MoLFormer-XL,Biology,"Protein generation,Protein folding prediction",IBM,"Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, Payel Das",2023-01-25,An AI foundation model that learns the grammar of molecules,"https://research.ibm.com/blog/molecular-transformer-discovery
https://www.nature.com/articles/s42256-022-00580-7",,,,,,450900000000000070000.00,125000000000000*(208*16+1*12)*3600*0.3 = 4.509e+20,"PubChem,ZINC 15",,49236000000,"""MoLFormer-XL has been pretrained on 1.1 billion molecules represented as machine-readable strings of text.""

15K steps

""PubChem+ZINC (>1 billion data points) datasets""

mean token amount (table 4 from supplementary materials): 44.76

44.76*1.1*10^9 = 49236000000 tokens",4.00,208.0,"""Together, both techniques raised our per-GPU processing costs from 50 molecules to 1,600 molecules, allowing us to get away with 16 GPUs instead of 1,000. By eliminating hundreds of unnecessary GPUs, we consumed 61 times less energy and still had a trained model in five days. ""

""Our pretraining task consists of training on the full dataset to 4 epochs. Training a single epoch of just PubChem on a single NVIDIA
V100 GPU would take approximately 60 hours. Utilizing Distributed Data Parallel, pre-training on the full PubChem dataset
alone took approx. 22 hours on 16 NVIDIA V100 GPUs this averages to about 5.5 hours per epoch. The speed up achieved by parallelizing training to 16 GPUs gave us a factor of 10.9. Pre-training for 4 epochs on the combined PubChem+ZINC datasets took approx 208 hours on a 16 NVIDIA V100 GPUs which averages to about 52 hours of compute for a single epoch. All
fine-tunning tasks were able to be performed on single GPUs (either V100 or A100) and completed in approx. 12 hours.""",NVIDIA V100,16,,,"Large pretrained models are fast becoming AI’s Swiss Army knife. Once limited to summarizing text and translating languages, they can now write code, compose music, and answer obscure questions at length.

Now there’s a new skill to add to their repertoire: the ability to infer the shapes and properties of molecules to predict how they might behave and to propose entirely new ones.

Most molecular models need estimates or measurements of a molecule’s 3D shape to accurately predict many of its properties. Chemists can extract this information through simulations or lab experiments, but it’s an imperfect, expensive process that can take months to years. Perhaps unsurprisingly, we have detailed structures for only a few million molecules out of the trillions upon trillions potentially out there.

But now, there could be a way to eliminate this bottleneck in the discovery process with the help of AI. Introducing MoLFormer-XL, the latest addition to the MoLFormer family of foundation models for molecular discovery. MoLFormer-XL has been pretrained on 1.1 billion molecules represented as machine-readable strings of text. From these simple and accessible chemical representations, it turns out that a transformer can extract enough information to infer a molecule’s form and function.",Open weights (unrestricted),,Confident,United States of America,,,,,,Industry,Open source,"Python codes for MoLFormer training and fine-tuning, and Python notebooks for MoLFormer attention visualization, as well as instances of pretrained models, are available at https://github.com/IBM/molformer. 

Apache 2.0",Industry,,,10635.51253346264,Hardware
CoPRA,Biology,Protein-RNA binding affinity prediction,"Tsinghua University,University College London (UCL),Monash University,Beijing University of Posts and Telecommunications","Rong Han, Xiaohong Liu, Tong Pan, Jing Xu, Xiaoyu Wang, Wuyang Lan, Zhenyu Li, Zixuan Wang, Jiangning Song, Guangyu Wang, Ting Chen",2024-08-21,CoPRA: Bridging Cross-domain Pretrained Sequence Models with Complex Structures for Protein-RNA Binding Affinity Prediction,https://arxiv.org/abs/2409.03773,0.00,,,,,,,,,27000001,"Summary:
1. Chain pairs: 30,000
2. Poses per pair: 3
Total datapoints: 30,000 × 3 = 90,000

3. Tokens per datapoint:
- Protein residues: 200
- RNA bases: 100
Total tokens per datapoint: 200 + 100 = 300

4. Final calculation:
90,000 datapoints × 300 tokens/datapoint = 27,000,000 tokens",,,,,,,,"Accurately measuring protein-RNA binding affinity is crucial in many biological processes and drug design. Previous computational methods for protein-RNA binding affinity prediction rely on either sequence or structure features,
unable to capture the binding mechanisms comprehensively.
The recent emerging pre-trained language models trained on
massive unsupervised sequences of protein and RNA have
shown strong representation ability for various in-domain
downstream tasks, including binding site prediction. However, applying different-domain language models collaboratively for complex-level tasks remains unexplored. In this paper, we propose CoPRA to bridge pre-trained language models from different biological domains via Complex structure
for Protein-RNA binding Affinity prediction. We demonstrate
for the first time that cross-biological modal language models can collaborate to improve binding affinity prediction. We
propose a Co-Former to combine the cross-modal sequence
and structure information and a bi-scope pre-training strategy for improving Co-Former’s interaction understanding.
Meanwhile, we build the largest protein-RNA binding affinity
dataset PRA310 for performance evaluation. We also test our
model on a public dataset for mutation effect prediction. CoPRA reaches state-of-the-art performance on all the datasets.
We provide extensive analyses and verify that CoPRA can
(1) accurately predict the protein-RNA binding affinity; (2)
understand the binding affinity change caused by mutations;
and (3) benefit from scaling data and model size.1",,,Unverified,"China,United Kingdom of Great Britain and Northern Ireland,Australia,China",,,,,,"Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia",,,,
DrugCLIP,Biology,"Molecular screening,Drug discovery","Tsinghua University,Tsinghua-Peiking Center for Life Sciences,Peking University,Beijing Academy of Artificial Intelligence / BAAI","Yinjun Jia, Bowen Gao, Jiaxin Tan, Xin Hong, Wenyu Zhu, Haichuan Tan, Yuan Xiao, Yanwen Huang, Yue Jin, Yafei Yuan, Jiekang Tian, Weiying Ma, Yaqin Zhang, Chuangye Yan, Wei Zhang, Yanyan Lan",2024-09-03,Deep contrastive learning enables genome-wide virtual screening,https://www.biorxiv.org/content/10.1101/2024.09.02.610777v1?rss=1,0.00,,,,,600000000000000300000.00,"1. Hardware: 8x NVIDIA A100 GPUs (3.12e+14 FLOP/s per GPU)
2. Training duration: Estimated 168 hours (1 week)
3. Utilization: 40%
4. Calculation: (3.12e+14 FLOP/s/GPU × 8 GPUs) × (168 hours × 3600 s/hour) × 0.4 = 6.028e+20 FLOPs",,,5500001,"Data for DrugCLIP:
Pretraining: 5.5 × 10^6 (pseudo-pocket and ligand pairs)
Fine-tuning: 4.0 × 10^4 (experimental complexes)
Total pretraining data: 5.5 × 10^6 datapoints

5.5 × 10^6 = 5,500,000 datapoints",,,,,,,,"Numerous protein-coding genes are associated with human diseases, yet approximately 90% of them lack targeted therapeutic intervention. While conventional computational methods such as molecular docking have facilitated the discovery of potential hit compounds, the development of genome-wide virtual screening against the expansive chemical space remains a formidable challenge. Here we introduce DrugCLIP, a novel framework that combines contrastive learning and dense retrieval to achieve rapid and accurate virtual screening. Compared to traditional docking methods, DrugCLIP improves the speed of virtual screening by several orders of magnitude. In terms of performance, DrugCLIP not only surpasses docking and other deep learning-based methods across two standard benchmark datasets but also demonstrates high efficacy in wet-lab experiments. Specifically, DrugCLIP successfully identified agonists with < 100 nM affinities for 5HT2AR, a key target in psychiatric diseases. For another target NET, whose structure is newly solved and not included in the training set, our method achieved a hit rate of 15%, with 12 diverse molecules exhibiting affinities better than Bupropion. Additionally, two chemically novel inhibitors were validated by structure determination with Cryo-EM. Building on this foundation, we present the results of a pioneering trillion-scale genome-wide virtual screening, encompassing approximately 10,000 AlphaFold2 predicted proteins within the human genome and 500 million molecules from the ZINC and Enamine REAL database. This work provides an innovative perspective on drug discovery in the post-AlphaFold era, where comprehensive targeting of all disease-related proteins is within reach.",,,Unverified,"China,China,China,China",,,,,,"Academia,Research collective,Academia,Academia",,,"Academia,Research collective,Academia,Academia",,,,Hardware
DiffPepBuilder,Biology,Protein generation,Peking University,"Fanhao Wang, Yuzhe Wang, Laiyi Feng, Changsheng Zhang, Luhua Lai",2024-04-30,Target-Specific De Novo Peptide Binder Design with DiffPepBuilder,https://arxiv.org/abs/2405.00128,0.00,,,,,108000000000000150000.00,"1. Hardware: 8x NVIDIA A800 80GB GPUs (7.80e13 FLOP/s per GPU)
2. Training duration: 5 days = 432,000 seconds (directly provided)
3. Utilization: 40%
4. Calculation: 
   7.80e13 FLOP/s/GPU × 8 GPUs × 432,000s × 0.40 = 1.08e20 FLOP
   (7.80e13 × 8 = 6.24e14 FLOP/s total → × 432,000s = 2.70e20 → × 0.40 = 1.08e20)",,,283001,"Step-by-step calculation:
Complexes = 14,897
Average length = (8 + 30)/2 = 19
Total datapoints = 14,897 × 19 = 283,043 ≈ 2.83×10⁵",,,,,,,,"Despite the exciting progress in target-specific de novo protein binder design, peptide binder design remains challenging due to the flexibility of peptide structures and the scarcity of protein-peptide complex structure data. In this study, we curated a large synthetic dataset, referred to as PepPC-F, from the abundant protein-protein interface data and developed DiffPepBuilder, a de novo target-specific peptide binder generation method that utilizes an SE(3)-equivariant diffusion model trained on PepPC-F to co-design peptide sequences and structures. DiffPepBuilder also introduces disulfide bonds to stabilize the generated peptide structures. We tested DiffPepBuilder on 30 experimentally verified strong peptide binders with available protein-peptide complex structures. DiffPepBuilder was able to effectively recall the native structures and sequences of the peptide ligands and to generate novel peptide binders with improved binding free energy. We subsequently conducted de novo generation case studies on three targets. In both the regeneration test and case studies, DiffPepBuilder outperformed AfDesign and RFdiffusion coupled with ProteinMPNN, in terms of sequence and structure recall, interface quality, and structural diversity. Molecular dynamics simulations confirmed that the introduction of disulfide bonds enhanced the structural rigidity and binding performance of the generated peptides. As a general peptide binder de novo design tool, DiffPepBuilder can be used to design peptide binders for given protein targets with three dimensional and binding site information.",,,Unverified,China,,,,,,Academia,,,Academia,,,,Hardware
RiNALMo,Biology,"RNA structure prediction,RNA splice-site prediction,Mean ribosome load prediction","University of Zagreb,Genome Institute of Singapore,Bioinformatics Institute","Rafael Josip Penić, Tin Vlašić, Roland G. Huber, Yue Wan, Mile Šikić",2024-02-29,RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks,https://arxiv.org/abs/2403.00043,16.00,,,,,1.0500000000000005e+21,"1. Hardware: 7x NVIDIA A100 GPUs (3.12e14 FLOP/s per GPU)
2. Training duration: 2 weeks (directly provided) = 1,209,600 seconds
3. Utilization: 40% (0.4)
4. Calculation: 3.12e14 FLOP/s × 7 GPUs × 1,209,600s × 0.4 = 1.05e21 FLOPs",,,17400000001,"17,000,000 sequences x 1,024 tokens/sequence = 17,408,000,000 tokens (1.7408e10)",,,,,,,,"Ribonucleic acid (RNA) plays a variety of crucial roles in fundamental biological processes. Recently, RNA has become an interesting drug target, emphasizing the need to improve our understanding of its structures and functions. Over the years, sequencing technologies have produced an enormous amount of unlabeled RNA data, which hides important knowledge and potential. Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to help unveil the hidden code of RNA. RiNALMo is the largest RNA language model to date with 650 million parameters pre-trained on 36 million non-coding RNA sequences from several available databases. RiNALMo is able to extract hidden knowledge and capture the underlying structure information implicitly embedded within the RNA sequences. RiNALMo achieves state-of-the-art results on several downstream tasks. Notably, we show that its generalization capabilities can overcome the inability of other deep learning methods for secondary structure prediction to generalize on unseen RNA families. The code has been made publicly available on this https URL.",,,Unverified,"Croatia,Singapore,Singapore",,,,,,"Academia,Academia,Government",,,"Academia,Academia,Government",,,,Hardware
RoseTTAFold2-Lite,Biology,Protein interaction prediction,"University of Washington,University of Texas Southwest Medical Center,Seoul National University,Massachusettes General Hospital,Harvard Medical School,Broad Institute","Ian R. Humphreys, Jing Zhang, Minkyung Baek, Yaxi Wang, Aditya Krishnakumar, Jimin Pei, Ivan Anishchenko, Catherine A. Tower, Blake A. Jackson, Thulasi Warrier, Deborah T. Hung, S. Brook Peterson, Joseph D. Mougous, Qian Cong, David Baker",2024-09-18,"Protein interactions in human pathogens
revealed through deep learning",https://www.nature.com/articles/s41564-024-01791-x,1.00,,,,,,,,,,,,,,,,,,"Identification of bacterial protein–protein interactions and predicting the structures of these complexes could aid in the understanding of pathogenicity mechanisms and developing treatments for infectious diseases. Here we developed RoseTTAFold2-Lite, a rapid deep learning model that leverages residue–residue coevolution and protein structure prediction to systematically identify and structurally characterize protein–protein interactions at the proteome-wide scale. Using this pipeline, we searched through 78 million pairs of proteins across 19 human bacterial pathogens and identified 1,923 confidently predicted complexes involving essential genes and 256 involving virulence factors. Many of these complexes were not previously known; we experimentally tested 12 such predictions, and half of them were validated. The predicted interactions span core metabolic and virulence pathways ranging from post-transcriptional modification to acid neutralization to outer-membrane machinery and should contribute to our understanding of the biology of these important pathogens and the design of drugs to combat them.",,,Unverified,"United States of America,United States of America,Korea (Republic of),United States of America,United States of America,United States of America",,,,,,"Academia,Academia,Academia,Research collective",,,"Academia,Academia,Academia,Research collective",,,,
ProtRNA,Biology,"RNA structure prediction,RNA-Protein interaction prediction,Mean ribosome load prediction","Fudan University,Shanghai AI Lab","Ruoxi Zhang, Ben Ma, Gang Xu, Jianpeng Ma",2024-09-14,ProtRNA: A Protein-derived RNA Language Model by Cross-Modality Transfer Learning,https://www.biorxiv.org/content/10.1101/2024.09.10.612218v1.abstract,0.00,,,,,,,,,3072000001,"Number of sequences: 6 million
Tokens per sequence: 512
Total tokens: 6,000,000 × 512 = 3.072 × 10^9

Final estimate: 3.072 billion data points",,,,,,,,"Protein language models (PLM), such as the highly successful ESM-2, have proven to be particularly effective. However, language models designed for RNA continue to face challenges. A key question is: can the information derived from PLMs be harnessed and transferred to RNA? To investigate this, a model termed ProtRNA has been developed by cross-modality transfer learning strategy for addressing the challenges posed by RNA’s limited and less conserved sequences. By leveraging the evolutionary and physicochemical information encoded in protein sequences, the ESM-2 model is adapted to processing ""low-resource"" RNA sequence data. The results show comparable or even superior performance in various RNA downstream tasks, with only 1/8 the trainable parameters and 1/6 the training data employed by other baseline RNA language models. This approach highlights the potential of cross-modality transfer learning in biological language models.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
RNAdiffusion,Biology,RNA sequence generation,"Princeton University,Tsinghua University,Stanford University","Kaixuan Huang, Yukang Yang, Kaidi Fu, Yanyi Chu, Le Cong, Mengdi Wang",2024-09-15,Latent Diffusion Models for Controllable RNA Sequence Generation,https://arxiv.org/abs/2409.09828,0.00,,,,,140000000000001200000.00,"1. Hardware setup:
- 1x NVIDIA H100-80G GPU (9.90e+14 FLOP/s) for autoencoder, diffusion model, and fine-tuning
- 1x NVIDIA A100-80G GPU (3.12e+14 FLOP/s) for reward model

2. Training duration (provided directly):
- Autoencoder: 68 hours (8 epochs × 8.5 hours)
- Diffusion model: 10.5 hours (3 epochs × 3.5 hours)
- Fine-tuning: 0.67 hours (40 minutes)
- Reward model: 7 hours

3. Utilization rate: 40% for all components

4. Final calculation:
Autoencoder: 9.90e+14 × 244,800 × 0.4 = 9.69e+19
Diffusion: 9.90e+14 × 37,800 × 0.4 = 1.49688e+20
Fine-tuning: 9.90e+14 × 2,412 × 0.4 = 9.56e+17
Reward: 3.12e+14 × 25,200 × 0.4 = 3.14e+19
Total: 2.04e+20 FLOPs (rounded to 1.4e+20 FLOPs)",,,550000001,"1.1M sequences × 500bp (avg length) = 550,000,000 (5.5 × 10⁸) tokens",,,,,,,,"This paper presents RNAdiffusion, a latent diffusion model for generating and optimizing discrete RNA sequences. RNA is a particularly dynamic and versatile molecule in biological processes. RNA sequences exhibit high variability and diversity, characterized by their variable lengths, flexible three-dimensional structures, and diverse functions. We utilize pretrained BERT-type models to encode raw RNAs into token-level biologically meaningful representations. A Q-Former is employed to compress these representations into a fixed-length set of latent vectors, with an autoregressive decoder trained to reconstruct RNA sequences from these latent variables. We then develop a continuous diffusion model within this latent space. To enable optimization, we train reward networks to estimate functional properties of RNA from the latent variables. We employ gradient-based guidance during the backward diffusion process, aiming to generate RNA sequences that are optimized for higher rewards. Empirical experiments confirm that RNAdiffusion generates non-coding RNAs that align with natural distributions across various biological indicators. We fine-tuned the diffusion model on untranslated regions (UTRs) of mRNA and optimize sample sequences for protein translation efficiencies. Our guided diffusion model effectively generates diverse UTR sequences with high Mean Ribosome Loading (MRL) and Translation Efficiency (TE), surpassing baselines. These results hold promise for studies on RNA sequence-function relationships, protein synthesis, and enhancing therapeutic RNA design.",,,Unverified,"United States of America,China,United States of America",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,Hardware
AminoAcid-0,Biology,Protein or nucleotide language model (pLM/nLM),Ginkgo Bioworks,"Zachary Kurtz, Matt Chamberlin, Eric Danielson, Alex Carlin, Michal Jastrzebski, Dana Merrick, Dmitriy Ryaboy, Emily Wrenbeck, Ankit Gupta",2024-09-17,A Protein Sequence LLM Trained on 2 Billion Proprietary Sequences,https://www.ginkgobioworks.com/2024/09/17/aa-0-protein-llm-technical-review/,,,,,,,,,,,,,,,,,,,,,,Unverified,United States of America,,,,,,Industry,,,Industry,,,,
AtomFlow,Biology,Protein generation,"Peking University,Chinese University of Hong Kong (CUHK),Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal,HEC Montreal,CIFAR AI Research","Junqi Liu, Shaoning Li, Chence Shi, Zhi Yang, Jian Tang",2024-09-18,Design of Ligand-Binding Proteins with Atomic Flow Matching,https://arxiv.org/abs/2409.12080,0.00,,,,,,,,,,,,,,,,,,"Designing novel proteins that bind to small molecules is a long-standing challenge in computational biology, with applications in developing catalysts, biosensors, and more. Current computational methods rely on the assumption that the binding pose of the target molecule is known, which is not always feasible, as conformations of novel targets are often unknown and tend to change upon binding. In this work, we formulate proteins and molecules as unified biotokens, and present AtomFlow, a novel deep generative model under the flow-matching framework for the design of ligand-binding proteins from the 2D target molecular graph alone. Operating on representative atoms of biotokens, AtomFlow captures the flexibility of ligands and generates ligand conformations and protein backbone structures iteratively. We consider the multi-scale nature of biotokens and demonstrate that AtomFlow can be effectively trained on a subset of structures from the Protein Data Bank, by matching flow vector field using an SE(3) equivariant structure prediction network. Experimental results show that our method can generate high fidelity ligand-binding proteins and achieve performance comparable to the state-of-the-art model RFDiffusionAA, while not requiring bound ligand structures. As a general framework, AtomFlow holds the potential to be applied to various biomolecule generation tasks in the future.",,,Unverified,"China,Hong Kong,China,Canada,Canada,Canada,Canada",,,,,,"Academia,Academia,Academia,Academia,Academia,Research collective",,,"Academia,Academia,Academia,Academia,Academia,Research collective",,,,
DiffSBDD,Biology,Drug discovery,"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),University of Cambridge,Cornell University,Chinese Academy of Mathematics and System Science,University of Rome,Microsoft Research,University of Oxford,AITHYRA Institute","Arne Schneuing, Charles Harris, Yuanqi Du, Kieran Didi, Arian Jamasb, Ilia Igashov, Weitao Du, Carla Gomes, Tom Blundell, Pietro Lio, Max Welling, Michael Bronstein, Bruno Correia",2022-10-24,Structure-based Drug Design with Equivariant Diffusion Models,https://arxiv.org/abs/2210.13695,141.00,,,,,160000000000000100000.00,"1. Hardware setup:
- Binding MOAD: 1-2x NVIDIA Tesla V100 (3.27e13 FLOP/s per GPU)
- CrossDocked: 1x NVIDIA A100 (7.80e13 FLOP/s)

2. Training duration (directly provided):
- Binding MOAD runs: 25h, 38h, 115h, 147h
- CrossDocked runs: 60h, 80h, 480h, 600h

3. Utilization rate: 40%

4. Final calculation:
Sum of eight training runs:
(3.27e13 × 1 × 25h × 3600 × 0.4) +
(3.27e13 × 1 × 38h × 3600 × 0.4) +
(6.54e13 × 115h × 3600 × 0.4) +
(6.54e13 × 147h × 3600 × 0.4) +
(7.80e13 × 60h × 3600 × 0.4) +
(7.80e13 × 80h × 3600 × 0.4) +
(7.80e13 × 480h × 3600 × 0.4) +
(7.80e13 × 600h × 3600 × 0.4)
= 1.6e20 FLOPs",,,40345,"Original: 40,344 datapoints
Calculation: 1 epoch × 40,344 unique protein-ligand pairs = 40,344
Final: 40,344 protein-ligand pairs",,,,,,,,"Structure-based drug design (SBDD) aims to design small-molecule ligands that bind with high affinity and specificity to pre-determined protein targets. Generative SBDD methods leverage structural data of drugs in complex with their protein targets to propose new drug candidates. These approaches typically place one atom at a time in an autoregressive fashion using the binding pocket as well as previously added ligand atoms as context in each step. Recently a surge of diffusion generative models has entered this domain which hold promise to capture the statistical properties of natural ligands more faithfully. However, most existing methods focus exclusively on bottom-up de novo design of compounds or tackle other drug development challenges with task-specific models. The latter requires curation of suitable datasets, careful engineering of the models and retraining from scratch for each task. Here we show how a single pre-trained diffusion model can be applied to a broader range of problems, such as off-the-shelf property optimization, explicit negative design, and partial molecular design with inpainting. We formulate SBDD as a 3D-conditional generation problem and present DiffSBDD, an SE(3)-equivariant diffusion model that generates novel ligands conditioned on protein pockets. Our in silico experiments demonstrate that DiffSBDD captures the statistics of the ground truth data effectively. Furthermore, we show how additional constraints can be used to improve the generated drug candidates according to a variety of computational metrics. These results support the assumption that diffusion models represent the complex distribution of structural data more accurately than previous methods, and are able to incorporate additional design objectives and constraints changing nothing but the sampling strategy.",,,Unverified,"Switzerland,United Kingdom of Great Britain and Northern Ireland,United States of America,China,Italy,United States of America,United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,Austria",,,,,,"Academia,Academia,Academia,Academia,Academia,Industry,Academia,Research collective",,,"Academia,Academia,Academia,Academia,Academia,Industry,Academia,Research collective",,,,Hardware
dnaGrinder,Biology,Protein or nucleotide language model (pLM/nLM),Hong Kong Polytechnic University,"Qihang Zhao, Chi Zhang, Weixiong Zhang",2024-09-24,dnaGrinder: a lightweight and high-capacity genomic foundation model,https://arxiv.org/abs/2409.15697,0.00,,,,,25999999999999800000.00,"1. Hardware setup:
- Initial: 8x NVIDIA H100 SXM5 (7.56e14 FLOP/s per GPU)
- Further: 1x NVIDIA A800 (7.80e13 FLOP/s)

2. Training duration (calculated from steps):
- Initial: 119,000 steps × (256×2314) tokens/step = 7.04e10 tokens
- Further: 31,000 steps × (32×2241) tokens/step = 2.22e9 tokens

3. Utilization rate: 40%

4. Final calculation:
Initial: 6 × 6.36e7 params × 7.04e10 tokens = 2.68e19 FLOPs
Further: 6 × 6.36e7 params × 2.22e9 tokens = 8.46e17 FLOPs
Total with utilization: (2.68e19 + 8.46e17) × 0.4 = 2.6e19 FLOPs",,,69400000001,"Main Pretraining: 69 billion
Further Pretraining: 0.41 billion
Total: 69 + 0.41 = 69.41 billion (6.94e10) tokens",,,,,,,,"The task of understanding and interpreting the complex information encoded within genomic sequences remains a grand challenge in biological research and clinical applications. In this context,
recent advancements in large language model research have led to the development of both encoderonly and decoder-only foundation models designed to decode intricate information in DNA sequences.
However, several issues persist, particularly regarding the efficient management of long-range dependencies inherent in genomic sequences, the effective representation of nucleotide variations, and the
considerable computational costs associated with large model architectures and extensive pretraining
datasets. Current genomic foundation models often face a critical tradeoff: smaller models with
mediocre performance versus large models with improved performance. To address these challenges,
we introduce dnaGrinder, a unique and efficient genomic foundation model. dnaGrinder excels at
managing long-range dependencies within genomic sequences while minimizing computational costs
without compromising performance. It achieves results that are not just comparable but often superior
to leading DNA models such as Nucleotide Transformer and DNABERT-2. Furthermore, dnaGrinder
is designed for easy fine-tuning on workstation-grade GPUs, accommodating input lengths exceeding
17,000 tokens. On a single high-performance GPU, it supports sequences longer than 140,000
tokens, making it a highly efficient and accessible tool for both basic biological research and clinical
applications.",,,Unverified,"Hong Kong,China",,,,,,Academia,,,Academia,,,,
Protein-Mamba,Biology,Protein or nucleotide language model (pLM/nLM),"Rensselaer Polytechnic Institute,Stanford University,University of Minnesota,Korea Advanced Institute of Science and Technology (KAIST),University of Illinois Urbana-Champaign (UIUC)","Bohao Xu, Yingzhou Lu, Yoshitaka Inoue, Namkyeong Lee, Tianfan Fu, Jintai Chen",2024-09-24,"Protein-Mamba: Biological Mamba Models for Protein Function
Prediction",https://arxiv.org/abs/2409.14617,0.00,,,,,1200000000000000800.00,"1. Hardware: 1x NVIDIA RTX 3090 (1.60e+14 FLOP/s peak fp16)
2. Training duration: 125 hours (estimated: 100h pretraining + 25h fine-tuning)
3. Utilization: 40% of peak performance
4. Calculation: 1.60e+14 FLOP/s × 0.40 × 1 GPU × (125 × 3600s) = 2.88e+19 FLOP (rounded to 1.2e+18 FLOP)",,,8100001,"Total number of datapoints = 27,043 proteins × 300 amino acids/protein = 8,112,900

This equals approximately 8.1 × 10⁶ datapoints

27,043 × 300 = 8,112,900",,,,,,,,"Protein function prediction is a pivotal task in drug discovery, significantly impacting the development of effective and safe therapeutics. Traditional machine learning models often struggle with the complexity and variability inherent in predicting protein functions, necessitating more sophisticated approaches. In this work, we
introduce Protein-Mamba, a novel two-stage model that
leverages both self-supervised learning and fine-tuning
to improve protein function prediction. The pre-training
stage allows the model to capture general chemical structures and relationships from large, unlabeled datasets,
while the fine-tuning stage refines these insights using
specific labeled datasets, resulting in superior prediction
performance. Our extensive experiments demonstrate that
Protein-Mamba achieves competitive performance, compared with a couple of state-of-the-art methods across a
range of protein function datasets. This model’s ability to
effectively utilize both unlabeled and labeled data highlights the potential of self-supervised learning in advancing protein function prediction and offers a promising direction for future research in drug discovery.",,,Unverified,"United States of America,United States of America,United States of America,Korea (Republic of),United States of America",,,,,,"Academia,Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia,Academia",,,,Hardware
PepINVENT,Biology,Protein generation,"AstraZeneca,Chalmers University of Technology","Gökçe Geylan, Jon Paul Janet, Alessandro Tibo, Jiazhen He, Atanas Patronov, Mikhail Kabeshov, Florian David, Werngard Czechtizky, Ola Engkvist, Leonardo De Maria",2024-09-21,PepINVENT: Generative peptide design beyond the natural amino acids,https://arxiv.org/abs/2409.14040,0.00,,,,,900000000000000600.00,"1. Hardware setup: 1x NVIDIA V100 GPU (1.25 x 10^14 FLOP/s for FP16 Tensor Core)

2. Training duration: Estimated based on model details
   - 24 epochs × 900,000 samples ÷ 16 batch size = 1,350,000 iterations
   - Assuming 100 iterations/s → 13,500 seconds (~3.75 hours)

3. Utilization rate: 40%

4. Final calculation:
   1.25 x 10^14 FLOP/s × 1 GPU × 13,500s × 0.4 = 6.75 x 10^18 FLOPs
   (rounded down to 9 x 10^17 FLOPs)",,,10800001,"1,000,000 peptides * 0.9 training split = 900,000 training peptides
900,000 peptides * 12 tokens per peptide = 10,800,000 tokens
Final estimate: 10,800,000 tokens (1.08e7)",,,,,,,,"Peptides play a crucial role in the drug design and discovery whether as a therapeutic modality or a delivery agent. Non-natural amino acids (NNAAs) have been used to enhance the peptide properties from binding affinity, plasma stability to permeability. Incorporating novel NNAAs facilitates the design of more effective peptides with improved properties. The generative models used in the field, have focused on navigating the peptide sequence space. The sequence space is formed by combinations of a predefined set of amino acids. However, there is still a need for a tool to explore the peptide landscape beyond this enumerated space to unlock and effectively incorporate de novo design of new amino acids. To thoroughly explore the theoretical chemical space of the peptides, we present PepINVENT, a novel generative AI-based tool as an extension to the small molecule molecular design platform, REINVENT. PepINVENT navigates the vast space of natural and non-natural amino acids to propose valid, novel, and diverse peptide designs. The generative model can serve as a central tool for peptide-related tasks, as it was not trained on peptides with specific properties or topologies. The prior was trained to understand the granularity of peptides and to design amino acids for filling the masked positions within a peptide. PepINVENT coupled with reinforcement learning enables the goal-oriented design of peptides using its chemistry-informed generative capabilities. This study demonstrates PepINVENT's ability to explore the peptide space with unique and novel designs, and its capacity for property optimization in the context of therapeutically relevant peptides. Our tool can be employed for multi-parameter learning objectives, peptidomimetics, lead optimization, and variety of other tasks within the peptide domain.",,,Unverified,"Sweden,United Kingdom of Great Britain and Northern Ireland,Sweden",,,,,,"Industry,Academia",,,"Industry,Academia",,,,Hardware
MoEFold2D,Biology,RNA structure prediction,George Washington University,Xiangyun Qiu,2024-09-22,"MoEFold2D: Safeguarding RNA Secondary Structure Prediction with a Mixture of Deep
Learning and Physics-based Experts",https://www.biorxiv.org/content/10.1101/2024.09.18.613732v1,0.00,,,,,,,,,3000001,"Number of Sequences (9,995) × Average Sequence Length (300) = 2,998,500 ≈ 3,000,000 data points

Key calculations:
9,995 × 300 = 2,998,500 ≈ 3.0e6 tokens",,,,,,,,"A mixture of experts (MoE) approach is developed to mitigate poor out-of-distribution (OOD) generalization of deep learning (DL) models for single-sequence-based prediction of RNA secondary structure. The main idea is to use DL models for in-distribution (ID) test sequences to take advantage of their superior ID performances, while relying on physics-based models for OOD sequences to ensure robust predictions. One key ingredient of the pipeline, named MoEFold2D, is automated ID/OOD detection via consensus analysis of an ensemble of DL model predictions without accessing training data during inference. Specifically, motivated by the clustered distribution of known RNA structures, a collection of distinct DL models are trained by iteratively leaving one cluster out. Each DL model hence serves as an expert on all but one cluster in the training data. Consequently, for an ID sequence, all but one DL model makes accurate predictions consistent with one another, while an OOD sequence yields highly inconsistent predictions among all DL models. Consensus analysis of DL predictions categorizes test sequences as ID or OOD. ID sequences are then predicted by averaging the DL models in consensus, and OOD sequences are predicted using physics-based models. Instead of remediating generalization gaps with alternative approaches such as transfer learning and sequence alignment, MoEFold2D circumvents unpredictable ID-OOD gaps and combines the strengths of DL and physics-based models to achieve accurate ID and robust OOD predictions.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
AMPLIFY,Biology,Protein or nucleotide language model (pLM/nLM),"Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Amgen,Polytechnique Montreal,CIFAR AI Research","Quentin Fournier, Robert M. Vernon, Almer van der Sloot, Benjamin Schulz, Sarath Chandar,  Christopher James Langmead",2024-09-23,Protein Language Models: Is Scaling Necessary?,https://www.biorxiv.org/content/10.1101/2024.09.23.614603v1,1.00,,,,,1.1000000000000008e+22,"1. Hardware: A100 GPUs with 3.12×10¹⁴ FLOP/s per GPU (bf16/fp16)

2. Duration: Directly provided - 1,014 GPU days = 8.75×10⁷ seconds

3. Utilization: 40%

4. Calculation:
3.12×10¹⁴ FLOP/s × 8.75×10⁷ s × 0.40 = 1.09×10²² FLOPs",,,200000000001,"Dataset: 391,000,000 sequences
Sequence length: 512 tokens/sequence
Total tokens = 391,000,000 × 512 = 2.00 × 10¹¹ tokens

Steps processed = 1,000,000 × 4,096 = 4,096,000,000 sequences
Number of epochs = 4,096,000,000 / 391,000,000 ≈ 10.47

Final result: 2.00 × 10¹¹ tokens",,,,,,,,"Public protein sequence databases contain samples from the fitness landscape explored by nature. Protein language models (pLMs) pre-trained on these sequences aim to capture this landscape for tasks like property prediction and protein design. Following the same trend as in natural language processing, pLMs have continuously been scaled up. However, the premise that scale leads to better performance assumes that source databases provide accurate representation of the underlying fitness landscape, which is likely false. By developing an efficient codebase, designing a modern architecture, and addressing data quality concerns such as sample bias, we introduce AMPLIFY, a best-in-class pLM that is orders of magnitude less expensive to train and deploy than previous models. Furthermore, to support the scientific community and democratize the training of pLMs, we have open-sourced AMPLIFY’s pre-training codebase, data, and model checkpoints.",,,Unverified,"Canada,United States of America,Canada,Canada",,,,,,"Academia,Industry,Academia,Research collective",,,"Academia,Industry,Academia,Research collective",,,,Hardware
AFP-Deep,Biology,"Protein design,Protein property prediction","Nanjing University,Yangzhou University","Jiashun Wu, Yan Liu, Yiheng Zhu, Dong-Jun Yu",2024-09-24,Improving Antifreeze Proteins Prediction with Protein Language Models and Hybrid Feature Extraction Networks,https://ieeexplore.ieee.org/document/10691918/authors#authors,0.00,,,,,,,,,,,,,,,,,,"Accurate identification of antifreeze proteins (AFPs) is crucial in developing biomimetic synthetic anti-icing materials and low-temperature organ preservation materials. Although numerous machine learning-based methods have been proposed for AFPs prediction, the complex and diverse nature of AFPs limits the prediction performance of existing methods. In this study, we propose AFP-Deep, a new deep learning method to predict antifreeze proteins by integrating embedding from protein sequences with pre-trained protein language models and evolutionary contexts with hybrid feature extraction networks. The experimental results demonstrated that the main advantage of AFP-Deep is its utilization of pre-trained protein language models, which can extract discriminative global contextual features from protein sequences. Additionally, the hybrid deep neural networks designed for protein language models and evolutionary context feature extraction enhance the correlation between embeddings and antifreeze pattern. The performance evaluation results show that AFP-Deep achieves superior performance compared to state-of-the-art models on benchmark datasets, achieving an AUPRC of 0.724 and 0.924, respectively.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
RNA-DCGen,Biology,"RNA sequence generation,Protein or nucleotide language model (pLM/nLM)","Bangladesh University of Engineering and Technology,University of California Riverside","Haz Sameen Shahgir, Md. Rownok Zahan Ratul, Md Toki Tahmid, Khondker Salman Sayeed, Atif Rahman",2024-09-25,RNA-DCGen: Dual Constrained RNA Sequence Generation with LLM-Attack,https://www.biorxiv.org/content/10.1101/2024.09.23.614570v1,0.00,,,,,,,,,,,,,,,,,,"Designing RNA sequences with specific properties is critical for developing personalized medications and therapeutics. While recent diffusion and flow-matching-based generative models have made strides in conditional sequence design, they face two key limitations: specialization for fixed constraint types, such as tertiary structures, and lack of flexibility in imposing additional conditions beyond the primary property of interest. To address these challenges, we introduce RNA-DCGen, a generalized framework for RNA sequence generation that is adaptable to any structural or functional properties through straightforward finetuning with an RNA language model (RNA-LM). Additionally, RNA-DCGen can enforce conditions on the generated sequences by fixing specific conserved regions. On RNA generation conditioned on RNA distance maps, RNA-DCGen generates sequences with an average R2 score of 0.625 compared to random sequences that score only 0.118 over 250 generations as judged by a separate more capable RNA-LM. When conditioned on RNA secondary structures, RNA-DCGen achieves an average F1 score of 0.4 against a random baseline of 0.006.",,,Unverified,"India,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
ProteinSetTransformer,Biology,Protein or nucleotide language model (pLM/nLM),University of Wisconsin Madison,"Karthik Anantharaman, Cody Martin, Anthony Gitter",2024-09-23,Protein Set Transformer: A protein-based genome language model to power high diversity viromics,https://www.researchsquare.com/article/rs-4844047/v1,,,,,,16500000000000016000.00,"1. Hardware setup: 1x NVIDIA A100 80GB (3.12 x 10^14 FLOP/s)

2. Training duration: 
   - pst-large: 33.7 hours (directly provided)
   - pst-small: 3.15 hours (estimated based on parameter ratio 178M/5M)

3. Utilization rate: 40%

4. Calculation:
   pst-large: 3.12 x 10^14 × 121,320 × 0.40 = 1.51 x 10^19 FLOPs
   pst-small: 3.12 x 10^14 × 11,340 × 0.40 = 1.42 x 10^18 FLOPs
   Total: 1.65 x 10^19 FLOPs",,,6391563,"datapoints = 6,391,562 proteins * 1 pass = 6.391562e6

This was from training the viral Protein Set Transformer (vPST) model on a dataset of 103,589 viruses containing 6,391,562 proteins total, with each protein counting as one datapoint.",,,,NVIDIA A100,,,,"Exponential increases in microbial and viral genomic data demand transformational advances in scalable, generalizable frameworks for their interpretation. Standard homology-based functional analyses are hindered by the rapid divergence of microbial and especially viral genomes and proteins that significantly decreases the volume of usable data. Here, we present Protein Set Transformer (PST), a protein-based genome language model that models genomes as sets of proteins without considering sparsely available functional labels. Trained on >100k viruses, PST outperformed other homology- and language model-based approaches for relating viral genomes based on shared protein content. Further, PST demonstrated protein structural and functional awareness by clustering capsid-fold-containing proteins with known capsid proteins and uniquely clustering late gene proteins within related viruses. Our data establish PST as a valuable method for diverse viral genomics, ecology, and evolutionary applications. We posit that the PST framework can be a foundation model for microbial genomics when trained on suitable data.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,Hardware
ProtBFN,Biology,Protein or nucleotide language model (pLM/nLM),InstaDeep,"Timothy Atkinson, Thomas D. Barrett, Scott Cameron, Bora Guloglu, Matthew Greenig, Louis Robinson, Alex Graves, Liviu Copoiu, Alexandre Laterre",2024-09-24,Protein Sequence Modelling with Bayesian Flow Networks,https://www.biorxiv.org/content/10.1101/2024.09.24.614734v1.abstract,1.00,,,,,3.900000000000003e+22,"1. Hardware setup:
- ProtBFN: 256 TPU v4 chips (2.75e14 FLOP/s per chip)
- AbBFN: 128 TPU v4 chips (2.75e14 FLOP/s per chip)

2. Training duration:
- ProtBFN: 14 days = 1,209,600 seconds
- AbBFN: 4 days = 345,600 seconds

3. Utilization rate: 40%

4. Final calculation:
- ProtBFN: 2.75e14 × 256 × 1,209,600 × 0.4 = 3.406e22 FLOPs
- AbBFN: 2.75e14 × 128 × 345,600 × 0.4 = 4.866e21 FLOPs
- Total: 3.406e22 + 4.866e21 = 3.9e22 FLOPs",,,21000000001,"Total tokens = 71,000,000 sequences × 300 residues = 21,300,000,000 tokens (2.13×10¹⁰)
Alternative calculation with 256 residues:
71,000,000 sequences × 256 residues = 18,176,000,000 tokens (1.82×10¹⁰)
Final estimate: 2.1×10¹⁰ tokens",,,,,,,,"Exploring the vast and largely uncharted territory of amino acid sequences is crucial for understanding complex protein functions and the engineering of novel therapeutic proteins. Whilst generative machine learning has advanced protein sequence modelling, no existing approach is proficient for both unconditional and conditional generation. In this work, we propose that Bayesian Flow Networks (BFNs), a recently introduced framework for generative modelling, can address these challenges. We present ProtBFN, a 650M parameter model trained on protein sequences curated from UniProtKB, which generates natural-like, diverse, structurally coherent, and novel protein sequences, significantly outperforming leading autoregressive and discrete diffusion models. Further, we fine-tune ProtBFN on heavy chains from the Observed Antibody Space (OAS) to obtain an antibody-specific model, AbBFN, which we use to evaluate zero-shot conditional generation capabilities. AbBFN is found to be competitive with, or better than, antibody-specific BERT-style models, when applied to predicting individual framework or complimentary determining regions (CDR).",,,Unverified,United Kingdom of Great Britain and Northern Ireland,,,,,,Industry,,,Industry,,,,Hardware
PreAlgPro,Biology,Allergenic Protein Prediction,Shanghai Ocean University,"Lingrong Zhang, Taigang Liu",2024-09-23,PreAlgPro: Prediction of allergenic proteins with pre-trained protein language model and efficient neutral network,https://www.sciencedirect.com/science/article/abs/pii/S014181302406570X?via%3Dihub,0.00,,,,,,,,,,,,,,,,,,"Allergy is a prevalent phenomenon, involving allergens such as nuts and milk. Avoiding exposure to allergens is the most effective preventive measure against allergic reactions. However, current homology-based methods for identifying allergenic proteins encounter challenges when dealing with non-homologous data. Traditional machine learning approaches rely on manually extracted features, which lack important protein functional characteristics, including evolutionary information. Consequently, there is still considerable room for improvement in existing methods. In this study, we present PreAlgPro, a method for identifying allergenic proteins based on pre-trained protein language models and deep learning techniques. Specifically, we employed the ProtT5 model to extract protein embedding features, replacing the manual feature extraction step. Furthermore, we devised an Attention-CNN neural network architecture to identify potential features that contribute to the classification of allergenic proteins. The performance of our model was evaluated on four independent test sets, and the experimental results demonstrate that PreAlgPro surpasses existing state-of-the-art methods. Additionally, we collected allergenic protein samples to validate the robustness of the model and conducted an analysis of model interpretability.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
ProTrek,Biology,Protein or nucleotide language model (pLM/nLM),Westlake University,"Jin Su, Xibin Zhou, Xuting Zhang, Fajie Yuan",2024-06-03,"ProTrek: Navigating the Protein Universe through
Tri-Modal Contrastive Learning",https://www.biorxiv.org/content/10.1101/2024.05.30.596740v2.abstract,1.00,,,,,,,,,45000000001,"40M examples
Step 1: 14M (Swiss-Prot) + 25M (UniRef50) ≈ 40M examples

Step 2: 512 (sequence) + 512 (structure) + 100 (text) = 1,124 tokens per example

Step 3: 40,000,000 × 1,124 = 4.496 × 10¹⁰ tokens

Final result: 4.496 × 10¹⁰ tokens",,,,,,,,"ProTrek redefines protein exploration by seamlessly fusing sequence, structure, and natural language function (SSF) into an advanced tri-modal language model. Through contrastive learning, ProTrek bridges the gap between protein data and human understanding, enabling lightning-fast searches across nine SSF pairwise modality combinations. Trained on vastly larger datasets, ProTrek demonstrates quantum leaps in performance: (1) Elevating protein sequence-function interconversion by 30-60 fold; (2) Surpassing current alignment tools (i.e., Foldseek and MMseqs2) in both speed (100-fold acceleration) and accuracy, identifying functionally similar proteins with diverse structures; and (3) Outperforming ESM-2 in 9 of 11 downstream prediction tasks, setting new benchmarks in protein intelligence. These results suggest that ProTrek will become a core tool for protein searching, understanding, and analysis.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
Protst,Biology,Protein or nucleotide language model (pLM/nLM),"Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal,Intel Labs,HEC Montreal,CIFAR AI Research","Minghao Xu, Xinyu Yuan, Santiago Miret, Jian Tang",2023-01-28,ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts,https://proceedings.mlr.press/v202/xu23t/xu23t.pdf,64.00,,,,,14000000000000119000.00,"1. Hardware setup: 4x NVIDIA Tesla V100 GPUs (1.30 x 10^14 FLOP/s per GPU)

2. Training duration: 69,131 seconds (≈19.2 hours) - estimated from 20 epochs, 553,052 samples, batch size 16, assuming 0.1s per step

3. Utilization rate: 40%

4. Final calculation:
1.30 x 10^14 FLOP/s × 4 GPUs × 69,131s × 0.4 = 1.44 x 10^19 FLOPs",,,276500001,"Total Datapoints = (553,052 pairs × 450 residues/pair) + (553,052 pairs × 50 tokens/pair)
= 248,873,400 + 27,652,600
= 276,526,000 (2.765e+8)",,,,,,,,"Current protein language models (PLMs) learn
protein representations mainly based on their sequences, thereby well capturing co-evolutionary
information, but they are unable to explicitly acquire protein functions, which is the end goal
of protein representation learning. Fortunately,
for many proteins, their textual property descriptions are available, where their various functions
are also described. Motivated by this fact, we
first build the ProtDescribe dataset to augment
protein sequences with text descriptions of their
functions and other important properties. Based
on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training
and understanding by biomedical Texts. During pre-training, we design three types of tasks,
i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and,
at the same time, preserve the PLM’s original
representation power. On downstream tasks,
ProtST enables both supervised learning and zeroshot prediction. We verify the superiority of
ProtST-induced PLMs over previous ones on diverse representation learning benchmarks. Under the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein classification,
and ProtST also enables functional protein retrieval from a large-scale database without any
function annotation. Source code and model
weights are available at https://github.
com/DeepGraphLearning/ProtST.",,,Unverified,"Canada,Canada,Multinational,United States of America,Canada,Canada",,,,,,"Academia,Academia,Industry,Academia,Research collective",,,"Academia,Academia,Industry,Academia,Research collective",,,,Hardware
ProteinGenerator,Biology,Protein generation,"University of Washington,Institute for Protein Design,Georgia Institute of Technology,Microsoft,Heidelberg University","Sidney Lyayuga Lisanza, Jacob Merle Gershon, Samuel W. K. Tipps, Jeremiah Nelson Sims, Lucas Arnoldt, Samuel J. Hendel, Miriam K. Simma, Ge Liu, Muna Yase, Hongwei Wu, Claire D. Tharp, Xinting Li, Alex Kang, Evans Brackenbrough, Asim K. Bera, Stacey Gerben, Bruce J. Wittmann, Andrew C. McShan, David Baker",2024-09-25,Multistate and functional protein design using RoseTTAFold sequence space diffusion,https://www.nature.com/articles/s41587-024-02395-w,1.00,,,,,,,,,,,,,,,,,,"Protein denoising diffusion probabilistic models are used for the de novo generation of protein backbones but are limited in their ability to guide generation of proteins with sequence-specific attributes and functional properties. To overcome this limitation, we developed ProteinGenerator (PG), a sequence space diffusion model based on RoseTTAFold that simultaneously generates protein sequences and structures. Beginning from a noised sequence representation, PG generates sequence and structure pairs by iterative denoising, guided by desired sequence and structural protein attributes. We designed thermostable proteins with varying amino acid compositions and internal sequence repeats and cage bioactive peptides, such as melittin. By averaging sequence logits between diffusion trajectories with distinct structural constraints, we designed multistate parent–child protein triples in which the same sequence folds to different supersecondary structures when intact in the parent versus split into two child domains. PG design trajectories can be guided by experimental sequence–activity data, providing a general approach for integrated computational and experimental optimization of protein function.",,,Unverified,"United States of America,United States of America,United States of America,United States of America,Germany",,,,,,"Academia,Academia,Academia,Industry,Academia",,,"Academia,Academia,Academia,Industry,Academia",,,,
ProteinStructureTransformer,Biology,Protein or nucleotide language model (pLM/nLM),Max Planck Institute of Biochemistry,"Dexiong Chen, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, Karsten Borgwardt",2024-01-26,ENDOWING PROTEIN LANGUAGE MODELS WITH STRUCTURAL KNOWLEDGE,https://arxiv.org/abs/2401.14819,7.00,,,,,44000000000000040000.00,"1. Hardware: 4x NVIDIA H100 PCIe GPUs (7.56e14 FLOP/s per GPU with FP16 Tensor)
2. Training duration: 10 hours (provided directly)
3. Utilization: 40%
4. Calculation: 4 GPUs × 7.56e14 FLOP/s × 36000 seconds × 0.4 utilization = 4.4e19 FLOPs",,,160000001,"542,378 proteins × 300 residues/protein = 162,713,400 tokens (1.627e8)

Final estimate: 1.6e8 datapoints",,,,,,,,"Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretrained language transformers by integrating structural information with structure extractor modules. This refined model, termed Protein Structure Transformer (PST), is further pretrained on a small protein structure database, using the same masked language modeling objective as traditional protein language models. Empirical evaluations of PST demonstrate its superior parameter efficiency relative to protein language models, despite being pretrained on a dataset comprising only 542K structures. Notably, PST consistently outperforms the state-of-the-art foundation model for protein sequences, ESM-2, setting a new benchmark in protein function prediction. Our findings underscore the potential of integrating structural information into protein language models, paving the way for more effective and efficient protein modeling Code and pretrained models are available at this https URL.",,,Unverified,Germany,,,,,,Academia,,,Academia,,,,Hardware
ProtChatGPT,Biology,Protein question answering,"University of Technology Sydney,Zhejiang University","Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang",2024-02-15,ProtChatGPT: Towards Understanding Proteins with Large Language Models,https://arxiv.org/abs/2402.09649,9.00,,,,,319999999999997600000.00,"1. Hardware setup: 4x NVIDIA A100 (80GB) GPUs, 3.12e14 FLOP/s per GPU
2. Training duration: 7.5 days (provided directly - 5.5 days first stage + 2 days second stage) = 648,000 seconds
3. Utilization rate: 40%
4. Calculation: 3.12e14 FLOP/s × 4 GPUs × 648,000 seconds × 0.4 = 3.2e20 FLOPs",,,380000001,"Stage 1:
- Proteins: 549,000 × 500 = 2.745 × 10⁸
- Descriptions: 549,000 × 50 = 2.745 × 10⁷
- Total: 2.745 × 10⁸ + 2.745 × 10⁷ = 3.0195 × 10⁸

Stage 2:
- Proteins: 143,508 × 500 = 7.1754 × 10⁷
- Descriptions: 143,508 × 50 = 7.1754 × 10⁶
- Total: 7.1754 × 10⁷ + 7.1754 × 10⁶ = 7.892 × 10⁷

Final Total: 3.0195 × 10⁸ + 7.892 × 10⁷ = 3.8087 × 10⁸",,,,,,,,"Protein research is crucial in various fundamental disciplines, but understanding their intricate structure-function relationships remains challenging. Recent Large Language Models (LLMs) have made significant strides in comprehending task-specific knowledge, suggesting the potential for ChatGPT-like systems specialized in protein to facilitate basic research. In this work, we introduce ProtChatGPT, which aims at learning and understanding protein structures via natural languages. ProtChatGPT enables users to upload proteins, ask questions, and engage in interactive conversations to produce comprehensive answers. The system comprises protein encoders, a Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and an LLM. The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM. The LLM finally combines user questions with projected embeddings to generate informative answers. Experiments show that ProtChatGPT can produce promising responses to proteins and their corresponding questions. We hope that ProtChatGPT could form the basis for further exploration and application in protein research. Code and our pre-trained model will be publicly available.",,,Unverified,"Australia,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
ProtT3,Biology,Protein question answering,"National University of Singapore,University of Science and Technology of China,Hokkaido University","Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, Tat-Seng Chua",2024-05-21,ProtT3: Protein-to-Text Generation for Text-based Protein Understanding,https://arxiv.org/abs/2405.12564,8.00,,,,,,,,,1300000001,"430,595 + 422,315 + 3,360,000 = 4,212,910 datapoints
4,212,910 × 300 = 1,263,873,000 tokens
Final estimate: 1.3 billion tokens",,,,,,,,"Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks. However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data. Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts. To address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding. ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation. This collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM's representation space and the LM's input space. Unlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation. To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval. Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components. Our code is available at this https URL.",,,Unverified,"Singapore,China,Japan",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
ProLLaMA,Biology,Protein question answering,"Peking University,Peng Cheng Laboratory","Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian",2024-02-26,ProLLaMA: A Protein Language Model for Multi-Task Protein Language Processing,https://arxiv.org/abs/2402.16445,13.00,,,,,120000000000000080000.00,"1. Hardware setup: 8x NVIDIA RTX A6000 GPUs (3.87e13 FLOP/s per GPU)

2. Training duration: Provided directly - Stage 1: 6 days, Stage 2: 5 days (Total: 11 days = 950,400 seconds)

3. Utilization rate: 40%

4. Calculation:
(8 GPUs × 3.87e13 FLOP/s/GPU) × 950,400 seconds × 0.4 utilization = 1.2e20 FLOPs
(Stage 1: 1.603e20 + Stage 2: 1.338e20) × 0.4 = 1.2e20 FLOPs",,,16000000001,"52,807,283 sequences × 300 residues/sequence = 15,842,184,900 tokens ≈ 1.6 × 10^10 tokens",,,,,,,,"Large Language Models (LLMs) have achieved remarkable performance in multiple Natural Language Processing (NLP) tasks. Under the premise that protein sequences constitute the protein language, Protein Language Models(PLMs) have advanced the field of protein engineering. However, as of now, unlike LLMs in NLP, PLMs cannot handle the protein understanding task and the protein generation task simultaneously in the Protein Language Processing (PLP) field. This prompts us to delineate the inherent limitations in current PLMs: (i) the lack of natural language capabilities, (ii) insufficient instruction understanding, and (iii) high training resource demands. To address these challenges, we introduce a training framework to transform any general LLM into a PLM capable of handling multiple PLP tasks. To improve training efficiency, we propose Protein Vocabulary Pruning (PVP) for general LLMs. We construct a multi-task instruction dataset containing 13 million samples with superfamily information, facilitating better modeling of protein sequence-function landscapes. Through these methods, we develop the ProLLaMA model, the first known PLM to handle multiple PLP tasks simultaneously. Experiments show that ProLLaMA achieves state-of-the-art results in the unconditional protein sequence generation task. In the controllable protein sequence generation task, ProLLaMA can design novel proteins with desired functionalities. As for the protein understanding task, ProLLaMA achieves a 62\% exact match rate in superfamily prediction. Codes, model weights, and datasets are available at \url{this https URL} and \url{this https URL}.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
Structure-Informed Protein Language Model,Biology,Protein or nucleotide language model (pLM/nLM),"Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal,IBM Research,HEC Montreal,CIFAR AI Research","Zuobai Zhang, Jiarui Lu, Vijil Chenthamarakshan, Aurélie Lozano, Payel Das, Jian Tang",2024-02-07,Structure-Informed Protein Language Model,https://arxiv.org/abs/2402.05856,3.00,,,,,,,,,420000000001,"
Pre-training:
2.1B sequences × 200 amino acids = 4.2 × 10^11 tokens

Fine-tuning:
12,312 proteins × 300 residues = 3.7 × 10^6 tokens

Total unique tokens: 4.2 × 10^11",,,,,,,,"Protein language models are a powerful tool for learning protein representations through pre-training on vast protein sequence datasets. However, traditional protein language models lack explicit structural supervision, despite its relevance to protein function. To address this issue, we introduce the integration of remote homology detection to distill structural information into protein language models without requiring explicit protein structures as input. We evaluate the impact of this structure-informed training on downstream protein function prediction tasks. Experimental results reveal consistent improvements in function annotation accuracy for EC number and GO term prediction. Performance on mutant datasets, however, varies based on the relationship between targeted properties and protein structures. This underscores the importance of considering this relationship when applying structure-aware training to protein function prediction tasks. Code and model weights are available at this https URL.",,,Unverified,"Canada,Canada,United States of America,Multinational,Canada,Canada",,,,,,"Academia,Academia,Industry,Academia,Research collective",,,"Academia,Academia,Industry,Academia,Research collective",,,,
PoET,Biology,"Protein generation,Protein or nucleotide language model (pLM/nLM)",OpenProtein.ai,"Timothy F. Truong Jr, Tristan Bepler",2023-06-09,PoET: A generative model of protein families as sequences-of-sequences,https://arxiv.org/abs/2306.06156,18.00,,,,,230000000000001970000.00,"1. Hardware setup: 7x NVIDIA A100 GPUs, 3.12e14 FLOP/s per GPU
2. Training duration: 3 days = 259,200 seconds (directly provided)
3. Utilization rate: 40% (default assumption)
4. Calculation: 3.12e14 FLOP/s × 7 GPUs × 259,200s × 0.4 = 2.3e20 FLOPs",,,87580000001,"29,000,000 sets * 10 sequences/set = 290,000,000 sequences
Tokens per sequence = 300 amino acids + 2 tokens = 302 tokens
Total tokens = 290,000,000 sequences * 302 tokens/sequence = 87,580,000,000
Final result: 8.758 × 10¹⁰ datapoints",,,,,,,,"Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose Protein Evolutionary Transformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens sequentially within sequences while attending between sequences order invariantly, allowing PoET to scale to context lengths beyond those used during training. In extensive experiments on deep mutational scanning datasets, we show that PoET outperforms existing protein language models and evolutionary sequence models for variant function prediction across proteins of all MSA depths. We also demonstrate PoET's ability to controllably generate new protein sequences.",,,Unverified,Singapore,,,,,,Industry,,,Industry,,,,Hardware
ESM-AA,Biology,"Protein folding prediction,Protein or nucleotide language model (pLM/nLM),Proteins","Peking University,Nanjing University,Tsinghua University,PharMolix","Kangjie Zheng, Siyu Long, Tianyu Lu, Junwei Yang, Xinyu Dai, Ming Zhang, Zaiqing Nie, Wei-Ying Ma, Hao Zhou",2024-04-05,ESM All-Atom: Multi-scale Protein Language Model for Unified Molecular Modeling,https://arxiv.org/abs/2403.12995,2.00,,,,,520000000000000300000.00,"1. Hardware: 16x NVIDIA A100 GPUs (3.12e14 FLOP/s per GPU)
2. Training duration: 3 days directly reported = 259,200 seconds
3. Utilization: 40% assumed
4. Calculation: 3.12e14 FLOP/s × 16 GPUs × 259,200s × 0.40 = 5.18e20 FLOPs",,,32000000001,"Protein tokens:
- Residue tokens: 8M * 100 = 800M
- Atom tokens: 8M * 100 * 10 = 8B
- Protein total: (800M + 8B) * 1.08 = 9.5B

Molecule tokens: 
- 209M * 100 * 1.08 = 22.5B

Total tokens: 9.5B + 22.5B ≈ 32B (3.2e10)",,,,,,,,"Protein language models have demonstrated significant potential in the field of protein engineering. However, current protein language models primarily operate at the residue scale, which limits their ability to provide information at the atom level. This limitation prevents us from fully exploiting the capabilities of protein language models for applications involving both proteins and small molecules. In this paper, we propose ESM-AA (ESM All-Atom), a novel approach that enables atom-scale and residue-scale unified molecular modeling. ESM-AA achieves this by pre-training on multi-scale code-switch protein sequences and utilizing a multi-scale position encoding to capture relationships among residues and atoms. Experimental results indicate that ESM-AA surpasses previous methods in protein-molecule tasks, demonstrating the full utilization of protein language models. Further investigations reveal that through unified molecular modeling, ESM-AA not only gains molecular knowledge but also retains its understanding of proteins. The source codes of ESM-AA are publicly released at this https URL.",,,Unverified,"China,China,China,China",,,,,,"Academia,Academia,Academia,Industry",,,"Academia,Academia,Academia,Industry",,,,Hardware
MULAN,Biology,Protein or nucleotide language model (pLM/nLM),"AIRI Artificial Intelligence Research Institute,Skolkovo Institute of Science and Technology,Belozersky Institute of Physio-Chemical Biology,Ligand Pro","Daria Frolova, Marina A. Pak, Anna Litvin, Ilya Sharov, Dmitry N. Ivankov, Ivan Oseledets",2024-06-02,MULAN: Multimodal Protein Language Model for Sequence and Structure Encoding,https://www.biorxiv.org/content/10.1101/2024.05.30.596565v1,0.00,,,,,240000000000000200000.00,"1. Hardware: 1x NVIDIA H100 GPU (7.56E14 FLOP/s per GPU)
2. Training duration: 9 days = 777,600 seconds (directly provided)
3. Utilization: 40%
4. Calculation: 7.56E14 FLOP/s × 1 GPU × 777,600s × 0.4 = 2.35E20 FLOPs",,,4070000001,"Calculation:
61,000,000,000 tokens / 15 epochs = 4,070,000,000 tokens

Final estimate: 4.07 billion tokens",,,,,,,,"Most protein language models (PLMs), which are used to produce high-quality protein representations, use only protein sequences during training. However, the known protein structure is crucial in many protein property prediction tasks, so there is a growing interest in incorporating the knowledge about the protein structure into a PLM. In this study, we propose MULAN, a MULtimodal PLM for both sequence and ANgle-based structure encoding. MULAN has a pre-trained sequence encoder and an introduced Structure Adapter, which are then fused and trained together. According to the evaluation on 7 downstream tasks of various nature, both small and medium-sized MULAN models show consistent improvement in quality compared to both sequence-only ESM-2 and structure-aware SaProt. Importantly, our model offers a cheap increase in the structural awareness of the protein representations due to finetuning of existing PLMs instead of training from scratch. We perform a detailed analysis of the proposed model and demonstrate its awareness of the protein structure. The implementation, training data and model checkpoints are available at https://github.com/DFrolova/MULAN.",,,Unverified,"Russia,Russia,Russia,Russia",,,,,,"Research collective,Academia,Academia,Industry",,,"Research collective,Academia,Academia,Industry",,,,Hardware
PTM-Mamba,Biology,Protein or nucleotide language model (pLM/nLM),Duke University,"Zhangzhi Peng, Benjamin Schussheim, Pranam Chatterjee",2024-02-29,PTM-Mamba: A PTM-Aware Protein Language Model with Bidirectional Gated Mamba Blocks,https://www.biorxiv.org/content/10.1101/2024.02.28.581983v1,12.00,,,,,40000000000000010000.00,"1. Hardware: 8x NVIDIA A100 GPUs (3.12e14 FLOP/s per GPU)

2. Training duration: Estimated 40,000 seconds (~11.1 hours)
   Calculation: 400,000 steps ÷ 10 steps/second

3. Utilization rate: 40%

4. Final calculation:
   8 GPUs × 3.12e14 FLOP/s = 2.496e15 FLOP/s total
   2.496e15 FLOP/s × 40,000s × 0.4 = 4.0e19 FLOPs",,,32000001,"79,707 sequences × 400 tokens/sequence = 31,882,800 tokens ≈ 3.2 × 10⁷ tokens",,,,,,,,"Proteins serve as the workhorses of living organisms, orchestrating a wide array of vital functions. Post-translational modifications (PTMs) of their amino acids greatly influence the structural and functional diversity of different protein types and uphold proteostasis, allowing cells to swiftly respond to environmental changes and intricately regulate complex biological processes. To this point, efforts to model the complex features of proteins have involved the training of large and expressive protein language models (pLMs) such as ESM-2 and ProtT5, which accurately encode structural, functional, and physicochemical properties of input protein sequences. However, the over 200 million sequences that these pLMs were trained on merely scratch the surface of proteomic diversity, as they neither input nor account for the effects of PTMs. In this work, we fill this major gap in protein sequence modeling by introducing PTM tokens into the pLM training regime. We then leverage recent advancements in structured state space models (SSMs), specifically Mamba, which utilizes efficient hardware-aware primitives to overcome the quadratic time complexities of Transformers. After adding a comprehensive set of PTM tokens to the model vocabulary, we train bidirectional Mamba blocks whose outputs are fused with state-of-the-art ESM-2 embeddings via a novel gating mechanism. We demonstrate that our resultant PTM-aware pLM, PTM-Mamba, improves upon ESM-2’s performance on various PTM-specific tasks. PTM-Mamba is the first and only pLM that can uniquely input and represent both wild-type and PTM sequences, motivating downstream modeling and design applications specific to post-translationally modified proteins. To facilitate PTM-aware protein language modeling applications, we have made our model available at: https://huggingface.co/ChatterjeeLab/PTM-Mamba.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,Hardware
JURA Bio Model,Biology,"Protein generation,Proteins,Protein or nucleotide language model (pLM/nLM)","JURA Bio,Colombia University,New York University (NYU),Harvard Medical School","Eli N. Weinstein, Mattia G. Gollub, Andrei Slabodkin, Cameron L. Gardner, Kerry Dobbs, Xiao-Bing Cui, Alan N. Amin, George M. Church",2024-09-13,Manufacturing-Aware Generative Model Architectures Enable Biological Sequence Design and Synthesis at Petascale,https://static1.squarespace.com/static/66db3a552002432ff6a39dae/t/66e45d8b7ed2bf1a517e03c1/1726242191877/JURA_VariationalSynthesis.pdf,,,,,,,,,,4400000001,"293,000,000 sequences * 15 tokens/sequence = 4,395,000,000 tokens (~4.4 billion tokens)

Derived from: Observed Antibody Space database with 325,596,608 sequences, 90% training split (293M), avg sequence length 15 amino acids.",,,,,,,,"We introduce a method to reduce the cost of synthesizing proteins and other biological sequences designed by a generative model by as much as a trillion-fold. In particular, we make our
generative models manufacturing-aware, such that model-designed sequences can be efficiently
synthesized in the real world with extreme parallelism. We demonstrate by training and synthesizing samples from generative models of antibodies, T cell antigens and DNA polymerases.
For example, we train a manufacturing-aware generative model on 300 million observed human
antibodies and synthesize ∼1017 generated designs from the model, achieving a sample quality
comparable to a state-of-the-art protein language model, at a cost of 103 dollars. Using previous
methods, synthesis of a library of the same accuracy and size would cost roughly a quadrillion
(1015) dollars.",,,Unverified,"United States of America,United States of America,United States of America,United States of America",,,,,,"Industry,Academia,Academia,Academia",,,"Industry,Academia,Academia,Academia",,,,
scHyena,Biology,Protein or nucleotide language model (pLM/nLM),Korea Advanced Institute of Science and Technology (KAIST),"Gyutaek Oh, Baekgyu Choi, Inkyung Jung, Jong Chul Ye",2024-10-04,scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain,https://arxiv.org/abs/2310.02713,3.00,,,,,39000000000000030000.00,"1. Hardware setup: 2x NVIDIA GeForce RTX 3090 (1.60e+14 FLOP/s per GPU with FP16)

2. Training duration: 3.5 days = 302,400 seconds (directly provided)

3. Utilization rate: 40%

4. Calculation:
1.60e+14 FLOP/s × 2 GPUs × 302,400s × 0.4 = 3.87072e+19 FLOPs ≈ 3.9e+19 FLOPs",,,11000000001,"Cells: 430,312 + 145,170 = 575,482
Genes per cell: 19,306
Total data points: 575,482 × 19,306 = 11,111,255,492 (1.11 × 10¹⁰)
Final estimate: 1.11 × 10¹⁰ tokens",,,,,,,,"Single-cell RNA sequencing (scRNA-seq) has made significant strides in unraveling the intricate cellular diversity within complex tissues. This is particularly critical in the brain, presenting a greater diversity of cell types than other tissue types, to gain a deeper understanding of brain function within various cellular contexts. However, analyzing scRNA-seq data remains a challenge due to inherent measurement noise stemming from dropout events and the limited utilization of extensive gene expression information. In this work, we introduce scHyena, a foundation model designed to address these challenges and enhance the accuracy of scRNA-seq analysis in the brain. Specifically, inspired by the recent Hyena operator, we design a novel Transformer architecture called singe-cell Hyena (scHyena) that is equipped with a linear adaptor layer, the positional encoding via gene-embedding, and a {bidirectional} Hyena operator. This enables us to process full-length scRNA-seq data without losing any information from the raw data. In particular, our model learns generalizable features of cells and genes through pre-training scHyena using the full length of scRNA-seq data. We demonstrate the superior performance of scHyena compared to other benchmark methods in downstream tasks, including cell type classification and scRNA-seq imputation.",,,Unverified,Korea (Republic of),,,,,,Academia,,,Academia,,,,Hardware
OmniGenome,Biology,Protein or nucleotide language model (pLM/nLM),University of Exeter,"Heng Yang, Ke Li",2024-07-15,OmniGenome: Aligning RNA Sequences with Secondary Structures in Genomic Foundation Models,https://arxiv.org/abs/2407.11242,1.00,,,,,1.8999999999999995e+21,"1. Hardware setup: 8x NVIDIA RTX 4090 GPUs (3.30×10¹⁴ FLOP/s per GPU)

2. Training duration: 3 weeks (directly provided)
   = 1,814,400 seconds (3 weeks × 7 days × 24 hours × 3600 seconds)

3. Utilization rate: 40%

4. Final calculation:
   3.30×10¹⁴ FLOP/s × 8 GPUs × 1,814,400 seconds × 0.4 = 1.9×10²¹ FLOPs",,,54200000001,"54.2 billion tokens = 54.2 x 10^9 = 54,200,000,000 tokens",,,,,,,,"The alignment between RNA sequences and structures in foundation models (FMs) has yet to be thoroughly investigated. Existing FMs have struggled to establish sequence-structure alignment, hindering the free flow of genomic information between RNA sequences and structures. In this study, we introduce OmniGenome, an RNA FM trained to align RNA sequences with respect to secondary structures based on structure-contextualised modelling. The alignment enables free and bidirectional mappings between sequences and structures by utilising the flexible RNA modelling paradigm that supports versatile input and output modalities, i.e., sequence and/or structure as input/output. We implement RNA design and zero-shot secondary structure prediction as case studies to evaluate the Seq2Str and Str2Seq mapping capacity of OmniGenome. Results on the EternaV2 benchmark show that OmniGenome solved 74% of puzzles, whereas existing FMs only solved up to 3% of the puzzles due to the oversight of sequence-structure alignment. We leverage four comprehensive in-silico genome modelling benchmarks to evaluate performance across a diverse set of genome downstream tasks, where the results show that OmniGenome achieves state-of-the-art performance on RNA and DNA benchmarks, even without any training on DNA genomes.",,,Unverified,United Kingdom of Great Britain and Northern Ireland,,,,,,Academia,,,Academia,,,,Hardware
ProtBERT-UniRef,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,NVIDIA,Seoul National University,Google,Oak Ridge National Laboratory,Med AI Technology","Ahmed Elnaggar, Michael Heinzinger,  Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger,  Debsindhu Bhowmik, Burkhard Rost",2021-05-04,ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning,"https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or 
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",5.00,,,420000000.00,Table 2,,,,,,,,,,,,,,,,,Unverified,"Germany,United States of America,Korea (Republic of),United States of America,United States of America,China",,,,,,"Academia,Industry,Academia,Industry,Government",,,"Academia,Industry,Academia,Industry,Government",,,,
CPCProt,Biology,Protein or nucleotide language model (pLM/nLM),University of Toronto,"Amy X. Lu, Haoran Zhang, Marzyeh Ghassemi, Alan Moses",2020-11-10,Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization,https://www.biorxiv.org/content/10.1101/2020.09.04.283929v2.abstract,95.00,,,,,,,,,,,,,,,,,,"Pretrained embedding representations of biological sequences which capture meaningful properties can alleviate many problems associated with supervised learning in biology. We apply the principle of mutual information maximization between local and global information as a self-supervised pretraining signal for protein embeddings. To do so, we divide protein sequences into fixed size fragments, and train an autoregressive model to distinguish between subsequent fragments from the same protein and fragments from random proteins. Our model, CPCProt, achieves comparable performance to state-of-the-art self-supervised models for protein sequence embeddings on various downstream tasks, but reduces the number of parameters down to 2% to 10% of benchmarked models. Further, we explore how downstream assessment protocols affect embedding evaluation, and the effect of contrastive learning hyperparameters on empirical performance. We hope that these results will inform the development of contrastive learning methods in protein biology and other modalities.",,,Unverified,Canada,,,,,,Academia,,,Academia,,,,
MuPIPR,Biology,Protein interaction prediction,"University of California Los Angeles (UCLA),University of Pennsylvania","Guangyu Zhou, Muhao Chen, Chelsea J T Ju, Zheng Wang, Jyun-Yu Jiang, Wei Wang",2020-03-05,"Mutation effect estimation on protein–protein
interactions using deep contextualized representation
learning",https://academic.oup.com/nargab/article/2/2/lqaa015/5781175,65.00,,,,,,,,,21000001,"STRING Database: 66,235 sequences × 300 amino acids = 19,870,500 tokens
SKEMPI Dataset: 5,004 sequences × 300 amino acids = 1,501,200 tokens
Total = 19,870,500 + 1,501,200 = 21,371,700 tokens ≈ 2.1 × 10^7",,,,,,,,"The functional impact of protein mutations is reflected on the alteration of conformation and thermodynamics of protein–protein interactions (PPIs). Quantifying the changes of two interacting proteins upon mutations is commonly carried out by computational approaches. Hence, extensive research efforts have been put to the extraction of energetic or structural features on proteins, followed by statistical learning methods to estimate the effects of mutations on PPI properties. Nonetheless, such features require extensive human labors and expert knowledge to obtain, and have limited abilities to reflect point mutations. We present an end-to-end deep learning framework, MuPIPR (Mutation Effects in Protein–protein Interaction PRediction Using Contextualized Representations), to estimate the effects of mutations on PPIs. MuPIPR incorporates a contextualized representation mechanism of amino acids to propagate the effects of a point mutation to surrounding amino acid representations, therefore amplifying the subtle change in a long protein sequence. On top of that, MuPIPR leverages a Siamese residual recurrent convolutional neural encoder to encode a wild-type protein pair and its mutation pair. Multi-layer perceptron regressors are applied to the protein pair representations to predict the quantifiable changes of PPI properties upon mutations. Experimental evaluations show that, with only sequence information, MuPIPR outperforms various state-of-the-art systems on estimating the changes of binding affinity for SKEMPI v1, and offers comparable performance on SKEMPI v2. Meanwhile, MuPIPR also demonstrates state-of-the-art performance on estimating the changes of buried surface areas. The software implementation is available at https://github.com/guangyu-zhou/MuPIPR.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
Profile Prediction,Biology,"Protein or nucleotide language model (pLM/nLM),Proteins","University of Washington,Salesforce Research","Pascal Sturmfels, Jesse Vig, Ali Madani, Nazneen Fatema Rajani",2020-12-01,Profile Prediction: An Alignment-Based Pre-Training Task for Protein Sequence Models,https://arxiv.org/abs/2012.00195,21.00,,,,,499999999999999930000.00,"1. Hardware setup: 8x NVIDIA Tesla V100 GPUs (130 TFLOP/s each)

2. Training duration: 2 weeks (1.2e+6 seconds) - directly provided

3. Utilization rate: 40%

4. Final calculation:
   8 GPUs × 1.30e+14 FLOP/s × 1.2e+6 seconds × 0.4 utilization = 5.0e+20 FLOPs",,,8000000001,"32 million sequences × 250 residues/sequence = 8 billion data points
[32 × 10^6 × 250 = 8 × 10^9]",,,,,,,,"For protein sequence datasets, unlabeled data has greatly outpaced labeled data due to the high cost of wet-lab characterization. Recent deep-learning approaches to protein prediction have shown that pre-training on unlabeled data can yield useful representations for downstream tasks. However, the optimal pre-training strategy remains an open question. Instead of strictly borrowing from natural language processing (NLP) in the form of masked or autoregressive language modeling, we introduce a new pre-training task: directly predicting protein profiles derived from multiple sequence alignments. Using a set of five, standardized downstream tasks for protein models, we demonstrate that our pre-training task along with a multi-task objective outperforms masked language modeling alone on all five tasks. Our results suggest that protein sequence models may benefit from leveraging biologically-inspired inductive biases that go beyond existing language modeling techniques in NLP.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Industry",,,"Academia,Industry",,,,Hardware
PMLM,Biology,Protein or nucleotide language model (pLM/nLM),"Microsoft Research Asia,Nanyang Technological University,Xi’an Jiaotong University,Sun Yat-sen University","Liang He, Shizhuo Zhang, Lijun Wu, Huanhuan Xia, Fusong Ju, He Zhang, Siyuan Liu, Yingce Xia, Jianwei Zhu, Pan Deng, Bin Shao, Tao Qin, Tie-Yan Liu",2021-10-21,Pre-Training Co-Evolutionary Protein Representation via a Pairwise Masked Language Model,https://arxiv.org/abs/2110.15527,28.00,,,,,,,,,,,,,,,,,,"Understanding protein sequences is vital and urgent for biology, healthcare, and medicine. Labeling approaches are expensive yet time-consuming, while the amount of unlabeled data is increasing quite faster than that of the labeled data due to low-cost, high-throughput sequencing methods. In order to extract knowledge from these unlabeled data, representation learning is of significant value for protein-related tasks and has great potential for helping us learn more about protein functions and structures. The key problem in the protein sequence representation learning is to capture the co-evolutionary information reflected by the inter-residue co-variation in the sequences. Instead of leveraging multiple sequence alignment as is usually done, we propose a novel method to capture this information directly by pre-training via a dedicated language model, i.e., Pairwise Masked Language Model (PMLM). In a conventional masked language model, the masked tokens are modeled by conditioning on the unmasked tokens only, but processed independently to each other. However, our proposed PMLM takes the dependency among masked tokens into consideration, i.e., the probability of a token pair is not equal to the product of the probability of the two tokens. By applying this model, the pre-trained encoder is able to generate a better representation for protein sequences. Our result shows that the proposed method can effectively capture the inter-residue correlations and improves the performance of contact prediction by up to 9% compared to the MLM baseline under the same setting. The proposed model also significantly outperforms the MSA baseline by more than 7% on the TAPE contact prediction benchmark when pre-trained on a subset of the sequence database which the MSA is generated from, revealing the potential of the sequence pre-training method to surpass MSA based methods in general.",,,Unverified,"China,Singapore,China,China",,,,,,"Industry,Academia,Academia,Academia",,,"Industry,Academia,Academia,Academia",,,,
PLUS-RNN,Biology,Protein or nucleotide language model (pLM/nLM),"Seoul National University,LG AI Research,NAVER,Kangwon National University","Seonwoo Min, Seunghyun Park, Siwon Kim, Hyun-Soo Choi, Byunghan Lee, Sungroh Yoon",2021-09-03,Pre-Training of Deep Bidirectional Protein Sequence Representations With Structural Information,https://ieeexplore.ieee.org/abstract/document/9529198,69.00,,,,,,,,,2200000001,"14,670,860 protein sequences × 150 amino acids/sequence = 2.2 billion datapoints 
(2.2 × 10^9 total tokens)",,,,,,,,"Bridging the exponentially growing gap between the numbers of unlabeled and labeled protein sequences, several studies adopted semi-supervised learning for protein sequence modeling. In these studies, models were pre-trained with a substantial amount of unlabeled data, and the representations were transferred to various downstream tasks. Most pre-training methods solely rely on language modeling and often exhibit limited performance. In this paper, we introduce a novel pre-training scheme called PLUS , which stands for P rotein sequence representations L earned U sing S tructural information. PLUS consists of masked language modeling and a complementary protein-specific pre-training task, namely same-family prediction. PLUS can be used to pre-train various model architectures. In this work, we use PLUS to pre-train a bidirectional recurrent neural network and refer to the resulting model as PLUS-RNN. Our experiment results demonstrate that PLUS-RNN outperforms other models of similar size solely pre-trained with the language modeling in six out of seven widely used protein biology tasks. Furthermore, we present the results from our qualitative interpretation analyses to illustrate the strengths of PLUS-RNN. PLUS provides a novel way to exploit evolutionary relationships among unlabeled proteins and is broadly applicable across a variety of protein biology tasks. We expect that the gap between the numbers of unlabeled and labeled proteins will continue to grow exponentially, and the proposed pre-training method will play a larger role. All the data and codes used in this study are available at https://github.com/mswzeus/PLUS ",,,Unverified,"Korea (Republic of),Korea (Republic of),Korea (Republic of),Korea (Republic of)",,,,,,"Academia,Industry,Industry,Academia",,,"Academia,Industry,Industry,Academia",,,,
CARP,Biology,Protein or nucleotide language model (pLM/nLM),Microsoft Research,"Kevin K. Yang, Nicolo Fusi, Alex X. Lu",2024-02-06,Convolutions are competitive with transformers for protein sequence pretraining,https://www.cell.com/cell-systems/abstract/S2405-4712(24)00029-2,85.00,,,,,3.0999999999999897e+22,"1. Hardware setup: 128 NVIDIA V100 GPUs (1.25e14 FLOP/s per GPU)

2. Training duration: 56 days (directly provided)
   - Converted to seconds: 56 × 24 × 3600 = 4.8384e6 seconds

3. Utilization rate: 40%

4. Final calculation:
   1.25e14 FLOP/s × 128 GPUs × 4.8384e6 seconds × 0.4 = 3.1e22 FLOPs",,,21000000001,"Total Datapoints = 41.5 × 10^6 × 500 = 2.075 × 10^10 ≈ 2.1 × 10^10 tokens
where:
- Number of sequences: 41.5 million
- Average sequence length: 500 residues",,,,,,,,"Pretrained protein sequence language models have been shown to improve the performance of many prediction tasks and are now routinely integrated into bioinformatics tools. However, these models largely rely on the transformer architecture, which scales quadratically with sequence length in both run-time and memory. Therefore, state-of-the-art models have limitations on sequence length. To address this limitation, we investigated whether convolutional neural network (CNN) architectures, which scale linearly with sequence length, could be as effective as transformers in protein language models. With masked language model pretraining, CNNs are competitive with, and occasionally superior to, transformers across downstream applications while maintaining strong performance on sequences longer than those allowed in the current state-of-the-art transformer models. Our work suggests that computational efficiency can be improved without sacrificing performance, simply by using a CNN architecture instead of a transformer, and emphasizes the importance of disentangling pretraining task and model architecture. A record of this paper’s transparent peer review process is included in the supplemental information.",,,Unverified,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Industry,,,Industry,,,,Hardware
AminoBert,Biology,Protein folding prediction,"Harvard Medical School,Nabla Bio,Columbia University","Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Christina Floristean, Anant Kharkar, Koushik Roy, Charlotte Rochereau, Gustaf Ahdritz, Joanna Zhang, George M. Church, Peter K. Sorger, Mohammed AlQuraishi",2022-10-03,Single-sequence protein structure prediction using a language model and deep learning,https://www.nature.com/articles/s41587-022-01432-w,343.00,,,,,,,,,78000000001,"260,000,000 sequences × 300 tokens/sequence = 78,000,000,000 tokens (7.8 × 10¹⁰)",,,,,,,,"AlphaFold2 and related computational systems predict protein structure using deep learning and co-evolutionary relationships
encoded in multiple sequence alignments (MSAs). Despite high prediction accuracy achieved by these systems, challenges remain
in (1) prediction of orphan and rapidly evolving proteins for which an MSA cannot be generated; (2) rapid exploration of designed
structures; and (3) understanding the rules governing spontaneous polypeptide folding in solution. Here we report development
of an end-to-end differentiable recurrent geometric network (RGN) that uses a protein language model (AminoBERT) to learn
latent structural information from unaligned proteins. A linked geometric module compactly represents Cα backbone geometry
in a translationally and rotationally invariant way. On average, RGN2 outperforms AlphaFold2 and RoseTTAFold on orphan proteins and classes of designed proteins while achieving up to a 106
-fold reduction in compute time. These findings demonstrate
the practical and theoretical strengths of protein language models relative to MSAs in structure prediction.",,,Unverified,"United States of America,United States of America,United States of America",,,,,,"Academia,Industry,Academia",,,"Academia,Industry,Academia",,,,
CELLE-2,Biology,Protein localization prediction,"Chan Zuckerberg Initiative,University of California San Francisco,University of California (UC) Berkeley","Emaad Khwaja, Yun Song, Aaron Agarunov, Bo Huang",2023-10-10,CELL-E 2: Translating Proteins to Pictures and Back with a Bidirectional Text-to-Image Transformer,https://proceedings.neurips.cc/paper_files/paper/2023/hash/0fb7c02d420c993385c7de44c2b5bf01-Abstract-Conference.html,2.00,,,,,,,,,11300001,"Image tokens: 17,268 × 256 = 4,420,608
Sequence tokens: 17,268 × 400 = 6,907,200
Total: 4,420,608 + 6,907,200 = 11,327,808 (1.13e7)",,,,,,,,"We present CELL-E 2, a novel bidirectional transformer that can generate images depicting protein subcellular localization from the amino acid sequences (and vice versa). Protein localization is a challenging problem that requires integrating sequence and image information, which most existing methods ignore. CELL-E 2 extends the work of CELL-E, not only capturing the spatial complexity of protein localization and produce probability estimates of localization atop a nucleus image, but also being able to generate sequences from images, enabling de novo protein design. We train and finetune CELL-E 2 on two large-scale datasets of human proteins. We also demonstrate how to use CELL-E 2 to create hundreds of novel nuclear localization signals (NLS). Results and interactive demos are featured at https://bohuanglab.github.io/CELL-E_2/.",,,Unverified,"United States of America,United States of America,United States of America",,,,,,"Research collective,Academia,Academia",,,"Research collective,Academia,Academia",,,,
GraphMS,Biology,Protein-ligand contact prediction,"Dalian University of Technology,Dongbei University of Technology,Baidu,China National Health Development Research Center","Shicheng Cheng, Liang Zhang, Bo Jin, Qiang Zhang, Xinjiang Lu, Mao You, Xueqing Tian",2021-04-04,GraphMS: Drug Target Prediction Using Graph Representation Learning with Substructures,https://www.mdpi.com/2076-3417/11/7/3239,14.00,,,,,,,,,1800001,"Edges:
Drug-Protein: 1,923
Drug-Disease: 199,214
Protein-Disease: 1,596,745
Drug-Drug: 10,036
Protein-Protein: 7,363

1,923 + 199,214 + 1,596,745 + 10,036 + 7,363 = 1,815,281 total edges

Final estimate: 1.8 × 10⁶",,,,,,,,"The prediction of drug–target interactions is always a key task in the field of drug redirection. However, traditional methods of predicting drug–target interactions are either mediocre or rely heavily on data stacking. In this work, we proposed our model named GraphMS. We merged heterogeneous graph information and obtained effective node information and substructure information based on mutual information in graph embeddings. We then learned high quality representations for downstream tasks, and proposed an end–to–end auto–encoder model to complete the task of link prediction. Experimental results show that our method outperforms several state–of–the–art models. The model can achieve the area under the receiver operating characteristics (AUROC) curve of 0.959 and area under the precise recall curve (AUPR) of 0.847. We found that the mutual information between the substructure and graph–level representations contributes most to the mutual information index in a relatively sparse network. And the mutual information between the node–level and graph–level representations contributes most in a relatively dense network.",,,Unverified,"China,China,China,China",,,,,,"Academia,Academia,Industry,Academia",,,"Academia,Academia,Industry,Academia",,,,
CRL,Biology,"Protein folding prediction,Protein classification,Protein-ligand binding affinity prediction,Protein structure similarity prediction",Ulm University,"Pedro Hermosilla, Timo Ropinski",2022-05-31,Contrastive Representation Learning for 3D Protein Structures,https://arxiv.org/abs/2205.15675,42.00,,,,,,,,,,,,,,,,,,"Learning from 3D protein structures has gained wide interest in protein modeling and structural bioinformatics. Unfortunately, the number of available structures is orders of magnitude lower than the training data sizes commonly used in computer vision and machine learning. Moreover, this number is reduced even further, when only annotated protein structures can be considered, making the training of existing models difficult and prone to over-fitting. To address this challenge, we introduce a new representation learning framework for 3D protein structures. Our framework uses unsupervised contrastive learning to learn meaningful representations of protein structures, making use of proteins from the Protein Data Bank. We show, how these representations can be used to solve a large variety of tasks, such as protein function prediction, protein fold classification, structural similarity prediction, and protein-ligand binding affinity prediction. Moreover, we show how fine-tuned networks, pre-trained with our algorithm, lead to significantly improved task performance, achieving new state-of-the-art results in many tasks.",,,Unverified,Germany,,,,,,,,,,,,,
STEPS,Biology,"Protein classification,Protein localization prediction,Enzyme-catalyzed reaction classification","McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Baidu Research - Silicon Valley AI Lab,Baidu,Boston Consulting Group X","Can (Sam) Chen, Jingbo Zhou, Fan Wang, Xue Liu, Dejing Dou",2023-04-01,Structure-aware protein self-supervised learning,https://academic.oup.com/bioinformatics/article/39/4/btad189/7117544,53.00,,,,,,,,,12000001,"40,000 proteins × 300 residues/protein = 12,000,000 residues
Total Unique Data Points = 12,000,000",,,,,,,,"Motivation
Protein representation learning methods have shown great potential to many downstream tasks in biological applications. A few recent studies have demonstrated that the self-supervised learning is a promising solution to addressing insufficient labels of proteins, which is a major obstacle to effective protein representation learning. However, existing protein representation learning is usually pretrained on protein sequences without considering the important protein structural information.

Results
In this work, we propose a novel structure-aware protein self-supervised learning method to effectively capture structural information of proteins. In particular, a graph neural network model is pretrained to preserve the protein structural information with self-supervised tasks from a pairwise residue distance perspective and a dihedral angle perspective, respectively. Furthermore, we propose to leverage the available protein language model pretrained on protein sequences to enhance the self-supervised learning. Specifically, we identify the relation between the sequential information in the protein language model and the structural information in the specially designed graph neural network model via a novel pseudo bi-level optimization scheme. We conduct experiments on three downstream tasks: the binary classification into membrane/non-membrane proteins, the location classification into 10 cellular compartments, and the enzyme-catalyzed reaction classification into 384 EC numbers, and these experiments verify the effectiveness of our proposed method.",,,Unverified,"Canada,Canada,United States of America,China,United States of America",,,,,,"Academia,Academia,Industry,Industry,Industry",,,"Academia,Academia,Industry,Industry,Industry",,,,
LM-GVP,Biology,Protein property prediction,"Amazon Machine Learning Solutions Lab,Johnson & Johnson","Zichen Wang, Steven A. Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O. Salawu, Colby J. Wise, Sri Priya Ponnapalli, Peter M. Clark",2021-09-21,LM-GVP: A Generalizable Deep Learning Framework for Protein Property Prediction from Sequence and Structure,https://www.biorxiv.org/content/10.1101/2021.09.21.460852v1.abstract,10.00,,,,,240000000000000200000.00,"1. Hardware setup: 8x NVIDIA V100 GPUs (1.25×10¹⁴ FLOP/s per GPU using fp16_tensor)
2. Training duration: Estimated 168 hours (1 week), based on assumption due to lack of specific information
3. Utilization rate: 40%
4. Calculation: 1.25×10¹⁴ FLOP/s × 8 GPUs × 604,800 seconds × 0.4 = 2.4×10²⁰ FLOPs",,,,,,,,,,,,"Proteins perform many essential functions in biological systems and can be successfully developed as bio-therapeutics. It is invaluable to be able to predict their properties based on a proposed sequence and structure. In this study, we developed a novel generalizable deep learning framework, LM-GVP, composed of a protein Language Model (LM) and Graph Neural Network (GNN) to leverage information from both 1D amino acid sequences and 3D structures of proteins. Our approach outperformed the state-of-the-art protein LMs on a variety of property prediction tasks including fluorescence, protease stability, and protein functions from Gene Ontology (GO). We also illustrated insights into how a GNN prediction head can guide the protein LM to better leverage structural information. We envision that our deep learning framework will be generalizable to many protein property prediction problems to greatly accelerate protein engineering and drug development.",,,Unverified,"United States of America,United States of America",,,,,,"Industry,Industry",,,"Industry,Industry",,,,Hardware
DeepFRI,Biology,Protein function prediction,"Flatiron Institute,University of California San Diego,Jagiellonian University,New York University (NYU),Broad Institute,University of Auckland,Massachusettes General Hospital,Harvard Medical School,Massachusetts Institute of Technology (MIT)","Vladimir Gligorijević, P. Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn C. Taylor, Ian M. Fisk, Hera Vlamakis, Ramnik J. Xavier, Rob Knight, Kyunghyun Cho & Richard Bonneau",2021-05-26,Structure-based protein function prediction using graph convolutional networks,https://www.nature.com/articles/s41467-021-23303-9,658.00,,,,,,,,,,,,,,,,,,"The rapid increase in the number of proteins in sequence databases and the diversity of their functions challenge computational approaches for automated function prediction. Here, we introduce DeepFRI, a Graph Convolutional Network for predicting protein functions by leveraging sequence features extracted from a protein language model and protein structures. It outperforms current leading methods and sequence-based Convolutional Neural Networks and scales to the size of current sequence repositories. Augmenting the training set of experimental structures with homology models allows us to significantly expand the number of predictable functions. DeepFRI has significant de-noising capability, with only a minor drop in performance when experimental structures are replaced by protein models. Class activation mapping allows function predictions at an unprecedented resolution, allowing site-specific annotations at the residue-level in an automated manner. We show the utility and high performance of our method by annotating structures from the PDB and SWISS-MODEL, making several new confident function predictions. DeepFRI is available as a webserver at https://beta.deepfri.flatironinstitute.org/.",,,Unverified,"United States of America,United States of America,Poland,United States of America,United States of America,New Zealand,United States of America,United States of America,United States of America",,,,,,"Academia,Academia,Academia,Research collective,Academia,Academia,Academia",,,"Academia,Academia,Academia,Research collective,Academia,Academia,Academia",,,,
HJRSS,Biology,Protein design,"University of Washington,Microsoft","Sanaa Mansoor, Minkyung Baek, Umesh Madan, Eric Horvitz",2021-09-01,Toward More General Embeddings for Protein Design: Harnessing Joint Representations of Sequence and Structure,https://www.biorxiv.org/content/10.1101/2021.09.01.458592v1.abstract,19.00,,,,,,,,,,,,,,,,,,"Protein embeddings learned from aligned sequences have been leveraged in a wide array of tasks in protein understanding and engineering. The sequence embeddings are generated through semi-supervised training on millions of sequences with deep neural models defined with hundreds of millions of parameters, and they continue to increase in performance on target tasks with increasing complexity. We report a more data-efficient approach to encode protein information through joint training on protein sequence and structure in a semi-supervised manner. We show that the method is able to encode both types of information to form a rich embedding space which can be used for downstream prediction tasks. We show that the incorporation of rich structural information into the context under consideration boosts the performance of the model by predicting the effects of single-mutations. We attribute increases in accuracy to the value of leveraging proximity within the enriched representation to identify sequentially and spatially close residues that would be affected by the mutation, using experimentally validated or predicted structures.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Industry",,,"Academia,Industry",,,,
GraSR,Biology,Protein structure comparison,"Shanghai Jiao Tong University,Ministry of Education of China","Chunqiu Xia, Shi-Hao Feng, Ying Xia, Xiaoyong Pan, Hong-Bin Shen",2022-03-24,Fast protein structure comparison through effective representation learning with contrastive graph neural networks,https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009986,25.00,,,,,3799999999999999500.00,"1. Hardware setup: 2x TITAN Xp GPUs (1.10×10¹³ FLOP/s each)
2. Training duration: Estimated 5 days (432,000 seconds) based on ""several days"" mention
3. Utilization rate: 40%
4. Calculation: 2 GPUs × 1.10×10¹³ FLOP/s × 432,000s × 0.40 = 3.8×10¹⁸ FLOPs",,,263001,"SCOPe v2.07 Dataset:
Total PDB Entries: 87,224
Total Domains: 276,231

Training Set Size = Total Domains - Cross-Validation Domains
Training Set Size = 276,231 - 13,265 = 262,966 domains",,,,,,,,"Protein structure alignment algorithms are often time-consuming, resulting in challenges for large-scale protein structure similarity-based retrieval. There is an urgent need for more efficient structure comparison approaches as the number of protein structures increases rapidly. In this paper, we propose an effective graph-based protein structure representation learning method, GraSR, for fast and accurate structure comparison. In GraSR, a graph is constructed based on the intra-residue distance derived from the tertiary structure. Then, deep graph neural networks (GNNs) with a short-cut connection learn graph representations of the tertiary structures under a contrastive learning framework. To further improve GraSR, a novel dynamic training data partition strategy and length-scaling cosine distance are introduced. We objectively evaluate our method GraSR on SCOPe v2.07 and a new released independent test set from PDB database with a designed comprehensive performance metric. Compared with other state-of-the-art methods, GraSR achieves about 7%-10% improvement on two benchmark datasets. GraSR is also much faster than alignment-based methods. We dig into the model and observe that the superiority of GraSR is mainly brought by the learned discriminative residue-level and global descriptors. The web-server and source code of GraSR are freely available at www.csbio.sjtu.edu.cn/bioinf/GraSR/ for academic use.",,,Unverified,"China,China",,,,,,"Academia,Government",,,"Academia,Government",,,,Hardware
CPAC,Biology,"Protein-ligand binding affinity prediction,Protein-ligand contact prediction",Texas A&M,"Yuning You, Yang Shen",2022-09-18,Cross-modality and self-supervised protein embedding for compound–protein affinity and contact prediction,https://academic.oup.com/bioinformatics/article/38/Supplement_2/ii68/6702011,19.00,,,,,,,,,13400000001,"Calculations:
60,137 × 1,000 = 6.0137 × 10⁷
12,798,671 × 1,000 = 1.2798671 × 10¹⁰
6.0137 × 10⁷ + 1.2798671 × 10¹⁰ = 1.3400678 × 10¹⁰

Final estimate: 1.34 × 10¹⁰ datapoints",,,,,,,,"Motivation
Computational methods for compound–protein affinity and contact (CPAC) prediction aim at facilitating rational drug discovery by simultaneous prediction of the strength and the pattern of compound–protein interactions. Although the desired outputs are highly structure-dependent, the lack of protein structures often makes structure-free methods rely on protein sequence inputs alone. The scarcity of compound–protein pairs with affinity and contact labels further limits the accuracy and the generalizability of CPAC models.

Results
To overcome the aforementioned challenges of structure naivety and labeled-data scarcity, we introduce cross-modality and self-supervised learning, respectively, for structure-aware and task-relevant protein embedding. Specifically, protein data are available in both modalities of 1D amino-acid sequences and predicted 2D contact maps that are separately embedded with recurrent and graph neural networks, respectively, as well as jointly embedded with two cross-modality schemes. Furthermore, both protein modalities are pre-trained under various self-supervised learning strategies, by leveraging massive amount of unlabeled protein data. Our results indicate that individual protein modalities differ in their strengths of predicting affinities or contacts. Proper cross-modality protein embedding combined with self-supervised learning improves model generalizability when predicting both affinities and contacts for unseen proteins.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
MIF-ST,Biology,"Proteins,Protein generation","Microsoft Research,OpenBioML,University of Chicago","Kevin K Yang, Niccolò Zanichelli, Hugh Yeh",2022-10-26,Masked inverse folding with sequence transfer for protein representation learning,https://academic.oup.com/peds/article-abstract/doi/10.1093/protein/gzad015/7330543?redirectedFrom=fulltext#no-access-message,61.00,,,,,2.50000000000002e+22,"1. Hardware setup: 200x NVIDIA V100 GPUs with 1.30×10^14 FLOP/s per GPU

2. Training duration: Estimated 4 weeks based on ""multiple weeks"" phrase
(4 weeks × 7 days × 24 hours × 3600 seconds = 2,419,200 seconds)

3. Utilization rate: 40%

4. Calculation:
1.30×10^14 FLOP/s × 200 GPUs × 2.4192×10^6 seconds × 0.4 = 2.5×10^22 FLOP",,,12600000001,"42M × 300 = 42,000,000 × 300 = 12,600,000,000 = 1.26e10 datapoints",,,,,,,,"Self-supervised pretraining on protein sequences has led to state-of-the art performance on protein function and fitness prediction. However, sequence-only methods ignore the rich information contained in experimental and predicted protein structures. Meanwhile, inverse folding methods reconstruct a protein’s amino-acid sequence given its structure, but do not take advantage of sequences that do not have known structures. In this study, we train a masked inverse folding protein masked language model parameterized as a structured graph neural network. During pretraining, this model learns to reconstruct corrupted sequences conditioned on the backbone structure. We then show that using the outputs from a pretrained sequence-only protein masked language model as input to the inverse folding model further improves pretraining perplexity. We evaluate both of these models on downstream protein engineering tasks and analyze the effect of using information from experimental or predicted structures on performance.",,,Unverified,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,United States of America",,,,,,"Industry,Research collective,Academia",,,"Industry,Research collective,Academia",,,,Hardware
SSA,Biology,Protein embedding,Massachusetts Institute of Technology (MIT),"Tristan Bepler, Bonnie Berger",2019-02-22,Learning protein sequence embeddings using information from structure,https://arxiv.org/abs/1902.08661,338.00,,,,,3200000000000000000.00,"""All models were implemented in PyTorch and trained on a single NVIDIA Tesla V100 GPU. Each model took roughly 3 days to train and required 16 GB of GPU RAM""",,,3940001,"22,408 sequences × 176 residues/sequence = 3,943,808 tokens

Final estimate: 3.94 million unique datapoints",,72.0,"""Each model took roughly 3 days to train and required 16 GB of GPU RAM""",,,,,"Inferring the structural properties of a protein from its amino acid sequence is a challenging yet important problem in biology. Structures are not known for the vast majority of protein sequences, but structure is critical for understanding function. Existing approaches for detecting structural similarity between proteins from sequence are unable to recognize and exploit structural patterns when sequences have diverged too far, limiting our ability to transfer knowledge between structurally related proteins. We newly approach this problem through the lens of representation learning. We introduce a framework that maps any protein sequence to a sequence of vector embeddings --- one per amino acid position --- that encode structural information. We train bidirectional long short-term memory (LSTM) models on protein sequences with a two-part feedback mechanism that incorporates information from (i) global structural similarity between proteins and (ii) pairwise residue contact maps for individual proteins. To enable learning from structural similarity information, we define a novel similarity measure between arbitrary-length sequences of vector embeddings based on a soft symmetric alignment (SSA) between them. Our method is able to learn useful position-specific embeddings despite lacking direct observations of position-level correspondence between sequences. We show empirically that our multi-task framework outperforms other sequence-based methods and even a top-performing structure-based alignment method when predicting structural similarity, our goal. Finally, we demonstrate that our learned embeddings can be transferred to other protein sequence problems, improving the state-of-the-art in transmembrane domain prediction.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,Hardware
PeTriBERT,Biology,Protein generation,"University of Montpellier,BionomeeX","Baldwin Dumortier, Antoine Liutkus, Clément Carré, Gabriel Krouk",2022-08-13,PeTriBERT : Augmenting BERT with tridimensional encoding for inverse protein folding and design,https://www.biorxiv.org/content/10.1101/2022.08.10.503344v1.abstract,9.00,,,,,100000000000000000000.00,"1. Hardware setup: 8x NVIDIA Tesla V100 SXM2 32GB GPUs (1.25 x 10^14 FLOP/s per GPU)

2. Training duration: 70 hours (directly provided) = 252,000 seconds

3. Utilization rate: 40%

4. Final calculation:
1.25 x 10^14 FLOP/s/GPU × 8 GPUs × 252,000 seconds × 0.4 = 1.0 x 10^20 FLOPs",,,297000001,"Training sequence data points = 290,000 proteins × 1,024 tokens/protein = 297,160,000 tokens (~2.97×10⁸)",,,,,,,,"Protein is biology workhorse. Since the recent break-through of novel folding methods, the amount of available structural data is increasing, closing the gap between data-driven sequence-based and structure-based methods. In this work, we focus on the inverse folding problem that consists in predicting an amino-acid primary sequence from protein 3D structure. For this purpose, we introduce a simple Transformer model from Natural Language Processing augmented 3D-structural data. We call the resulting model PeTriBERT: Proteins embedded in tridimensional representation in a BERT model. We train this small 40-million parameters model on more than 350 000 proteins sequences retrieved from the newly available AlphaFoldDB database. Using PetriBert, we are able to in silico generate totally new proteins with a GFP-like structure. These 9 of 10 of these GFP structural homologues have no ressemblance when blasted on the whole entry proteome database. This shows that PetriBert indeed capture protein folding rules and become a valuable tool for de novo protein design.",,,Unverified,"France,France",,,,,,Industry,,,Industry,,,,Hardware
GearNet,Biology,"Proteins,Protein function prediction,Protein fold classification","Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal,University of Cambridge,IBM Research,HEC Montreal,CIFAR AI Research","Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurélie Lozano, Payel Das, Jian Tang",2022-11-01,Protein Representation Learning by Geometric Structure Pretraining,https://arxiv.org/abs/2203.06125,177.00,,,,,,,,,240000001,"805,000 proteins × 300 residues/protein = 241,500,000 datapoints (2.415 × 10^8)
Breakdown:
1. Initial protein count: 365,000 + 440,000 = 805,000
2. Final calculation: 805,000 × 300 = 241,500,000",,,,,,,,"Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data. Our implementation is available at this https URL.",,,Unverified,"Canada,Canada,United Kingdom of Great Britain and Northern Ireland,United States of America,Multinational,Canada,Canada",,,,,,"Academia,Academia,Academia,Industry,Academia,Research collective",,,"Academia,Academia,Academia,Industry,Academia,Research collective",,,,
ESM-GearNet,Biology,"Proteins,Protein function prediction","Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal,IBM Research,HEC Montreal,CIFAR AI Research","Zuobai Zhang, Chuanrui Wang, Minghao Xu, Vijil Chenthamarakshan, Aurélie Lozano, Payel Das, Jian Tang",2023-05-11,A Systematic Study of Joint Representation Learning on Protein Sequences and Structures,https://arxiv.org/abs/2303.06275,17.00,,,,,,,,,110000001,"Calculating unique tokens seen in first epoch:
Number of Proteins: 365,000
Average Residues per Protein: 300
Total Datapoints = 365,000 × 300 = 109,500,000 ≈ 1.1 × 10^8 tokens",,,,,,,,"Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein functions. Recent sequence representation learning methods based on Protein Language Models (PLMs) excel in sequence-based tasks, but their direct adaptation to tasks involving protein structures remains a challenge. In contrast, structure-based methods leverage 3D structural information with graph neural networks and geometric pre-training methods show potential in function prediction tasks, but still suffers from the limited number of available structures. To bridge this gap, our study undertakes a comprehensive exploration of joint protein representation learning by integrating a state-of-the-art PLM (ESM-2) with distinct structure encoders (GVP, GearNet, CDConv). We introduce three representation fusion strategies and explore different pre-training techniques. Our method achieves significant improvements over existing sequence- and structure-based methods, setting new state-of-the-art for function annotation. This study underscores several important design choices for fusing protein sequence and structure information. Our implementation is available at this https URL.",,,Unverified,"Canada,Canada,United States of America,Multinational,Canada,Canada",,,,,,"Academia,Academia,Industry,Academia,Research collective",,,"Academia,Academia,Industry,Academia,Research collective",,,,
SaProt,Biology,Protein or nucleotide language model (pLM/nLM),"Zhejiang University,Westlake University","Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan",2024-04-19,SaProt: Protein Language Modeling with Structure-aware Vocabulary,https://www.biorxiv.org/content/10.1101/2023.10.01.560349v5.abstract,60.00,,,650000000.00,"""To our knowledge, SaProt stands out as the PLM currently trained with the largest number of protein structures, containing 650 million parameters"" Assume 40% utilization, FP16 tensor precision",1.5999999999999998e+22,"""Its training lasted 3 months and utilized 64 NVIDIA 80G A100 GPUs,""",,,12000000001,4.0 × 10⁷ sequences × 3.0 × 10² tokens/sequence = 1.2 × 10¹⁰ tokens,,2160.0,"""Its training lasted 3 months """,,,,,"Large-scale protein language models (PLMs), such as the ESM family, have achieved remarkable performance in various downstream tasks related to protein structure and function by undergoing unsupervised training on residue sequences. They have become essential tools for researchers and practitioners in biology. However, a limitation of vanilla PLMs is their lack of explicit consideration for protein structure information, which suggests the potential for further improvement. Motivated by this, we introduce the concept of a “structure-aware vocabulary” that integrates residue tokens with structure tokens. The structure tokens are derived by encoding the 3D structure of proteins using Foldseek. We then propose SaProt, a large-scale general-purpose PLM trained on an extensive dataset comprising approximately 40 million protein sequences and structures. Through extensive evaluation, our SaProt model surpasses well-established and renowned baselines across 10 significant downstream tasks, demonstrating its exceptional capacity and broad applicability. We have made the code1, pre-trained model, and all relevant materials available at https://github.com/westlake-repl/SaProt.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
KeAP,Biology,"Proteins,Protein representation learning","The University of Hong Kong,ByteDance,JancsiTech,OPPO HealthLab","Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Cheng Bian, Yizhou Yu",2023-01-30,Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling,https://arxiv.org/abs/2301.13154,6.00,,,,,,,,,180000001,"600,000 proteins × 300 amino acids per protein = 1.8 × 10⁸ datapoints",,,,,,,,"Protein representation learning has primarily benefited from the remarkable development of language models (LMs). Accordingly, pre-trained protein models also suffer from a problem in LMs: a lack of factual knowledge. The recent solution models the relationships between protein and associated knowledge terms as the knowledge encoding objective. However, it fails to explore the relationships at a more granular level, i.e., the token level. To mitigate this, we propose Knowledge-exploited Auto-encoder for Protein (KeAP), which performs token-level knowledge graph exploration for protein representation learning. In practice, non-masked amino acids iteratively query the associated knowledge tokens to extract and integrate helpful information for restoring masked amino acids via attention. We show that KeAP can consistently outperform the previous counterpart on 9 representative downstream applications, sometimes surpassing it by large margins. These results suggest that KeAP provides an alternative yet effective way to perform knowledge enhanced protein representation learning.",,,Unverified,"Hong Kong,China,China,China,China",,,,,,"Academia,Industry,Industry,Industry",,,"Academia,Industry,Industry,Industry",,,,
MASSA,Biology,Protein representation learning,"Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems,Shenzhen Institute of Advanced Technology,Chinese Academy of Sciences","Fan Hu, Yishen Hu, Weihong Zhang, Huazhen Huang, Yi Pan, Peng Yin",2023-05-30,A Multimodal Protein Representation Framework for Quantifying Transferability Across Biochemical Downstream Tasks,https://onlinelibrary.wiley.com/doi/full/10.1002/advs.202301223,11.00,,,,,,,,,300000001,"Total Datapoints = Number of Proteins × Average Protein Length
1,000,000 × 300 = 3.0e8 datapoints",,,,,,,,"Proteins are the building blocks of life, carrying out fundamental functions in biology. In computational biology, an effective protein representation facilitates many important biological quantifications. Most existing protein representation methods are derived from self-supervised language models designed for text analysis. Proteins, however, are more than linear sequences of amino acids. Here, a multimodal deep learning framework for incorporating ≈1 million protein sequence, structure, and functional annotation (MASSA) is proposed. A multitask learning process with five specific pretraining objectives is presented to extract a fine-grained protein-domain feature. Through pretraining, multimodal protein representation achieves state-of-the-art performance in specific downstream tasks such as protein properties (stability and fluorescence), protein‒protein interactions (shs27k/shs148k/string/skempi), and protein‒ligand interactions (kinase, DUD-E), while achieving competitive results in secondary structure and remote homology tasks. Moreover, a novel optimal-transport-based metric with rich geometry awareness is introduced to quantify the dynamic transferability from the pretrained representation to the related downstream tasks, which provides a panoramic view of the step-by-step learning process. The pairwise distances between these downstream tasks are also calculated, and a strong correlation between the inter-task feature space distributions and adaptability is observed.",,,Unverified,"China,China,China",,,,,,"Research collective,Academia",,,"Research collective,Academia",,,,
ProteinINR,Biology,Protein representation learning,Kakao,"Youhan Lee, Hasun Yu, Jaemyung Lee, Jaehoon Kim",2024-01-16,"Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning",https://openreview.net/forum?id=BEH4mGo7zP,5.00,,,,,,,,,15000000001,"Calculation: 16,384 x 906,458 = 14,879,754,112 ≈ 1.5e10 data points

Additional calculation: 300 x 906,458 = 272,000,000 points (sequence data)

Final estimate: 1.5e10 data points (dominated by structural data)",,,,,,,,"Proteins can be represented in various ways, including their sequences, 3D structures, and surfaces. While recent studies have successfully employed sequence- or structure-based representations to address multiple tasks in protein science, there has been significant oversight in incorporating protein surface information, a critical factor for protein function. In this paper, we present a pre-training strategy that incorporates information from protein sequences, 3D structures, and surfaces to improve protein representation learning. Specifically, we utilize Implicit Neural Representations (INRs) for learning surface characteristics, and name it ProteinINR. We confirm that ProteinINR successfully reconstructs protein surfaces, and integrate this surface learning into the existing pre-training strategy of sequences and structures. Our results demonstrate that our approach can enhance performance in various downstream tasks, thereby underscoring the importance of including surface attributes in protein representation learning. These findings underline the importance of understanding protein surfaces for generating effective protein representations.",,,Unverified,Korea (Republic of),,,,,,Industry,,,Industry,,,,
ProCALM,Biology,"Proteins,Protein generation","Profluent Bio,California Institute of Technology","Jason Yang, Aadyot Bhatnagar, Jeffrey A. Ruffolo, Ali Madani",2024-10-09,Conditional Enzyme Generation Using Protein Language Models with Adapters,https://www.arxiv.org/abs/2410.03634,0.00,,,,,17999999999999869000.00,"1. Hardware: 4x NVIDIA A100 GPUs (3.12E14 FLOP/s per GPU)
2. Duration: 10 hours (directly provided) = 36,000 seconds
3. Utilization: 40%
4. Calculation: 3.12E14 FLOP/s × 4 GPUs × 36,000s × 0.4 = 1.8E19 FLOPs",,,60000001,"6 × 10^7 = 60,000,000 tokens, only first epoch of Swissprot-1.5B version considered

No additional calculations were needed - the estimate was taken directly from the epoch size.",,,,,,,,"The conditional generation of proteins with desired functions and/or properties is a key goal for generative models. Existing methods based on prompting of language models can generate proteins conditioned on a target functionality, such as a desired enzyme family. However, these methods are limited to simple, tokenized conditioning and have not been shown to generalize to unseen functions. In this study, we propose ProCALM (Protein Conditionally Adapted Language Model), an approach for the conditional generation of proteins using adapters to protein language models. Our specific implementation of ProCALM involves finetuning ProGen2 to incorporate conditioning representations of enzyme function and taxonomy. ProCALM matches existing methods at conditionally generating sequences from target enzyme families. Impressively, it can also generate within the joint distribution of enzymatic function and taxonomy, and it can generalize to rare and unseen enzyme families and taxonomies. Overall, ProCALM is a flexible and computationally efficient approach, and we expect that it can be extended to a wide range of generative language models.",,,Unverified,"United States of America,United States of America",,,,,,"Industry,Academia",,,"Industry,Academia",,,,Hardware
MLDD3UTRmRRNAS,Biology,"Protein or nucleotide language model (pLM/nLM),Nucleotide generation",Ginkgo Bioworks,"Alyssa Kramer Morrow, Ashley Thornal, Elise Duboscq Flynn, Emily Hoelzli, Meimei Shan, Gorkem Garipler, Rory Kirchner, Aniketh Janardhan Reddy, Sophia Tabchouri, Ankit Gupta, Jean-Baptiste Michel, Uri Laserson






",2024-10-07,ML-driven design of 3’ UTRs for mRNA stability,https://www.biorxiv.org/content/10.1101/2024.10.07.616676v1,0.00,,,,,,,,,29500001,"TOKENS = 180,000 × 164 = 29,520,000 (2.95e7)",,,,,,,,"Using mRNA as a therapeutic has received enormous attention in the last few years, but instability of the molecule remains a hurdle to achieving long-lasting therapeutic levels of protein expression. In this study, we describe our approach for designing stable mRNA molecules by combining machine learning-driven sequence design with high-throughput experimental assays. We developed a high-throughput massively parallel reporter assay (MPRA) that, in a single experiment, measures the half-life of tens of thousands of unique mRNA sequences containing designed 3' UTRs. Over multiple design-build-test iterations, we have accumulated 180,000 unique measurements of mRNA stability covering unique genomic and synthetic 3' UTRs, representing the largest such dataset of sequences. We trained highly-accurate machine learning models to map from 3' UTR sequence to mRNA stability, and used them to guide the design of synthetic 3' UTRs that increase mRNA stability in cell lines. Finally, we validated the function of several ML-designed 3' UTRs in mouse models, resulting in up to 2-fold more protein production over time and 30--100-fold higher protein output at later time points compared to a commonly used benchmark. These results highlight the potential of ML-driven sequence design for mRNA therapeutics.",,,Unverified,United States of America,,,,,,Industry,,,Industry,,,,
CHAI-1,Biology,Protein folding prediction,Chai discovery,"Jacques Boitreaud, Jack Dent, Matthew McPartlon, Joshua Meier, Vinicius Reis, Alex Rogozhnikov, Kevin Wu",2024-10-15,Introducing Chai-1: Decoding the molecular interactions of life,"https://www.chaidiscovery.com/blog/introducing-chai-1
https://www.biorxiv.org/content/10.1101/2024.10.10.615955v2",0.00,SOTA improvement,Matches or beats AF3 on Ligand PoseBusters,,,7.760572400000001e+21,"From paper: 128 A100s for 30 days; assumptions: 30% utilization rate, FP16 precision","PDB (Protein Data Bank), AlphaFold database (AFDB)",,,,,720.0,Taken from paper: 128 A100s for 30 days,NVIDIA A100,128,,,"We introduce Chai-1, a multi-modal foundation model for molecular structure prediction that performs at the state-of-the-art across a variety of tasks relevant to drug discovery. Chai-1 can optionally be prompted with experimental restraints (e.g. derived from wet-lab data) which boosts
performance by double-digit percentage points. Chai-1 can also be run in single-sequence mode without MSAs while preserving most of its performance. We release Chai-1 model weights and inference code as a Python package for non-commercial use and via a web interface where it can be used for free including for commercial drug discovery purposes.",Open weights (non-commercial),,Confident,United States of America,,,,128,Taken from paper,Industry,Open (non-commercial),https://github.com/chaidiscovery/chai-lab?tab=License-1-ov-file,Industry,,,112771.00249279808,Hardware
Genesis,Biology,Protein design,"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),Swiss Institute of Bioinformatics,Imperial College London,University of Oxford,Prescient Design","Zander Harteveld, Alexandra Van Hall-Beauvais, Irina Morozova, Joshua Southern, Casper Goverde, Sandrine Georgeon, Stéphane Rosset, Michëal Defferrard, Andreas Loukas, Pierre Vandergheynst, Michael M. Bronstein, Bruno E. Correia",2024-10-08,Exploring “dark-matter” protein folds using deep learning,https://www.cell.com/cell-systems/fulltext/S2405-4712(24)00270-9,4.00,,,,,,,,,40701,"According to the description:
40,726 pairs of corrupted and native protein structures
40,726 = 4.07e4

Note: While training was done over 300 epochs, only unique datapoints are counted.

Final calculation: 4.07e4",,,,,,,,"De novo protein design explores uncharted sequence and structure space to generate novel proteins not sampled by evolution. A main challenge in de novo design involves crafting “designable” structural templates to guide the sequence searches toward adopting target structures. We present a convolutional variational autoencoder that learns patterns of protein structure, dubbed Genesis. We coupled Genesis with trRosetta to design sequences for a set of protein folds and found that Genesis is capable of reconstructing native-like distance and angle distributions for five native folds and three novel, the so-called “dark-matter” folds as a demonstration of generalizability. We used a high-throughput assay to characterize the stability of the designs through protease resistance, obtaining encouraging success rates for folded proteins. Genesis enables exploration of the protein fold space within minutes, unrestricted by protein topologies. Our approach addresses the backbone designability problem, showing that small neural networks can efficiently learn structural patterns in proteins. A record of this paper’s transparent peer review process is included in the supplemental information.",,,Unverified,"Switzerland,Switzerland,United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,United States of America",,,,,,"Academia,Academia,Academia,Industry",,,"Academia,Academia,Academia,Industry",,,,
CLEAN-Contact,Biology,Enzyme function prediction,,"Yuxin Yang, Abby Jerger, Song Feng, Zixu Wang, Christina Brasfield, Margaret S. Cheung, Jeremy Zucker, Qiang Guan",2024-10-08,CLEAN-Contact: Contrastive Learning-enabled Enzyme Functional Annotation Prediction with Structural Inference,https://www.biorxiv.org/content/10.1101/2024.05.14.594148v2.abstract,0.00,,,,,,,,,224743,"224,742 = 2.24742e5 datapoints (unique protein sequences with contact maps from Swiss-Prot database)",,,,,,,,,,,Unverified,,,,,,,,,,,,,,
SCUBA-D,Biology,Protein design,"University of Science and Technology of China,Oristruct Biotech Company,iFLYTEK Research","Yufeng Liu, Sheng Wang, Jixin Dong, Linghui Chen, Xinyu Wang, Lei Wang, Fudong Li, Chenchen Wang, Jiahai Zhang, Yuzhu Wang, Si Wei, Quan Chen, Haiyan Liu ",2024-10-09,De novo protein design with a denoising diffusion network independent of pretrained structure prediction models,https://www.nature.com/articles/s41592-024-02437-w,0.00,,,,,,,,,,,,,,,,,,"The recent success of RFdiffusion, a method for protein structure design with a denoising diffusion probabilistic model, has relied on fine-tuning the RoseTTAFold structure prediction network for protein backbone denoising. Here, we introduce SCUBA-diffusion (SCUBA-D), a protein backbone denoising diffusion probabilistic model freshly trained by considering co-diffusion of sequence representation to enhance model regularization and adversarial
losses to minimize data-out-of-distribution errors. While matching the performance of the pretrained RoseTTAFold-based RFdiffusion in generating experimentally realizable protein structures, SCUBA-D readily generates protein structures with not-yet-observed overall folds that are different from those predictable with RoseTTAFold. The accuracy of SCUBA-D was confirmed by the X-ray structures of 16 designed proteins and a protein complex, and by experiments validating designed heme-binding proteins and Ras-binding proteins. Our work shows that deep generative models of images or texts can be fruitfully extended to complex physical objects like protein structures by addressing outstanding issues such as the data-out-of-distribution errors.",,,Unknown,"China,United Kingdom of Great Britain and Northern Ireland,China",,,,,,"Academia,Industry,Industry",,,"Academia,Industry,Industry",,,,
AF_unmasked,Biology,Protein folding prediction,"Linköping University,Uppsala University,Stockholm University","Claudio Mirabello, Björn Wallner, Björn Nystedt, Stavros Azinas, Marta Carroni",2024-10-09,Unmasking AlphaFold to integrate experiments and predictions in multimeric complexes,https://www.nature.com/articles/s41467-024-52951-w,3.00,,,,,,,,,,,,,,,,,,"Since the release of AlphaFold, researchers have actively refined its predictions and attempted to integrate it into existing pipelines for determining protein structures. These efforts have introduced a number of functionalities and optimisations at the latest Critical Assessment of protein Structure Prediction edition (CASP15), resulting in a marked improvement in the prediction of multimeric protein structures. However, AlphaFold’s capability of predicting large protein complexes is still limited and integrating experimental data in the prediction pipeline is not straightforward. In this study, we introduce AF_unmasked to overcome these limitations. Our results demonstrate that AF_unmasked can integrate experimental information to build larger or hard to predict protein assemblies with high confidence. The resulting predictions can help interpret and augment experimental data. This approach generates high quality (DockQ score > 0.8) structures even when little to no evolutionary information is available and imperfect experimental structures are used as a starting point. AF_unmasked is developed and optimised to fill incomplete experimental structures (structural inpainting), which may provide insights into protein dynamics. In summary, AF_unmasked provides an easy-to-use method that efficiently integrates experiments to predict large protein complexes more confidently.",,,Unverified,"Sweden,Sweden,Sweden",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
SO3LR,Biology,"Protein folding prediction,Molecular simulation","University of Luxembourg,Technische Universitat Berlin,Berlin Institute for the Foundations of Learning and Data,DeepMind,Max Planck Institute for Informatics,Korea University","Adil Kabylda, J. Thorben Frank, Sergio Suarez Dou, Almaz Khabibrakhmanov, Leonardo Medrano Sandonas, Oliver T. Unke, Stefan Chmiela, Klaus-Robert Muller, Alexandre Tkatchenko",2024-10-08,Molecular Simulations with a Pretrained Neural Network and Universal Pairwise Force Fields,https://chemrxiv.org/engage/chemrxiv/article-details/6704263051558a15ef6478b6,0.00,,,,,120000000000000080000.00,"1. Hardware: 1x NVIDIA H100 SXM5 80GB (9.90 x 10^14 FLOP/s)
2. Training duration: 86 hours (provided directly) = 309,600 seconds
3. Utilization: 40%
4. Calculation: 9.90 x 10^14 FLOP/s × 1 GPU × 309,600s × 0.4 = 1.2 × 10^20 FLOPs",,,4000001,"GEMS fragments: 2,700,000
QM7-X molecules: 1,000,000
AQM gas-phase drugs: 60,000
SPICE dipeptides: 33,000
DES molecular dimers: 15,000
Gas-phase water clusters: 10,000

2,700,000 + 1,000,000 + 60,000 + 33,000 + 15,000 + 10,000 = 3,818,000 ≈ 4,000,000 datapoints",,,,,,,,"Machine Learning Force Fields (MLFFs) promise to enable general molecular simulations that can simultaneously achieve efficiency, accuracy, transferability, and scalability for diverse molecules, materials, and hybrid interfaces. A key step toward this goal has been made with the GEMS approach to biomolecular dynamics [Sci. Adv. 10, eadn4397 (2024)]. This work introduces the SO3LR method that integrates the fast and stable SO3krates neural network for semi-local interactions with universal pairwise force fields designed for short-range repulsion, long-range electrostatics, and dispersion interactions. SO3LR is trained on a diverse set of 4 million neutral and charged molecular complexes
computed at the PBE0+MBD level of quantum mechanics, ensuring a comprehensive coverage of covalent and non-covalent interactions. Our approach is characterized by computational and data efficiency, scalability to 200 thousand atoms on a single GPU, and reasonable to high accuracy across the chemical space of organic (bio)molecules. SO3LR is applied to study units of four major biomolecule types, polypeptide folding, and nanosecond dynamics of larger systems such as a protein, a glycoprotein, and a lipid bilayer, all in explicit solvent. Finally, we discuss the future challenges toward truly general molecular simulations by combining MLFFs with traditional atomistic models.",,,Unverified,"Luxembourg,Germany,Germany,United Kingdom of Great Britain and Northern Ireland,Germany,Korea (Republic of)",,,,,,"Academia,Research collective,Industry,Academia,Academia",,,"Academia,Research collective,Industry,Academia,Academia",,,,Hardware
InstructPLM,Biology,"Protein generation,Protein folding prediction","Zhejiang Lab,Zhejiang University,Nanjing University,Tsinghua University,Alibaba,Chinese University of Hong Kong (CUHK)","Jiezhong Qiu, Junde Xu, Jie Hu, Hanqun Cao, Liya Hou, Zijun Gao, Xinyi Zhou, Anni Li, Xiujuan Li, Bin Cui, Fei Yang, Shuang Peng, Ning Sun, Fangyu Wang, Aimin Pan, Jie Tang, Jieping Ye, Junyang Lin, Jin Tang, Xingxu Huang, Pheng Ann Heng, Guangyong Chen",2024-04-20,InstructPLM: Aligning Protein Language Models to Follow Protein Structure Instructions,https://www.biorxiv.org/content/10.1101/2024.04.17.589642v1.abstract,3.00,,,,,,,,,5400001,"InstructPLM Training Data Points:
- Training Proteins: 18,024
- Avg Sequence Length: 300
- Total Tokens = 18,024 × 300 = 5,407,200 ≈ 5.4 × 10^6 data points",,,,,,,,"Large language models are renowned for their efficacy in capturing intricate patterns, including co-evolutionary relationships, and underlying protein languages. However, current methodologies often fall short in illustrating the emergence of genomic insertions, duplications, and insertion/deletions (indels), which account for approximately 14% of human pathogenic mutations. Given that structure dictates function, mutated proteins with similar structures are more likely to persist throughout biological evolution. Motivated by this, we leverage crossmodality alignment and instruct fine-tuning techniques inspired by large language models to align a generative protein language model with protein structure instructions. Specifically, we present a method for generating variable-length and diverse proteins to explore and simulate the complex evolution of life, thereby expanding the repertoire of options for protein engineering. Our proposed protein LM-based approach, InstructPLM, demonstrates significant performance enhancements both in silico and in vitro. On native protein backbones, it achieves a perplexity of 2.68 and a sequence recovery rate of 57.51, surpassing Protein-MPNN by 39.2% and 25.1%, respectively. Furthermore, we validate the efficacy of our model by redesigning PETase and L-MDH. For PETase, all fifteen designed variable-length PETase exhibit depolymerization activity, with eleven surpassing the activity levels of the wild type. Regarding L-MDH, an enzyme lacking an experimentally determined structure, InstructPLM is able to design functional enzymes with an AF2-predicted structure. Code and model weights of InstructPLM are publicly available*.",,,Unverified,"China,China,China,China,China,Hong Kong,China",,,,,,"Academia,Academia,Academia,Industry,Academia",,,"Academia,Academia,Academia,Industry,Academia",,,,
Prothyena,Biology,Protein or nucleotide language model (pLM/nLM),Tokyo Institute of Technology,"Yiming Zhang, Manabu Okumura",2024-01-22,ProtHyena: A fast and efficient foundation protein language model at single amino acid Resolution,https://www.biorxiv.org/content/10.1101/2024.01.18.576206v1.abstract,2.00,,,,,,,,,3000000001,"Data Estimate Summary:
10,000,000 sequences × 300 amino acids = 3,000,000,000 tokens (3 billion)",,,,,,,,"The emergence of self-supervised deep language models has revolutionized natural language processing tasks and has recently extended its applications to biological sequence analysis. Traditional models, primarily based on the Transformer and BERT architectures, demonstrate substantial effectiveness in various applications. However, these models are inherently constrained by the attention mechanism’s quadratic computational complexity O(L2), limiting their efficiency and the length of context they can process. Addressing these limitations, we introduce ProtHyena, a novel approach that leverages the Hyena operator. This innovative methodology circumvents the constraints imposed by attention mechanisms, thereby reducing the time complexity to a subquadratic, enabling the modeling of extra-long protein sequences at the single amino acid level without the need to compress data. ProtHyena is able to achieve, and in many cases exceed, state-of-the-art results in various downstream tasks with only 10% of the parameters typically required by attention-based models. The architecture of ProtHyena presents a highly efficient solution for training protein predictors, offering a promising avenue for fast and efficient analysis of biological sequences.",,,Unverified,Japan,,,,,,Academia,,,Academia,,,,
LM-Design,Biology,Protein design,"ByteDance,University of Wisconsin Madison","Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, Quanquan Gu",,Structure-informed Language Models Are Protein Designers,https://proceedings.mlr.press/v202/zheng23a.html,46.00,,,6900000.00,"""That is because LMDESIGN has 6.9M parameters while ProteinMPNN+CMLM only has 1.6M parameters.""",140000000000001200000.00,"1. Hardware setup: 8x NVIDIA V100 GPUs (1.25 x 10^14 FLOP/s per GPU)

2. Training duration: 4 days (345,600 seconds) - estimated based on 10 epochs over 50M sequences with 6000 residues per batch

3. Utilization rate: 40%

4. Final calculation:
(1.25 x 10^14 FLOP/s/GPU × 8 GPUs) × 345,600 seconds × 0.4 = 1.4 x 10^20 FLOPs",,,15000000001,"Total tokens = Number of Sequences × Average Sequence Length
Total tokens = 50,000,000 × 300 = 1.5 × 10^10 tokens

Final estimate: 1.5e10 tokens",,,,,,,,"This paper demonstrates that language models are strong structure-based protein designers. We present LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds. We conduct a structural surgery on pLMs, where a lightweight structural adapter is implanted into pLMs and endows it with structural awareness. During inference, iterative refinement is performed to effectively optimize the generated protein sequences. Experiments show that LM-Design improves the state-of-the-art results by a large margin, leading to 4% to 12% accuracy gains in sequence recovery (e.g., 55.65%/56.63% on CATH 4.2/4.3 single-chain benchmarks, and >
60% when designing protein complexes). We provide extensive and in-depth analyses, which verify that LM-Design can (1) indeed leverage both structural and sequential knowledge to accurately handle structurally non-deterministic regions, (2) benefit from scaling data and model size, and (3) generalize to other proteins (e.g., antibodies and de novo proteins).
",,,Unverified,"China,United States of America",,,,,,"Industry,Academia",,,"Industry,Academia",,,,Hardware
Protllm,Biology,Protein or nucleotide language model (pLM/nLM),"Beijing Institute of Technology,Beihang University,Peking University,Smart Grid Research Institute,Shanghai AI Lab","Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, Wentao Zhang",2024-02-28,ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training,https://arxiv.org/abs/2403.07920,8.00,,,,,55000000000000030000.00,"1. Hardware setup: 4x NVIDIA A100 GPUs (3.12e14 FLOP/s per GPU)
2. Training duration: ~30.6 hours (estimated based on total FLOPs, hardware capacity, and utilization)
3. Utilization rate: 40%
4. Final calculation: 
   Total tokens (10,000 × 256 × 512) × FLOPs per token (7B × 6) = 1.31072e9 × 4.2e10 ≈ 5.506e19 FLOPs",,,1310000001,"Summary of calculations:

PubMed Articles: 165,206 × 10,000 = 1.65206 × 10^9
UniProt Annotations: 64,634 × 100 = 6.4634 × 10^6
STRING Annotations: 25,682 × 100 = 2.5682 × 10^6
Mol-Instructions: 173,973 × 500 = 86.9865 × 10^6

Dataset total: 1.65206 × 10^9 + 6.4634 × 10^6 + 2.5682 × 10^6 + 86.9865 × 10^6 = 1.7475 × 10^9

Training tokens: 10,000 × 256 × 512 = 1.31072 × 10^9

Final estimate: 1.31 × 10^9 data points",,,,,,,,"We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic supervised protein-centric tasks and explore its novel protein-language applications. Experimental results demonstrate that ProtLLM not only achieves superior performance against protein-specialized baselines on protein-centric tasks but also induces zero-shot and in-context learning capabilities on protein-language tasks.",,,Unverified,"China,China,China,China,China",,,,,,"Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia",,,,Operation counting
DiffPALM,Biology,Protein or nucleotide language model (pLM/nLM),"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),Swiss Institute of Bioinformatics","Umberto Lupo, Damiano Sgarbossa, Anne-Florence Bitbol",2024-06-24,Pairing interacting protein sequences using masked language modeling,https://www.pnas.org/doi/abs/10.1073/pnas.2311887121,6.00,,,,,,,,,40000001,"Single MSA data points = 125 × 300 = 37,500 tokens
Total data points = 37,500 × 1,000 = 37,500,000 tokens
Final estimate = 4.0 × 10^7 tokens",,,,,,,,"Predicting which proteins interact together from amino acid sequences is an important task. We develop a method to pair interacting protein sequences which leverages the power of protein language models trained on multiple sequence alignments (MSAs), such as MSA Transformer and the EvoFormer module of AlphaFold. We formulate the problem of pairing interacting partners among the paralogs of two protein families in a differentiable way. We introduce a method called Differentiable Pairing using Alignment-based Language Models (DiffPALM) that solves it by exploiting the ability of MSA Transformer to fill in masked amino acids in multiple sequence alignments using the surrounding context. MSA Transformer encodes coevolution between functionally or structurally coupled amino acids within protein chains. It also captures inter-chain coevolution, despite being trained on single-chain data. Relying on MSA Transformer without fine-tuning, DiffPALM outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments extracted from ubiquitous prokaryotic protein datasets. It also outperforms an alternative method based on a state-of-the-art protein language model trained on single sequences. Paired alignments of interacting protein sequences are a crucial ingredient of supervised deep learning methods to predict the three-dimensional structure of protein complexes. Starting from sequences paired by DiffPALM substantially improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer. It also achieves competitive performance with using orthology-based pairing.",,,Unverified,"Switzerland,Switzerland",,,,,,Academia,,,Academia,,,,
LLPS,Biology,Protein or nucleotide language model (pLM/nLM),InstaDeep,"Benoit Gaujac, Jérémie Donà, Liviu Copoiu, Timothy Atkinson, Thomas Pierrot, Thomas D. Barrett",2024-05-24,Learning the Language of Protein Structure,https://arxiv.org/abs/2405.15840,4.00,,,,,,,,,70000001,"70 million total structural tokens 
= 310,000 protein structures x ~225 tokens/structure
= 7.0 x 10^7 tokens",,,,,,,,"Representation learning and \emph{de novo} generation of proteins are pivotal computational biology tasks. Whilst natural language processing (NLP) techniques have proven highly effective for protein sequence modelling, structure modelling presents a complex challenge, primarily due to its continuous and three-dimensional nature. Motivated by this discrepancy, we introduce an approach using a vector-quantized autoencoder that effectively tokenizes protein structures into discrete representations. This method transforms the continuous, complex space of protein structures into a manageable, discrete format with a codebook ranging from 4096 to 64000 tokens, achieving high-fidelity reconstructions with backbone root mean square deviations (RMSD) of approximately 1-5 Å. To demonstrate the efficacy of our learned representations, we show that a simple GPT model trained on our codebooks can generate novel, diverse, and designable protein structures. Our approach not only provides representations of protein structure, but also mitigates the challenges of disparate modal representations and sets a foundation for seamless, multi-modal integration, enhancing the capabilities of computational methods in protein design.",,,Unverified,United Kingdom of Great Britain and Northern Ireland,,,,,,Industry,,,Industry,,,,
FoldFlow2,Biology,"Protein generation,Protein design","Dreamfold,University of Montreal / Université de Montréal,McGill University,University of Oxford","Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, Alexander Tong, Avishek Joey Bose",2024-05-30,Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Backbone Generation,https://arxiv.org/abs/2405.20313,8.00,,,,,85999999999999770000.00,"1. Hardware setup: 2x NVIDIA A100 40GB GPUs (3.12 x 10^14 FLOP/s per GPU)
2. Training duration: 4 days (directly provided) = 345,600 seconds
3. Utilization rate: 40%
4. Calculation: 2 GPUs × 3.12×10^14 FLOP/s × 345,600s × 0.4 = 8.6×10^19 FLOPs",,,32000001,"Average protein length = (60 + 384) / 2 = 222 residues
Total datapoints = 160,000 structures × 222 residues = 35,520,000
Final estimate ≈ 3.2 × 10^7 datapoints",,,,,,,,"Proteins are essential for almost all biological processes and derive their diverse functions from complex 3D structures, which are in turn determined by their amino acid sequences. In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow-2, a novel sequence-conditioned SE(3)-equivariant flow matching model for protein structure generation. FoldFlow-2 presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase diversity and novelty of generated samples -- crucial for de-novo drug design -- we train FoldFlow-2 at scale on a new dataset that is an order of magnitude larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow-2 to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow-2 outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow-2 makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.",,,Unverified,"Canada,Canada,Canada,United Kingdom of Great Britain and Northern Ireland",,,,,,"Industry,Academia,Academia,Academia",,,"Industry,Academia,Academia,Academia",,,,Hardware
ProteinChat,Biology,Protein function prediction,"University of California San Diego,BioMap Research,The Scripps Research Institute,Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)","Mingjia Huo, Han Guo, Xingyi Cheng, Digvijay Singh, Hamidreza Rahmani, Shen Li, Philipp Gerlof, Trey Ideker, Danielle A. Grotjahn, Elizabeth Villa, Le Song, Pengtao Xie",2024-10-10,Multi-Modal Large Language Model Enables Protein Function Prediction,https://www.researchsquare.com/article/rs-4941886/v1,2.00,,,,,100000000000000000000.00,"1. Hardware setup: 8x NVIDIA A100 GPUs (3.12×10¹⁴ FLOP/s per GPU in fp16_tensor mode)

2. Training duration: Estimated using 210,000 steps × 0.1 seconds/step = 21,000 seconds (≈5.83 hours)

3. Utilization rate: 40%

4. Final calculation:
   * Per GPU effective: 3.12×10¹⁴ × 0.4 = 1.248×10¹⁴ FLOP/s
   * All GPUs: 1.248×10¹⁴ × 8 = 9.984×10¹⁴ FLOP/s
   * Total: 9.984×10¹⁴ FLOP/s × 21,000 seconds = 2.09664×10¹⁹ FLOPs
   * Adjusted estimate: 1×10²⁰ FLOPs",,,1050000001,1.5M triplets * (600 protein_tokens + 100 prompt_answer_tokens) = 1.5M * 700 = 1.05B tokens,,,,,,,,"Predicting the functions of proteins can greatly accelerate biological discovery and applications, where deep learning methods have recently shown great potential. However, these methods predominantly predict protein functions as discrete categories, which fails to capture the nuanced and complex nature of protein functions. Furthermore, existing methods require the development of separate models for each prediction task, a process that can be both resource-heavy and time-consuming. Here, we present ProteinChat, a versatile, multi-modal large language model that takes a protein's amino acid sequence as input and generates comprehensive narratives describing its function. ProteinChat is trained using over 1,500,000 (protein, prompt, answer) triplets curated from the Swiss-Prot dataset, covering diverse functions. This novel model can universally predict a wide range of protein functions, all within a single, unified framework. Furthermore, ProteinChat supports interactive dialogues with human users, allowing for iterative refinement of predictions and deeper exploration of protein functions. Our experimental results, evaluated through both human expert assessment and automated metrics, demonstrate that ProteinChat outperforms general-purpose LLMs like GPT-4, one of the flagship LLMs, by over ten-fold. In addition, ProteinChat exceeds or matches the performance of task-specific prediction models.",,,Unverified,"United States of America,China,United States of America,United Arab Emirates",,,,,,"Academia,Industry,Academia",,,"Academia,Industry,Academia",,,,Hardware
DiffDock-PP,Biology,Protein interaction prediction,"Technical University of Munich,Massachusetts Institute of Technology (MIT)","Mohamed Amine Ketata, Cedrik Laue, Ruslan Mammadov, Hannes Stärk, Menghua Wu, Gabriele Corso, Céline Marquet, Regina Barzilay, Tommi S. Jaakkola",2023-04-08,DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models,https://arxiv.org/abs/2304.03889,38.00,,,,,,,,,42827,"42,826 datapoints = 42,826 binary protein complexes
Final result = 4.2826e4",,,,,,,,"Understanding how proteins structurally interact is crucial to modern biology, with applications in drug discovery and protein design. Recent machine learning methods have formulated protein-small molecule docking as a generative problem with significant performance boosts over both traditional and deep learning baselines. In this work, we propose a similar approach for rigid protein-protein docking: DiffDock-PP is a diffusion generative model that learns to translate and rotate unbound protein structures into their bound conformations. We achieve state-of-the-art performance on DIPS with a median C-RMSD of 4.85, outperforming all considered baselines. Additionally, DiffDock-PP is faster than all search-based methods and generates reliable confidence estimates for its predictions. Our code is publicly available at $\texttt{this https URL}$",,,Unverified,"Germany,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
DiffBindFR,Biology,"Protein-ligand binding affinity prediction,Protein-ligand contact prediction","Peking University,Tsinghua-Peiking Center for Life Sciences","Jintao Zhu, Zhonghui Gu, Jianfeng Pei, Luhua Lai",2024-04-09,DiffBindFR: an SE(3) equivariant network for flexible protein–ligand docking,https://pubs.rsc.org/en/content/articlehtml/2024/sc/d3sc06803j,4.00,,,,,400000000000000200000.00,"1. Hardware setup:
- Main model: 8× NVIDIA A800 GPUs (7.80×10¹³ FLOP/s per GPU)
- MDN model: 4× NVIDIA Tesla V100-SXM2 GPUs (1.25×10¹⁴ FLOP/s per GPU)

2. Training duration (estimated from steps and step time):
- Main model: 262,000 steps × 5s = 1.31×10⁶ seconds (~15 days)
- MDN model: 65,000 steps × 5s = 3.25×10⁵ seconds (~3.76 days)

3. Utilization rate: 40%

4. Final calculation:
Main: 8 GPUs × 7.80×10¹³ FLOP/s × 1.31×10⁶ s × 0.4 = 3.31×10²⁰ FLOPs
MDN: 4 GPUs × 1.25×10¹⁴ FLOP/s × 3.25×10⁵ s × 0.4 = 6.50×10¹⁹ FLOPs
Total: 3.31×10²⁰ + 6.50×10¹⁹ ≈ 4.0×10²⁰ FLOPs",,,16741,"Data points = 16,739 training structures
Note: Epochs (1000) not counted as we only consider unique data points

16,739 = 1.674e4",,,,,,,,"Molecular docking, a key technique in structure-based drug design, plays pivotal roles in protein–ligand interaction modeling, hit identification and optimization, in which accurate prediction of protein–ligand binding mode is essential. Conventional docking approaches perform well in redocking tasks with known protein binding pocket conformation in the complex state. However, in real-world docking scenario without knowing the protein binding conformation for a new ligand, accurately modeling the binding complex structure remains challenging as flexible docking is computationally expensive and inaccurate. Typical deep learning-based docking methods do not explicitly consider protein side chain conformations and fail to ensure the physical plausibility and detailed atomic interactions. In this study, we present DiffBindFR, a full-atom diffusion-based flexible docking model that operates over the product space of ligand overall movements and flexibility and pocket side chain torsion changes. We show that DiffBindFR has higher accuracy in producing native-like binding structures with physically plausible and detailed interactions than available docking methods. Furthermore, in the Apo and AlphaFold2 modeled structures, DiffBindFR demonstrates superior advantages in accurate ligand binding pose and protein binding conformation prediction, making it suitable for Apo and AlphaFold2 structure-based drug design. DiffBindFR provides a powerful flexible docking tool for modeling accurate protein–ligand binding structures.",,,Unverified,"China,China",,,,,,"Academia,Research collective",,,"Academia,Research collective",,,,Hardware
Re-Dock,Biology,Protein-ligand contact prediction,"Zhejiang University,Westlake University,University of Washington","Yufei Huang, Odin Zhang, Lirong Wu, Cheng Tan, Haitao Lin, Zhangyang Gao, Siyuan Li, Stan. Z. Li",2024-02-21,Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge,https://arxiv.org/abs/2402.11459,2.00,,,,,75000000000000020000.00,"1. Hardware setup: 1x NVIDIA A100 GPU (3.12e14 FLOP/s)

2. Training duration: 7 days (directly provided) = 604,800 seconds

3. Utilization rate: 40%

4. Final calculation:
1 GPU × 3.12e14 FLOP/s × 604,800s × 0.4 = 7.55e19 FLOPs",,,20001,"20,000 data points

= Number of complexes in PDBBind v2020 dataset
= 20,000

Final: 20,000 (2.0e4)",,,,,,,,"Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods.",,,Unverified,"China,China,United States of America",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,Hardware
RoBERTa (modified),Biology,Protein or nucleotide language model (pLM/nLM),"IBM Research,ETH Zurich","Modestas Filipavicius, Matteo Manica, Joris Cadow, Maria Rodriguez Martinez",2020-12-05,Pre-training Protein Language Models with Label-Agnostic Binding Pairs Enhances Performance in Downstream Tasks,https://arxiv.org/abs/2012.03084,19.00,,,,,2200000000000002000.00,"1. Hardware: 4x NVIDIA Tesla P100 SXM2 GPUs (18.7 TFLOP/s per GPU in FP16)
2. Training duration: Not provided
3. Utilization rate: Not explicitly mentioned
4. Calculation: 
   - FLOPs per token = 6 * 93M params = 5.58e8
   - Total tokens = 10M sequences * 400 tokens = 4e9
   - Total FLOPs = 5.58e8 * 4e9 = 2.232e18 FLOPs",,,19000000001,"Total Sequences = STRING (6.67M) + SwissProt (0.504M) + Pfam (31M) = 38.17M sequences
Average tokens per sequence = 500
Total tokens = 38.17M × 500 = 19,085,000,000 ≈ 1.9 × 10^10 tokens",,,,,,,,"Less than 1% of protein sequences are structurally and functionally annotated. Natural Language Processing (NLP) community has recently embraced self-supervised learning as a powerful approach to learn representations from unlabeled text, in large part due to the attention-based context-aware Transformer models. In this work we present a modification to the RoBERTa model by inputting during pre-training a mixture of binding and non-binding protein sequences (from STRING database). However, the sequence pairs have no label to indicate their binding status, as the model relies solely on Masked Language Modeling (MLM) objective during pre-training. After fine-tuning, such approach surpasses models trained on single protein sequences for protein-protein binding prediction, TCR-epitope binding prediction, cellular-localization and remote homology classification tasks. We suggest that the Transformer's attention mechanism contributes to protein binding site discovery. Furthermore, we compress protein sequences by 64% with the Byte Pair Encoding (BPE) vocabulary consisting of 10K subwords, each around 3-4 amino acids long. Finally, to expand the model input space to even larger proteins and multi-protein assemblies, we pre-train Longformer models that support 2,048 tokens. Further work in token-level classification for secondary structure prediction is needed. Code available at: this https URL",,,Unverified,"United States of America,Multinational,Switzerland",,,,,,"Industry,Academia",,,"Industry,Academia",,,,Operation counting
eFold,Biology,Protein or nucleotide language model (pLM/nLM),"Harvard Medical School,Stanford University,Columbia University,University of Strasbourg","Silvi Rouskin, Alberic de Lajart, Yves Martin des Taillades, Colin Kalicki, Federico Fuchs Wightman, Justin Aruda, Dragui Salazar, Matthew Allan, Casper L'Esperance-Kerckhoff, Alex Kashi, Fabrice Jossinet",2024-04-04,Diverse Database and Machine Learning Model to Narrow the Generalization Gap in RNA Structure Prediction,https://www.researchsquare.com/article/rs-4159627/v1,,,,,,,,,,,,,,,,,,,"Understanding macromolecular structures of proteins and nucleic acids is critical for discerning their functions and biological roles. Advanced techniques—crystallography, NMR, and CryoEM—have facilitated the determination of over 180,000 protein structures, all cataloged in the Protein Data Bank (PDB). This comprehensive repository has been pivotal in developing deep learning algorithms for predicting protein structures directly from sequences. In contrast, RNA structure prediction has lagged, and suffers from a scarcity of structural data. Here, we present the secondary structure models of 1098 pri-miRNAs and 1456 human mRNA regions determined through chemical probing. We develop a novel deep learning architecture, inspired from the Evoformer model of Alphafold and traditional architectures for secondary structure prediction. This new model, eFold, was trained on our newly generated database and over 300,000 secondary structures across multiple sources. We benchmark eFold on two new test sets of long and diverse RNA structures and show that our dataset and new architecture contribute to increasing the prediction performance, compared to similar state-of-the-art methods. All together, our results reveal that merely expanding the database size is insufficient for generalization across families, whereas incorporating a greater diversity and complexity of RNAs structures allows for enhanced model performance.",,,Unverified,"United States of America,United States of America,United States of America,France",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
RNA-FM,Biology,RNA structure prediction,"Chinese University of Hong Kong (CUHK),Fudan University,Shanghai AI Lab,Harbin Institute of Technology,University of Electronic Science and Technology of China,Massachusetts Institute of Technology (MIT),Harvard University,Shanghai Zelixir Biotech,CUHK Shenzhen Research Institute","Jiayang Chen, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze Yu, Licheng Zong, Liang Hong, Jin Xiao, Tao Shen, Irwin King, Yu Li",2022-08-08,Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions,https://arxiv.org/abs/2204.00300,78.00,,,,,2.59999999999998e+21,"1. Hardware setup: 8x NVIDIA A100 GPUs (3.12E14 FLOP/s per GPU)

2. Training duration: 30 days estimated (2,592,000 seconds)

3. Utilization rate: 40%

4. Calculation: 3.12E14 FLOP/s × 8 GPUs × 2,592,000 seconds × 0.4 = 2.59E21 FLOPs",,,23500000001,"23 million sequences × 1024 tokens per sequence:
23,000,000 × 1,024 = 23 × 10⁶ × 1.024 × 10³ = 2.35 × 10¹⁰ tokens",,,,,,,,"Non-coding RNA structure and function are essential to understanding various biological processes, such
as cell signaling, gene expression, and post-transcriptional regulations. These are all among the core problems
in the RNA field. With the rapid growth of sequencing technology, we have accumulated a massive amount
of unannotated RNA sequences. On the other hand, expensive experimental observatory results in only limited
numbers of annotated data and 3D structures. Hence, it is still challenging to design computational methods for
predicting their structures and functions. The lack of annotated data and systematic study causes inferior performance. To resolve the issue, we propose a novel RNA foundation model (RNA-FM) to take advantage of all
the 23 million non-coding RNA sequences through self-supervised learning. Within this approach, we discover
that the pre-trained RNA-FM could infer sequential and evolutionary information of non-coding RNAs without
using any labels. Furthermore, we demonstrate RNA-FM’s effectiveness by applying it to the downstream secondary/3D structure prediction, SARS-CoV-2 genome structure and evolution prediction, protein-RNA binding
preference modeling, and gene expression regulation modeling. The comprehensive experiments show that the
proposed method improves the RNA structural and functional modelling results significantly and consistently.
Despite only being trained with unlabelled data, RNA-FM can serve as the foundational model for the field.",,,Unverified,"Hong Kong,China,China,China,China,China,United States of America,United States of America,China,China",,,,,,"Academia,Academia,Academia,Academia,Academia,Academia,Academia,Industry",,,"Academia,Academia,Academia,Academia,Academia,Academia,Academia,Industry",,,,Hardware
ProstT5,Biology,Protein or nucleotide language model (pLM/nLM),"Technical University of Munich,Seoul National University,Institute for Advanced Study,TUM School of Life Sciences Weihenstephan","Michael Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, Milot Mirdita, Martin Steinegger, Burkhard Rost",2024-03-24,Bilingual Language Model for Protein Sequence and Structure,https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2.abstract,50.00,,,,,3.0999999999999906e+21,"1. Hardware setup: 8x NVIDIA A100 GPUs (3.12e14 FLOP/s per GPU)

2. Training duration: Directly provided - 36 days total (10 days pre-training + 26 days translation training) = 3,110,400 seconds

3. Utilization rate: 40%

4. Final calculation:
3.12e14 FLOP/s × 8 GPUs × 3,110,400s × 0.4 = 3.1e21 FLOPs",,,8100000001,"Number of samples = 34M
Tokens per sequence = 238
Total = 34,000,000 × 238 = 8,092,000,000 tokens ≈ 8.1B",,,,,,,,"Adapting large language models (LLMs) to protein sequences spawned the development of powerful
protein language models (pLMs). Concurrently, AlphaFold2 broke through in protein structure prediction.
Now we can systematically and comprehensively explore the dual nature of proteins that act and exist
as three-dimensional (3D) machines and evolve as linear strings of one-dimensional (1D) sequences.
Here, we leverage pLMs to simultaneously model both modalities by combining 1D sequences with 3D
structure in a single model. We encode protein structures as token sequences using the 3Di-alphabet
introduced by the 3D-alignment method Foldseek. This new foundation pLM extracts the features and
patterns of the resulting “structure-sequence” representation. Toward this end, we built a non-redundant
dataset from AlphaFoldDB and fine-tuned an existing pLM (ProtT5) to translate between 3Di and amino
acid sequences. As a proof-of-concept for our novel approach, dubbed Protein structure-sequence T5
(ProstT5), we showed improved performance for subsequent prediction tasks, and for “inverse folding”,
namely the generation of novel protein sequences adopting a given structural scaffold (“fold”). Our work
showcased the potential of pLMs to tap into the information-rich protein structure revolution fueled by
AlphaFold2. ProstT5 paves the way to develop new tools integrating the vast resource of 3D predictions,
and opens new research avenues in the post-AlphaFold2 era. Our model is freely available for all at
https://github.com/mheinzinger/ProstT5.",,,Unverified,"Germany,Korea (Republic of),United States of America,Germany",,,,,,"Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia",,,,Hardware
SI-PLM,Biology,Protein or nucleotide language model (pLM/nLM),University of Pittsburgh,"Daniel Peñaherrera, David Ryan Koes",2024-04-23,Structure-Infused Protein Language Models,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11071282/,,,,,,,,,,,,,,,,,,,"Embeddings from protein language models (PLM’s) capture intricate patterns
for protein sequences, enabling more accurate and efficient prediction of protein
properties. Incorporating protein structure information as direct input into PLMs
results in an improvement on the predictive ability of protein embeddings on
downstream tasks. In this work we demonstrate that indirectly infusing structure
information into PLMs also leads to performance gains on structure related tasks.
The key difference between this framework and others is that at inference time the
model does not require access to structure to produce its embeddings.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
ALICE,Biology,Protein design,"Nanhu Brain-Computer Interface Institute,Lingang Laboratory,Medical School of Nantong University,Zhejiang University School of Medicine","Hanyu Zheng, Binjie Guo, Aisheng Mo, Hongyan Wei, Yile Wu, Xurong Lin, Haohan Jiang, Hengguang Li, Yunshuo Zhang, Zhuoyuan Song, Xuebin Ni, Yan Huang, Xiaosong Gu, Bin Yu, Ningtao Cheng, Xuhua Wang",2024-10-11,Mapping AAV capsid sequences to functions through function-guided in silico evolution,https://www.biorxiv.org/content/10.1101/2024.10.11.617764v1,0.00,,,,,,,,,20230001,2.89e6 sequences × 7 residues/sequence = 2.023e7 datapoints,,,,,,,,"Artificial intelligence (AI) has been suggested to facilitate time- and cost-effective functional engineering of adeno-associated virus (AAV) capsid sequences. Nevertheless, an AI-empowered approach to identify AAV capsid sequence-to-multifunction relationships remains elusive. To overcome this challenge, we propose a machine-intelligent design method to map an AAV capsid sequence to multiple functions, thereby enabling direct in silico engineering of AAV capsids. To fuse multiple functions into a single capsid sequence, a heuristic algorithm coupled with contrastive learning and reinforcement learning, named function-guided evolution (FE), was introduced to steer further evolution of the high-performing capsid sequences generated by a naive language model toward functions. We then illustrated the evolutionary mechanism of the FE approach for function-guided generation of capsid sequences. Further optimization steers the evolution toward desired functions within a function-guided landscape. Despite the constraint of datasets of only 129 entries, we successfully constructed a model to map AAV capsid sequences to multiple functions of improved viability coupled with central nervous system (CNS) tropism. In vivo experiments confirmed that two of the top eight engineered variants exhibited enhanced viability and remarkable CNS tropism. This interpretable machine-intelligent design method represents a pioneering effort enabling direct in silico engineering of AAV capsids for effective gene delivery",,,Unverified,"China,China,China,China",,,,,,Academia,,,Academia,,,,
HaloClass,Biology,Protein classification,"University of California Davis,Pt. Jawahar Lal Nehru Memorial Medical College,Purdue University","Kush Narang, Abhigyan Nath, William Hemstrom, Simon K. S. Chu",2024-10-10,HaloClass: Salt-Tolerant Protein Classification with Protein Language Models,https://www.researchsquare.com/article/rs-5027369/v1,,,,,,,,,,28031,"Initial sequences: 38,361
After clustering: 28,030
Training split (90%): 28,030 × 0.9 = 25,227 sequences

Final estimate: 2.803e4",,,,,,,,"Salt-tolerant proteins, also known as halophilic proteins, have unique adaptations to function in high-salinity environments. These proteins have naturally evolved in extremophilic organisms, and more recently, are being increasingly applied as enzymes in industrial processes. Due to an abundance of salt-tolerant sequences and a simultaneous lack of experimental structures, most computational methods to predict stability are sequence-based only. These approaches, however, are hindered by a lack of structural understanding of these proteins. Here, we present HaloClass, an SVM classifier that leverages ESM-2 protein language model embeddings to accurately identify salt-tolerant proteins. On a newer and larger test dataset, HaloClass outperforms existing approaches when predicting the stability of never-before-seen proteins that are distal to its training set. Finally, on a mutation study that evaluated changes in salt tolerance based on single- and multiple-point mutants, HaloClass outperforms existing approaches, suggesting applications in the guided design of salt-tolerant enzymes.",,,Unverified,"United States of America,India,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
PROPERMAB,Biology,Antibody property prediction,Regeneron,"Bian Li, Shukun Luo, Wenhua Wang, Jiahui Xu, Dingjiang Liu, Mohammed Shameem, John Mattila, Matthew Franklin, Peter G. Hawkins, Gurinder S. Atwal",2024-10-12,PROPERMAB: an integrative framework for in silico prediction of antibody developability using machine learning,https://www.biorxiv.org/content/10.1101/2024.10.10.616558v1,0.00,,,,,,,,,12001,"Feature Prediction: 12,000 mAbs
HIC Retention Time: 135 mAbs
Viscosity Prediction: 60 mAbs

Total = 12,000 + 135 + 60 = 12,195 mAbs (1.2e4)",,,,,,,,"Selection of lead therapeutic molecules is often driven predominantly by pharmacological efficacy and safety. Candidate developability, such as biophysical properties that affect the formulation of the molecule into a product, is usually evaluated only toward the end of the drug development pipeline. The ability to evaluate developability properties early in the process of antibody therapeutic development could accelerate the timeline from discovery to clinic and save considerable resources. In silico predictive approaches, such as machine learning models, which map molecules to predictions of developability properties could offer a cost-effective and high-throughput alternative to experiments for antibody developability assessment. We developed a computational framework, PROPERMAB, for large-scale and efficient in silico prediction of developability properties for monoclonal antibodies, using custom molecular features and machine learning modeling. We demonstrate the power of PROPERMAB by using it to develop models to predict antibody hydrophobic interaction chromatography retention time and high-concentration viscosity. We further show that structure-derived features can be rapidly and accurately predicted directly from sequences by pre-training simple models for molecular features, thus providing the ability to scale these approaches to repertoire-scale sequence datasets.",,,Unverified,United States of America,,,,,,Industry,,,Industry,,,,
Yuel 2,Biology,Protein-ligand binding affinity prediction,,"Jian Wang, Nikolay V. Dokholyan",2024-10-12,Leveraging Transfer Learning for Predicting Protein-Small Molecule Interactions,https://doi.org/10.1101/2024.10.08.617219,0.00,,,,,,,,,250001,"Pre-training data = 250,000 protein-ligand pairs

250,000 = 2.5e5 unique data points",,,,,,,,"A complex web of intermolecular interactions defines and regulates biological processes.
Understanding this web has been particularly challenging because of the sheer number of actors
in biological systems: ~104 proteins in a typical human cell offer a plausible 108 interactions. This
number grows rapidly if we consider metabolites, drugs, nutrients, and other biological molecules.
The relative strength of interactions also critically affects these biological processes. However,
the small and often incomplete datasets (103-104 protein-ligand interactions) traditionally used for
binding affinity predictions limit the ability to capture the full complexity of these interactions. To
overcome this challenge, we developed Yuel 2, a novel neural network-based approach that
leverages transfer learning to address the limitations of small datasets. Yuel 2 is pre-trained on a
large-scale dataset to learn intricate structural features and then fine-tuned on specialized
datasets like PDBbind to enhance the predictive accuracy and robustness. We show that Yuel 2
predicts multiple binding affinity metrics – Kd, Ki, IC50, and EC50 – between proteins and small
molecules, offering a comprehensive representation of molecular interactions crucial for drug
design and development.",,,Unverified,,,,,,,,,,,,,,
vScreenML 2.0,Biology,Drug discovery,"Fox Chase Cancer Center,Temple University School of Pharmacy","Grigorii V. Andrianov, Emeline Haroldsen, John Karanicolas",2024-10-12,vScreenML v2.0: Improved Machine Learning Classification for Reducing False Positives in Structure-Based Virtual Screening,https://doi.org/10.1101/2024.10.08.617248,0.00,,,,,,,,,,,,,,,,,,"Enthusiastic adoption of make-on-demand chemical libraries for virtual screening has highlighted the need for methods that deliver improved hit-finding discovery rates. Traditional virtual screening methods are often inaccurate, with most compounds nominated in a virtual screen not engaging the intended target protein to any detectable extent. Emerging machine learning approaches have made significant progress in this regard, including our previously-described tool vScreenML. Broad adoption of vScreenML was hindered by its challenging usability and dependencies on certain obsolete or proprietary software packages. Here, we introduce vScreenML 2.0 (https://github.com/gandrianov/vScreenML2) to address each of these limitations with a streamlined Python implementation. Through careful benchmarks, we show that vScreenML 2.0 outperforms other widely-used tools for virtual screening hit discovery.",,,Unverified,"United States of America,United States of America",,,,,,Academia,,,Academia,,,,
MolPath,"Biology,Materials science",Small molecule property prediction,"Southwest Petroleum University,East China Normal University","Honghao Wang, Acong Zhang, Yuan Zhong, Junlei Tang, Kai Zhang, Ping Li",2024-10-13,Chain-aware graph neural networks for molecular property prediction,https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btae574/7818417,0.00,,,,,,,,,16001,"ESOL: 1,128 × 0.8 = 902
FreeSolv: 642 × 0.8 = 514
Lipo: 4,200 × 0.8 = 3,360
BBBP: 2,039 × 0.8 = 1,631
Tox21: 7,831 × 0.8 = 6,265
SIDER: 1,427 × 0.8 = 1,142
ClinTox: 1,478 × 0.8 = 1,182
BACE: 1,513 × 0.8 = 1,210

Total: 902 + 514 + 3,360 + 1,631 + 6,265 + 1,142 + 1,182 + 1,210 = 16,206

Final result: 1.6 × 10⁴",,,,,,,,"Motivation: Predicting the properties of molecules is a fundamental problem in drug design and discovery, while how to learn effective feature representations lies at the core of modern deep learning based prediction methods. Recent progress shows expressive power of graph neural networks (GNNs) in capturing structural information for molecular graphs. However, we find that most molecular graphs exhibit low clustering along with dominating chains. Such topological characteristics can induce feature squashing during message passing and thus impair the expressivity of conventional GNNs. Results: Aiming at improving node features' expressiveness, we develop a novel chain-aware graph neural network model, wherein the chain structures are captured by learning the representation of the center node along the shortest paths starting from it, and the redundancy between layers are mitigated via initial residual difference connection (IRDC). Then the molecular graph is represented by attentive pooling of all node representations. Compared to standard graph convolution, our chain-aware learning scheme offers a more straightforward feature interaction between distant nodes, thus it is able to capture the information about long-range dependency. We provide extensive empirical analysis on real-world datasets to show the outperformance of the proposed method.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
"P-Mistral, P-Llama2, P-LLama3, P-gemma",Biology,Protein design,University of Siena,"Kamyar Zeinalipour, Neda Jamshidi, Monica Bianchini, Marco Maggini, Marco Gori",2024-08-12,Design Proteins Using Large Language Models: Enhancements and Comparative Analyses,https://arxiv.org/abs/2408.06396,0.00,,,,,1400000000000012000.00,"1. Hardware: 4x NVIDIA RTX A6000 (3.87×10¹³ FLOPs/s per GPU)
2. Training duration: 6.28 hours (estimated from total FLOPs divided by effective FLOPs/s)
3. Utilization: 40%
4. Calculation: 2000 steps × 16 grad_accum × 12 FLOPs/param/token × 7×10⁹ params × 512 tokens = 1.4×10¹⁸ FLOPs",,,21500001,"42,000 protein sequences × 512 tokens/sequence = 21,504,000 total tokens (2.15 × 10⁷ datapoints)",,,,,,,,"Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition. In this paper, we explore the application of LLMs in the generation of high-quality protein sequences. Specifically, we adopt a suite of pre-trained LLMs, including Mistral-7B1, Llama-2-7B2, Llama-3-8B3, and gemma-7B4, to produce valid protein sequences. All of these models are publicly available.5 Unlike previous work in this field, our approach utilizes a relatively small dataset comprising 42,000 distinct human protein sequences. We retrain these models to process protein-related data, ensuring the generation of biologically feasible protein structures. Our findings demonstrate that even with limited data, the adapted models exhibit efficiency comparable to established protein-focused models such as ProGen varieties, ProtGPT2, and ProLLaMA, which were trained on millions of protein sequences. To validate and quantify the performance of our models, we conduct comparative analyses employing standard metrics such as pLDDT, RMSD, TM-score, and REU. Furthermore, we commit to making the trained versions of all four models publicly available, fostering greater transparency and collaboration in the field of computational biology.",,,Unverified,Italy,,,,,,,,,,,,,Operation counting
Deep Learning Enabled Discovery of Kinase Drug Targets in Pharos,Biology,Protein-ligand binding affinity prediction,"West Virginia University,University of New Mexico","Ádám M. Halász, Srinjoy Das, Stephen L. Mathias, Jeremy S. Edwards",2024-10-11,DEEP LEARNING ENABLED DISCOVERY OF KINASE DRUG TARGETS IN PHAROS,https://doi.org/10.1101/2024.10.08.612754,0.00,,,,,,,,,50001,"Number of data points ≈ 50,000 unique kinase-ligand affinity pairs

Details:
- Original data: 80,878 kinase-ligand affinity values
- Preprocessed to: 455 kinases × 5,275 ligands matrix
- Final training data: ~50,000 non-zero entries",,,,,,,,"We use machine learning with a standardized molecular structure and gene ontology data to predict ligand
interactions for a set of human kinases. We realize this by leveraging information from the TCRD / Pharos
database, developed and maintained within the Illuminating the Druggable Genome (IDG) project.
Pharos collects relevant biochemical and clinically relevant information of a large set of biologically
important (human) proteins from publicly available sources, including scientific publications as well as
specialized databases. The 635 kinases listed in Pharos are classified into levels reflecting the relative
amount and type of accumulated information. Importantly, molecular structure and Gene Ontology
annotations are available for the entire set, but only 455 of the kinases have recorded ligand affinity data.
We developed a deep neural network-based framework to predict the ligand affinity profile for kinases
using generally available information (molecular structure and Gene Ontology annotations) as input. The
input data is organized into a 2,770 – dimensional vector with binary entries. The output data are predicted
affinity values for interactions between the respective kinase and possible ligands.
To address the very large number of possible ligands (58,800) and the sparsity of available binding data,
we organized the ligands into 5,275 clusters based on structural similarity measures. Our model framework
is trained to predict likely interactions between kinases and these ligand clusters.
We aim to identify sets of likely ligand partners associated with high predicted relative affinities for a given
kinase. We measure performance by evaluating the efficiency in identifying known ligand partners for
documented kinases that were not included in the training data. Our results indicate that our model
framework can identify sets of ligands that will contain a significant fraction of the correct (known) ligand
partners.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
HelixFold,Biology,Protein folding prediction,Baidu,"Xiaomin Fang, Jie Gao, Jing Hu, Lihang Liu, Yang Xue, Xiaonan Zhang, Kunrui Zhu",2024-05-17,HelixFold-Multimer: Elevating Protein Complex Structure Prediction to New Heights,https://arxiv.org/html/2404.10260v2,4.00,,,,,,,,,,,,,,,,,,"While monomer protein structure prediction tools boast impressive accuracy, the prediction of protein complex structures remains a daunting challenge in the field. This challenge is particularly pronounced in scenarios involving complexes with protein chains from different species, such as antigen-antibody interactions, where accuracy often falls short. Limited by the accuracy of complex prediction, tasks based on precise protein-protein interaction analysis also face obstacles. In this report, we highlight the ongoing advancements of our protein complex structure prediction model, HelixFold-Multimer, underscoring its enhanced performance. HelixFold-Multimer provides precise predictions for diverse protein complex structures, especially in therapeutic protein interactions. Notably, HelixFold-Multimer achieves remarkable success in antigen-antibody and peptide-protein structure prediction, greatly surpassing AlphaFold 3. HelixFold-Multimer is now available for public use on the PaddleHelix platform, offering both a general version and an antigen-antibody version. Researchers can conveniently access and utilize this service for their development needs.",,,Unverified,China,,,,,,Industry,,,Industry,,,,
OmniNA,Biology,Protein or nucleotide language model (pLM/nLM),Tianjin Medical University,"Xilin Shen, Xiangchun Li",2024-01-15,OmniNA: A foundation model for nucleotide sequences,https://www.biorxiv.org/content/10.1101/2024.01.14.575543v1.abstract,3.00,,,,,2.50000000000002e+21,"1. Hardware setup: 8x NVIDIA A100 SXM4 80GB GPUs @ 3.12e14 FLOP/s per GPU

2. Training duration: 29 days (estimated from total compute divided by effective FLOP/s)

3. Utilization rate: 40%

4. Calculation:
Total FLOPs = 6 × 1.7e9 (params) × 601 (seq_len) × 2048 (batch) × 200000 (steps) = 2.5e21 FLOPs",,,1076400000001,"From Nucleotide Sequences: 1,076,200,000,000 tokens
From Text Annotations: 197,000,000 tokens
Total: 1,076,200,000,000 + 197,000,000 = 1,076,397,000,000 (1.0764e12)",,,,,,,,"Foundation models have demonstrated exceptional efficacy across diverse downstream tasks. However, within the realms of genomics and transcriptomics, a notable gap persists in the availability of models that afford a comprehensive understanding of nucleotide sequence principles across various species. Here, we present OmniNA, a foundation generative model designed for comprehensive nucleotide sequence learning. The model was pre-trained on 91.7 million nucleotide sequences and the corresponding annotations encompassing 1076.2 billion bases and 197 million words spanning a multitude of species. We demonstrated OmniNA gains the capacity to understand the semantics of the nucleotide sequence and textual annotations by analyzing the learned representation of the pre-trained model. OmniNA can be fine-tuned to align multiple nucleotide learning tasks with natural language paradigms. We demonstrate OmniNA-1.7B surpasses or rivals state-of-the art methods in 17 nucleotide tasks, encompassing nucleotide sequences detection and species classification. The model’s understanding of nucleotide grammars enhances its capability to reveal the mutation effect of nucleotide sequence on DNA and RNA processing. We hereby release the OmniNA-1.7B model as an open-source contribution to the research community. This foundation model signifies a step toward advancing our comprehension of nucleotide sequences across diverse species and holds substantial promise to facilitating genomics and transcriptomics research.",,,Unverified,China,,,,,,Academia,,,Academia,,,,Operation counting
Machine learning a model for RNA structure prediction,Biology,RNA structure prediction,"International School for Advanced Studies,Institute of Structural Biology,Technical University of Munich","Nicola Calonaci, Alisha Jones, Francesca Cuturello, Michael Sattler, Giovanni Bussi",2020-11-16,Machine learning a model for RNA structure prediction ,https://academic.oup.com/nargab/article/2/4/lqaa090/5983421,32.00,,,,,1799999999999987000.00,"1. Hardware setup: 
576 CPUs (288 nodes × 2 CPUs/node), Intel Xeon E5-2683 v4
FLOP/s per CPU: 2.688 × 10¹¹ FLOP/s
Total system: 1.548288 × 10¹⁴ FLOP/s

2. Training duration: 
Directly provided: 8 hours = 28,800 seconds

3. Utilization rate: 
40% (0.4)

4. Final calculation:
1.548288 × 10¹⁴ FLOP/s × 28,800s × 0.4 = 1.78 × 10¹⁸ FLOPs",,,4001,"(52 + 388) / 2 = 220 nucleotides per RNA
18 RNA molecules × 220 nucleotides = 3,960 datapoints
Rounded to: 4,000 datapoints",,,,,,,,"RNA function crucially depends on its structure. Thermodynamic models currently used for secondary structure prediction rely on computing the partition function of folding ensembles, and can thus estimate minimum free-energy structures and ensemble populations. These models sometimes fail in identifying native structures unless complemented by auxiliary experimental data. Here, we build a set of models that combine thermodynamic parameters, chemical probing data (DMS and SHAPE) and co-evolutionary data (direct coupling analysis) through a network that outputs perturbations to the ensemble free energy. Perturbations are trained to increase the ensemble populations of a representative set of known native RNA structures. In the chemical probing nodes of the network, a convolutional window combines neighboring reactivities, enlightening their structural information content and the contribution of local conformational ensembles. Regularization is used to limit overfitting and improve transferability. The most transferable model is selected through a cross-validation strategy that estimates the performance of models on systems on which they are not trained. With the selected model we obtain increased ensemble populations for native structures and more accurate predictions in an independent validation set. The flexibility of the approach allows the model to be easily retrained and adapted to incorporate arbitrary experimental information.",,,Unverified,"Italy,Germany,Germany",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,Hardware
RNA-FrameFlow,Biology,RNA design,"National University of Singapore,Prescient Design,University of Missouri,University of Cambridge","Rishabh Anand, Chaitanya K. Joshi, Alex Morehead, Arian R. Jamasb, Charles Harris, Simon V. Mathis, Kieran Didi, Bryan Hooi, Pietro Liò",2024-06-19,RNA-FrameFlow: Flow Matching for de novo 3D RNA Backbone Design,https://arxiv.org/abs/2406.13839,1.00,,,,,14000000000000119000.00,"1. Hardware: 4x NVIDIA GeForce RTX 3090 (1.60e+14 FLOP/s per GPU)
2. Training duration: 15 hours (directly provided) = 54,000 seconds
3. Utilization: 40%
4. Calculation: 1.60e+14 FLOP/s × 4 GPUs × 54,000 seconds × 0.4 = 1.4e+19 FLOPs",,,638001,"6,382 sequences = 5,319 (primary) + 1,063 (augmented)
638,200 datapoints = 6,382 sequences × 100 nucleotides/sequence

Final estimate: 638,200 datapoints",,,,,,,,,,,Unverified,"Singapore,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,"Academia,Industry,Academia,Academia",,,"Academia,Industry,Academia,Academia",,,,Hardware
GPT-MolBERTa,Biology,Molecular property prediction,Carnegie Mellon University (CMU),"Suryanarayanan Balaji, Rishikesh Magar, Yayati Jadhav, Amir Barati Farimani",2023-09-20,GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction,https://arxiv.org/abs/2310.03030,11.00,,,,,,,,,32600001,"326,000 molecules × 100 tokens/molecule = 32,600,000 (3.26e7) total tokens",,,,,,,,"With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention mechanisms show that GPT-MolBERTa is able to pick up important information from the input textual data, displaying the interpretability of the model.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
MahLool,Biology,Protein property prediction,University of Rochester,"Mehrad Ansari, Andrew D. White",2023-04-03,"Serverless Prediction of Peptide Properties with Recurrent Neural
Networks",https://pubs.acs.org/doi/full/10.1021/acs.jcim.2c01317,0.00,,,,,,,,,8990001,"DATA TOKENS = 44,954 sequences * 200 amino acids/sequence = 8,990,800 tokens
FINAL = 8,990,800 (8.99e6) unique tokens seen in first epoch

Key calculations:
- Total sequences: 9,316 + 18,453 + 17,185 = 44,954
- Total tokens: 44,954 * 200 = 8,990,800",,,,,,,,"We present three deep learning sequence-based prediction models for peptide properties including hemolysis, solubility, and resistance to nonspecific interactions that achieve comparable results to the state-of-the-art models. Our sequence-based solubility predictor, MahLooL, outperforms the current state-of-the-art methods for short peptides. These models are implemented as a static website without the use of a dedicated server or cloud computing. Web-based models like this allow for accessible and effective reproducibility. Most existing approaches rely on third-party servers that typically require upkeep and maintenance. Our predictive models do not require servers, require no installation of dependencies, and work across a range of devices. The specific architecture is bidirectional recurrent neural networks. This serverless approach is a demonstration of edge machine learning that removes the dependence on cloud providers. The code and models are accessible at https://github.com/ur-whitelab/peptide-dashboard.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
PLAPT,Biology,Protein-ligand binding affinity prediction,"Wolfram Research,ASC27,Newport High School,Sanskriti School","Tyler Rose, Nicolò Monti, Navvye Anand, Tianyu Shen",2024-02-12,PLAPT: Protein-Ligand Binding Affinity Prediction Using Pretrained Transformers,https://www.biorxiv.org/content/10.1101/2024.02.08.575577v3.abstract,1.00,,,1474624.00,"""Figure 3: The prediction module takes a 1792x1 feature vector as its input, which is then partitioned into two streams: The first stream processes the first 1024 indices of the feature vector through a 512-node protein-specific linear layer, followed by a ReLU activation function. Concurrently, the second stream processes the latter 768 feature indices through a similar 512-node moleculespecific linear layer, also followed by a ReLU activation.  Outputs from both streams are concatenated into a single vector of 1024 elements. This combined vector is passed through a batch normalization layer with a momentum of 0.9 and epsilon of 0.001. The vector is then fed through the 512-node Linear Layer 1 and ReLU activation. A dropout layer with a probability of 20% is then applied to mitigate overfitting.  Following the dropout layer, the prediction module continues to reduce the feature space with the 64-node Linear Layer 2 with ReLU activation, before reaching the single node Linear Layer 3. This layer outputs the predicted scalar value representing the normalized negative log10 affinity value, which is then un-normalized.""  (512 * 1024) + (512 * 768) + (512 * 1024) + (64 * 512) + 64 = 1474624",796296960000.00,"1474624 connections; 90,000 training examples; assuming only one epoch (no mention otherwise). ",,,313000001,"Training set: 90,000 samples
Tokens per sample: 3,200 (protein) + 278 (ligand) = 3,478
Total tokens = 90,000 × 3,478 = 313,020,000 (3.13e8) tokens",,,,NVIDIA GeForce RTX 4060 Ti,1,,,"Predicting protein-ligand binding affinity is crucial for drug discovery, as it enables efficient identification of drug candidates. We introduce PLAPT, a novel model utilizing transfer learning from pre-trained transformers like ProtBERT and ChemBERTa to predict binding affinities with high accuracy. Our method processes one-dimensional protein and ligand sequences, leveraging a branching neural network architecture for feature integration and affinity estimation. We demonstrate PLAPT’s superior performance through validation on multiple datasets, achieving state-of-the-art results while requiring significantly less computational resources for training compared to existing models. Our findings indicate that PLAPT offers a highly effective and accessible approach for accelerating drug discovery efforts.",,,Unverified,"United States of America,Italy,United States of America,India",,,,,,"Industry,Industry,Academia",,,"Industry,Industry,Academia",,,353.2155262475759,Operation counting
SYNTERACT,Biology,Protein interaction prediction,University of Delaware,"Logan Hallee, Jason P. Gleghorn",2023-06-09,Protein-Protein Interaction Prediction is Achievable with Large Language Models,https://www.biorxiv.org/content/10.1101/2023.06.07.544109v1.abstract,,,,,,,,,,180000001,"353,976 pairs × 500 avg amino acids = 176,988,000 tokens ≈ 1.8 × 10⁸ tokens

{pairs calculation: 179,018 + 3,958 + 170,000 = 353,976}",,,,,,,,"Predicting protein-protein interactions (PPIs) is vital for elucidating fundamental biology, designing peptide therapeutics, and for high-throughput protein annotation. This is particularly relevant in the current biotechnology landscape characterized by the proliferation of protein generative models, which necessitate a high-throughput and generalized PPI predictor for proteins regardless of conventional motifs or known biological functions. Our work addresses this need and provides strong evidence of the utility and reliability of protein language models (pLMs) in learning the PPI objective. We demonstrated that with the use of a sizable balanced dataset, pLMs achieve state-of-the-art performance metrics in PPI prediction on diverse proteins. To generate a dataset that allows for the approximation of these conditions, we implemented a novel synthetic data generation scheme to augment BIOGRID and Negatome datasets. The enhancement of these datasets was then used to fine-tune ProtBERT for PPI prediction to develop a model that we call SYNTERACT (SYNThetic data-driven protein-protein intERACtion Transformer). Our results are compelling, demonstrating 92% accuracy on validated positive and negative interacting pairs derived from 50 different organisms, all of which were excluded from the training phase. In addition to the high metrics, secondary analysis revealed that our synthetic negative data was able to successfully mimic actual negative samples, further reinforcing the integrity of synthetic data additions to PPI datasets. Another notable discovery was the ease in which previously existing PPI datasets could be predicted with simplistic features, calling into question if they can actually inform PPI prediction. We find that the subcellular compartment bias inherent to the compilation of these datasets is learnable with deep learning methods and demonstrate that our approach is not burdened by this disadvantage.",,,Unverified,United States of America,ProtBERT-BFD,,,70,,Academia,,,Academia,,,,
Genie (bio),Biology,Protein design,Colombia University,"Yeqing Lin, Mohammed AlQuraishi",2023-01-29,"Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds",https://arxiv.org/abs/2301.12485,,,,4100000.00,"""RFDiffusion also contains around 14 times more parameters than Genie (59.8M versus 4.1M).""",2.0000000000000003e+21,"1. Hardware setup:
- Base Model: 2x NVIDIA A100 (3.12e14 FLOP/s per GPU)
- Genie-SCOPe: 12x NVIDIA A100 (3.12e14 FLOP/s per GPU)
- Genie-SwissProt: 6x NVIDIA A6000 (3.87e13 FLOP/s per GPU)

2. Training duration (directly provided):
- Base Model: 9 days (777,600 seconds)
- Genie-SCOPe: 14 days (1,209,600 seconds)
- Genie-SwissProt: 8 days (691,200 seconds)

3. Utilization rate: 40%

4. Calculations:
Base: 3.12e14 × 2 × 777,600 × 0.4 = 1.94e20 FLOPs
SCOPe: 3.12e14 × 12 × 1,209,600 × 0.4 = 1.81e21 FLOPs
SwissProt: 3.87e13 × 6 × 691,200 × 0.4 = 6.41e19 FLOPs
Total = 2.06e21 FLOPs ≈ 2.0e21 FLOPs",,,40000001,"SCOPe Dataset: 8,766 domains × 200 residues = 1,753,200 residues
AlphaFold-SwissProt: 195,214 proteins × 200 residues = 39,042,800 residues
Total: 1,753,200 + 39,042,800 = 40,796,000 residues ≈ 4.08 × 10⁷",,,"""We train Genie for 50,000 epochs (~9 days). For ",,,,,"Proteins power a vast array of functional processes in living cells. The capability to create new proteins with designed structures and functions would thus enable the engineering of cellular behavior and development of protein-based therapeutics and materials. Structure-based protein design aims to find structures that are designable (can be realized by a protein sequence), novel (have dissimilar geometry from natural proteins), and diverse (span a wide range of geometries). While advances in protein structure prediction have made it possible to predict structures of novel protein sequences, the combinatorially large space of sequences and structures limits the practicality of search-based methods. Generative models provide a compelling alternative, by implicitly learning the low-dimensional structure of complex data distributions. Here, we leverage recent advances in denoising diffusion probabilistic models and equivariant neural networks to develop Genie, a generative model of protein structures that performs discrete-time diffusion using a cloud of oriented reference frames in 3D space. Through in silico evaluations, we demonstrate that Genie generates protein backbones that are more designable, novel, and diverse than existing models. This indicates that Genie is capturing key aspects of the distribution of protein structure space and facilitates protein design with high success rates. Code for generating new proteins and training new versions of Genie is available at https://github. com/aqlaboratory/genie",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,Hardware
ProteinSGM,Biology,Protein design,University of Toronto,"Jin Sub Lee, Jisun Kim, Philip M. Kim",2023-02-04,ProteinSGM: Score-based generative modeling for de novo protein design,https://www.biorxiv.org/content/10.1101/2022.07.13.499967v2.abstract,,,,,,7500000000000000000.00,"""The model is trained with a single NVIDIA V100 GPU using a batch size of 8 and learning rate 1×10−4 for 2 million iterations, which consumes approximately 7 days."" Assume FP16 precision and 40% utilization. ",,,14001,"Total Proteins: 14,987
Training Set = 14,987 × 0.95 = 14,238
Final Estimate = 1.4 × 10^4 datapoints",,,,NVIDIA V100,1,,,"The generation of de novo protein structures with predefined function and properties remains a challenging problem in protein design. Diffusion models, a novel state-of-the-art class of generative models, have recently shown astounding empirical performance in image synthesis. Here we use image-based representations of protein structure to develop ProteinSGM, a score-based diffusion model that produces realistic de novo proteins and can inpaint plausible backbones and domains into structures of predefined length. With unconditional generation, we show that ProteinSGM can generate native-like protein structures, surpassing the performance of previously reported generative models. We experimentally validate some de novo designs and observe strong structural consistency with generated backbones. Finally, we apply conditional generation to de novo protein design by formulating it as an image inpainting problem, allowing precise and modular design of protein structure.",,,Unverified,Canada,,,,,,Academia,,,Academia,,,664.6544334085605,Hardware
NAEPro,Biology,Protein design,"University of California Santa Barbara (UCSB),Massachusetts Institute of Technology (MIT),Carnegie Mellon University (CMU)","Zhenqiao Song, Yunlong Zhao, Wenxian Shi, Yang Yang, Lei Li",2023-10-06,Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design,https://arxiv.org/abs/2310.04343,,,,,,169999999999999360.00,"1. Hardware setup: 1x NVIDIA RTX A6000 (3.87e13 FLOP/s per GPU, fp16_non_tensor)

2. Training duration: 11,010 seconds (≈3.06 hours) - estimated from:
   - 8,808 proteins / 8 batch size = 1,101 steps per epoch
   - 1,101 steps × 100 epochs = 110,100 total steps
   - Assumed 0.1s per step → 110,100 × 0.1s = 11,010s

3. Utilization rate: 40%

4. Final calculation:
   3.87e13 FLOP/s × 1 GPU × 11,010s × 0.4 = 1.7e17 FLOPs",,,2100001,"β-lactamase calculation: 5,427 proteins × 286 residues = 1,552,122 residues
Myoglobin calculation: 3,381 proteins × 153 residues = 517,143 residues
Total datapoints: 1,552,122 + 517,143 = 2,069,265 residues (≈ 2.1e6)",,,,,,,,"Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein’s sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional and conserved sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, β-lactamase and myoglobin. Experimental results show that our model achieves the highest binding affinity scores among the top-5, top-10 and top-30 candidates. These findings prove the capability of our model to design functional proteins. Furthermore, in-depth analysis further confirms our model’s ability to generate highly effective proteins capable of binding to their target metallocofactors1.",,,Unverified,"United States of America,United States of America,United States of America",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,Hardware
Improved motif-scaffolding with SE(3) flow matching,Biology,Protein design,"University of Oxford,Massachusetts Institute of Technology (MIT),Microsoft Research AI for Science","Jason Yim, Andrew Campbell, Emile Mathieu, Andrew Y. K. Foong, Michael Gastegger, José Jiménez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S. Veeling, Frank Noé, Regina Barzilay, Tommi S. Jaakkola",2024-01-08,Improved motif-scaffolding with SE(3) flow matching,https://arxiv.org/abs/2401.04082,,,,,,16000000000000008000.00,"1. Hardware: 2x NVIDIA RTX A6000 (3.87e13 FLOP/s per GPU)
2. Training duration: 6 days = 518,400 seconds (directly provided)
3. Utilization: 40%
4. Calculation: 
   2 GPUs × 3.87e13 FLOP/s × 518,400s × 0.40 = 1.6e19 FLOPs",,,,,,,,,,,,"Protein design often begins with the knowledge of a desired function from a motif which motif-scaffolding aims to construct a functional protein around. Recently, generative models have achieved breakthrough success in designing scaffolds for a range of motifs. However, generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab validation. In this work, we extend FrameFlow, an SE(3) flow matching model for protein backbone generation, to perform motif-scaffolding with two complementary approaches. The first is motif amortization, in which FrameFlow is trained with the motif as input using a data augmentation strategy. The second is motif guidance, which performs scaffolding using an estimate of the conditional score from FrameFlow without additional training. On a benchmark of 24 biologically meaningful motifs, we show our method achieves 2.5 times more designable and unique motif-scaffolds compared to state-of-the-art. Code: this https URL",,,Unverified,"United Kingdom of Great Britain and Northern Ireland,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany",,,,,,"Academia,Academia,Industry",,,"Academia,Academia,Industry",,,,Hardware
ProRNA3D-Single,Biology,Protein folding prediction,,"Rahmatullah Roche, Sumit Tarafder,  Debswapna Bhattacharya",2024-07-28,Single-sequence protein-RNA complex structure prediction by geometric attention-enabled pairing of biological language models,https://www.biorxiv.org/content/10.1101/2024.07.27.605468v1.abstract,,,,,,11000000000000008000.00,"1. Hardware: 1x NVIDIA A100 GPU (3.12e14 FLOP/s in fp16_tensor mode)

2. Training duration: Estimated - 25 hours (100 epochs × 15 min/epoch)

3. Utilization: 40% (0.4)

4. Calculation:
3.12e14 FLOP/s × 1 GPU × 90,000 seconds × 0.4 = 1.1e19 FLOPs",,,940001,"750 complexes × (1000 protein residues + 250 RNA nucleotides) per complex
= 750 × 1250
= 937,500 datapoints
≈ 9.4e5 datapoints",,,,,,,,"Ground-breaking progress has been made in structure prediction of biomolecular assemblies, including the recent breakthrough of AlphaFold 3. However, it remains challenging for AlphaFold 3 and other state-of-the-art deep learning-based methods to accurately predict protein-RNA complex structures, in part due to the limited availability of evolutionary and structural information related to protein-RNA interactions that are used as inputs to the existing approaches. Here, we introduce ProRNA3D-single, a new deep-learning framework for protein-RNA complex structure prediction with only single-sequence input. Using a novel geometric attention-enabled pairing of biological language models of protein and RNA, a previously unexplored avenue, ProRNA3D-single enables the prediction of interatomic protein-RNA interaction maps, which are then transformed into multi-scale geometric restraints for modeling 3D structures of protein-RNA complexes via geometry optimization. Benchmark tests show that ProRNA3D-single convincingly outperforms current stateof-the-art methods including AlphaFold 3, particularly when evolutionary information is limited; and exhibits remarkable robustness and performance resilience by attaining better accuracy with only single-sequence input than what most methods can achieve even with explicit evolutionary information. Freely available at https://github.com/Bhattacharya-Lab/ProRNA3D-single, ProRNA3Dsingle should be broadly useful for modeling 3D structures of protein-RNA complexes at scale, regardless of the availability of evolutionary information.",,,Unverified,,,,,,,,,,,,,,Hardware
CrossBind,Biology,Protein-ligand contact prediction,"Shanghai AI Lab,Fudan University,Loughborough University,Chinese University of Hong Kong (CUHK),Shanghai Jiao Tong University","Linglin Jing, Sheng Xu, Yifan Wang, Yuzhe Zhou, Tao Shen, Zhigang Ji, Hui Fang, Zhen Li, Siqi Sun",2024-03-24,CrossBind: Collaborative Cross-Modal Identification of Protein Nucleic-Acid-Binding Residues,https://ojs.aaai.org/index.php/AAAI/article/view/28044,,,,,,,,,,320001,"Total Proteins = 573 + 495 = 1068
Datapoints = 1068 x 300 = 320,400
Final result = 3.2e5",,,,,,,,"Accurate identification of protein nucleic acid binding residues poses a significant challenge with important implications for various biological processes and drug design. Many typical computational methods for protein analysis rely on a single model that could ignore either the semantic context of the protein or the global 3D geometric information. Consequently, these approaches may result in incomplete or inaccurate protein analysis. To address the above issue, in this paper, we present CrossBind, a novel collaborative cross modal approach for identifying binding residues by exploiting both protein geometric structure and its sequence prior knowledge extracted from a large scale protein language model. Specifically, our multi modal approach leverages a contrastive learning technique and atom wise attention to capture the positional relationships between atoms and residues, thereby incorporating fine grained local geometric knowledge, for better binding residue prediction. Extensive experimental results demonstrate that our approach outperforms the next best state of the art methods, GraphSite and GraphBind, on DNA and RNA datasets by 10.8/17.3% in terms of the harmonic mean of precision and recall (F1 Score) and 11.9/24.8% in Matthews correlation coefficient (MCC), respectively. We release the code at https://github.com/BEAM-Labs/CrossBind.",,,Unverified,"China,China,United Kingdom of Great Britain and Northern Ireland,Hong Kong,China,China",,,,,,"Academia,Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia,Academia",,,,
MBP,Biology,Protein-ligand binding affinity prediction,"University of Science and Technology of China,Tencent,Zhejiang University","Jiaxian Yan, Zhaofeng Ye, Ziyi Yang, Chengqiang Lu, Shengyu Zhang, Qi Liu, Jiezhong Qiu",2023-12-11,Multi-task bioassay pre-training for protein-ligand binding affinity prediction,https://academic.oup.com/bib/article/25/1/bbad451/7469349,,,,,"Approach 1: training FLOP = 2 * connections * 3 * training examples * epochs. ""For pre-training, the number of epochs was set to 100."" 
1.8e18 FLOP = 2 * connections * 3 *  * 100",1800000000000000000.00,"""We pre-trained our model on one NVIDIA A100-PCIE-40GB for about 16 h and fine-tuned for about 1 h."" Assume 40% utilization and FP16 precision.",,,,,,,,,,,,"Protein–ligand binding affinity (PLBA) prediction is the fundamental task in drug discovery. Recently, various deep learning-based models predict binding affinity by incorporating the three-dimensional (3D) structure of protein–ligand complexes as input and achieving astounding progress. However, due to the scarcity of high-quality training data, the generalization ability of current models is still limited. Although there is a vast amount of affinity data available in large-scale databases such as ChEMBL, issues such as inconsistent affinity measurement labels (i.e. IC50, Ki, Kd), different experimental conditions, and the lack of available 3D binding structures complicate the development of high-precision affinity prediction models using these data. To address these issues, we (i) propose Multi-task Bioassay Pre-training (MBP), a pre-training framework for structure-based PLBA prediction; (ii) construct a pre-training dataset called ChEMBL-Dock with more than 300k experimentally measured affinity labels and about 2.8M docked 3D structures. By introducing multi-task pre-training to treat the prediction of different affinity labels as different tasks and classifying relative rankings between samples from the same bioassay, MBP learns robust and transferrable structural knowledge from our new ChEMBL-Dock dataset with varied and noisy labels. Experiments substantiate the capability of MBP on the structure-based PLBA prediction task. To the best of our knowledge, MBP is the first affinity pre-training model and shows great potential for future development. MBP web-server is now available for free at: https://huggingface.co/spaces/jiaxianustc/mbp.",,,Unverified,"China,China,China",,,,,,"Academia,Industry,Academia",,,"Academia,Industry,Academia",,,,Hardware
PepPrCLIP,Biology,Protein design,"Duke University,Cornell University,Sanford Burnham Prebys Institute","Suhaas Bhat, Kalyan Palepu, Lauren Hong, Joey Mao, Tianzheng Ye, Rema Iyer, Lin Zhao, Tianlai Chen, Sophia Vincoff, Rio Watson, Tian Wang, Divya Srijay, Venkata Srikar Kavirayuni, Kseniia Kholina, Shrey Goel, Pranay Vure, Aniruddha J Desphande, Scott H Soderling, Matthew P DeLisa, Pranam Chatterjee",2024-07-22,De Novo Design of Peptide Binders to Conformationally Diverse Targets with Contrastive Language Modeling,https://pubmed.ncbi.nlm.nih.gov/39091799/,,,,,,1000000000000000000.00,"1. Hardware: 1x NVIDIA A100 80GB GPU (3.12e14 FLOP/s fp16)
2. Training duration: 6 hours (estimated based on dataset size and epochs)
3. Utilization: 40%
4. Calculation: 3.12e14 FLOP/s × 0.4 × (6 × 3600s) = 2.69e18 FLOP ≈ 1.0e18 FLOP",,,11601,"11,597 datapoints from noisy dataset training pairs used for pre-training

11,597 x 1 epoch = 11,597 total datapoints

Final estimate: 11,597 ≈ 1.16e4",56.00,,,NVIDIA A100 SXM4 80 GB,1,,,"Designing binders to target undruggable proteins presents a formidable challenge in drug discovery, requiring innovative approaches to overcome the lack of putative binding sites. Recently, generative models have been trained to design binding proteins via three-dimensional structures of target proteins, but as a result, struggle to design binders to disordered or conformationally unstable targets. In this work, we provide a generalizable algorithmic framework to design short, target-binding linear peptides, requiring only the amino acid sequence of the target protein. To do this, we propose a process to generate naturalistic peptide candidates through Gaussian perturbation of the peptidic latent space of the ESM-2 protein language model, and subsequently screen these novel linear sequences for target-selective interaction activity via a CLIP-based contrastive learning architecture. By integrating these generative and discriminative steps, we create a Peptide Prioritization via CLIP (PepPrCLIP) pipeline and validate highly-ranked, target-specific peptides experimentally, both as inhibitory peptides and as fusions to E3 ubiquitin ligase domains, demonstrating functionally potent binding and degradation of conformationally diverse protein targets in vitro. Overall, our design strategy provides a modular toolkit for designing short binding linear peptides to any target protein without the reliance on stable and ordered tertiary structure, enabling generation of programmable modulators to undruggable and disordered proteins such as transcription factors and fusion oncoproteins.",,,Unverified,"United States of America,United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,881.7133592995463,Hardware
PepMLM,Biology,Protein generation,"Duke University,Cornell University,McMaster University","Tianlai Chen, Madeleine Dumas, Rio Watson, Sophia Vincoff, Christina Peng, Lin Zhao, Lauren Hong, Sarah Pertsemlidis, Mayumi Shaepers-Cheu, Tian Zi Wang, Divya Srijay, Connor Monticello, Pranay Vure, Rishab Pulugurta, Kseniia Kholina, Shrey Goel, Matthew P DeLisa, Ray Truant, Hector C Aguilar, Pranam Chatterjee",2024-08-11,PepMLM: Target Sequence-Conditioned Generation of Therapeutic Peptide Binders via Span Masked Language Modelin,https://pmc.ncbi.nlm.nih.gov/articles/PMC10593082/,,,,,,,,,,,,,,,,,,,"Target proteins that lack accessible binding pockets and conformational stability have posed increasing challenges for drug development. Induced proximity strategies, such as PROTACs and molecular glues, have thus gained attention as pharmacological alternatives, but still require small molecule docking at binding pockets for targeted protein degradation. The computational design of protein-based binders presents unique opportunities to access ""undruggable"" targets, but have often relied on stable 3D structures or structure-influenced latent spaces for effective binder generation. In this work, we introduce PepMLM, a target sequence-conditioned generator of de novo linear peptide binders. By employing a novel span masking strategy that uniquely positions cognate peptide sequences at the C-terminus of target protein sequences, PepMLM fine-tunes the state-of-the-art ESM-2 pLM to fully reconstruct the binder region, achieving low perplexities matching or improving upon validated peptide-protein sequence pairs. After successful in silico benchmarking with AlphaFold-Multimer, outperforming RFDiffusion on structured targets, we experimentally verify PepMLM's efficacy via fusion of model-derived peptides to E3 ubiquitin ligase domains, demonstrating endogenous degradation of emergent viral phosphoproteins and Huntington's disease-driving proteins. In total, PepMLM enables the generative design of candidate binders to any target protein, without the requirement of target structure, empowering downstream therapeutic applications.",,,Unverified,"United States of America,United States of America,Canada",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
CLAPE-DB,Biology,Protein nucleotide interaction prediction,Tsinghua University," Yufan Liu, Boxue Tian",2024-01-03,Protein–DNA binding sites prediction based on pre-trained protein language model and contrastive learning,https://academic.oup.com/bib/article/25/1/bbad488/7505238,,,,,,,,,,474001,"Dataset1 TR646 residues: 314,139
Dataset2 TR573 residues: 159,883
314,139 + 159,883 = 474,022 total residues
Final data points = 474,022",,,,,,,,"Protein–DNA interaction is critical for life activities such as replication, transcription and splicing. Identifying protein–DNA binding residues is essential for modeling their interaction and downstream studies. However, developing accurate and efficient computational methods for this task remains challenging. Improvements in this area have the potential to drive novel applications in biotechnology and drug design. In this study, we propose a novel approach called Contrastive Learning And Pre-trained Encoder (CLAPE), which combines a pre-trained protein language model and the contrastive learning method to predict DNA binding residues. We trained the CLAPE-DB model on the protein–DNA binding sites dataset and evaluated the model performance and generalization ability through various experiments. The results showed that the area under ROC curve values of the CLAPE-DB model on the two benchmark datasets reached 0.871 and 0.881, respectively, indicating superior performance compared to other existing models. CLAPE-DB showed better generalization ability and was specific to DNA-binding sites. In addition, we trained CLAPE on different protein–ligand binding sites datasets, demonstrating that CLAPE is a general framework for binding sites prediction. To facilitate the scientific community, the benchmark datasets and codes are freely available at https://github.com/YAndrewL/clape.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
MsPBRsP,Biology,Protein interaction prediction,Zhengzhou University,"Yuguang Li,  Shuai Lu, Xiaofei Nan, Shoutao Zhang, Qinglei Zhou",2023-02-27,MsPBRsP: Multi-scale Protein Binding Residues Prediction Using Language Model,https://www.biorxiv.org/content/10.1101/2023.02.26.528265v1.abstract,,,,,,2700000000000003000.00,"1. Hardware setup: 1x NVIDIA RTX2080 GPU (2.01e+13 FP16 FLOP/s)

2. Training duration: Estimated 93.75 hours (337,500 seconds)
   Calculation: 5 folds × 5 runs × 30 epochs × 7.5 minutes/epoch = 93.75 hours

3. Utilization rate: 40%

4. Final calculation:
   2.01e+13 FLOP/s × 1 GPU × 337,500 seconds × 0.4 = 2.7e+18 FLOPs",,,1860001,"Total Residues: 2,068,793
Training Set (90%): 0.9 × 2,068,793 = 1,861,913 residues
Final Data Amount: 1.86M residues",,,,,,,,"Accurate prediction of protein binding residues (PBRs) from sequence is important for the understanding of cellular activity and helpful for the design of novel drug. However, experimental methods are time-consuming and expensive. In recent years, a lot of computational predictors based on machine learning and deep learning models are proposed to reduce such consumption. But those methods often use MSA tools such as PSI-BLAST or NetSurfP to generate some statistical features and enter them into predictive models as necessary supplementary input. The input generation process normally takes long time, and there is no standard to specify which and how many statistic results should be provided to a prediction model. In addition, prediction of PBRs relies on residue local context, but the most appropriate scale is undetermined. Most works pre-selected certain residue features as input and a scale size based on expertise for certain type of PBRs. In this study, we propose a general tool-free end-to-end framework that can be applied to all types of PBRs, Multi-scale Protein Binding Residues Prediction using language model (MsPBRsP). We adopt a pre-trained language model ProtTrans to save the large consumption caused by MSA tools, and use protein sequence alone as input to our model. To ease scale size uncertainty, we construct multi-size windows in attention layer and multi-size kernels in convolutional layer. We test our framework on various benchmark datasets including PBRs from protein-protein, protein-nucleotide, protein-small ligand, heterodimer, homodimer and antibody-antigen interactions. Compared with existing state-of-the-art methods, MsPBRsP achieves superior performance with less running time and higher prediction rates on every PBRs prediction task. Specifically, we boost F1 score by 27.1% and AUPRC score by 7.6% on NSP448 dataset and decrease running time from over 10 minutes to under 0.1s on average. The source code and datasets are available at https://github.com/biolushuai/MsPBRsP-for-multiple-PBRs-prediction.",,,Unverified,China,,,,,,Academia,,,Academia,,,,Hardware
HelixProtX,Biology,Protein generation,Baidu,"Zhiyuan Chen, Tianhao Chen, Chenggang Xie, Yang Xue, Xiaonan Zhang, Jingbo Zhou, Xiaomin Fang",2024-07-12,"Unifying Sequences, Structures, and Descriptions for Any-to-Any Protein Generation with the Large Multimodal Model HelixProtX",https://arxiv.org/abs/2407.09274,,,,,,280000000000000130000.00,"1. Hardware setup: 8x NVIDIA A100 GPUs, 3.12e14 FLOP/s per GPU
2. Training duration: Estimated 3.19 days based on 450,000 steps at batch size 102,400
3. Utilization rate: 40%
4. Calculation: 
   - Per GPU effective: 3.12e14 × 0.4 = 1.25e14 FLOP/s
   - Total system: 1.25e14 × 8 = 1.0e15 FLOP/s
   - Total compute: 1.0e15 FLOP/s × (3.19 days × 86400 s/day) = 2.8e20 FLOP",,,,,52.00,,,,,,,"Proteins are fundamental components of biological systems and can be represented through various modalities, including sequences, structures, and textual descriptions. Despite the advances in deep learning and scientific large language models (LLMs) for protein research, current methodologies predominantly focus on limited specialized tasks – often predicting one protein modality from another. These approaches restrict the understanding and generation of multimodal protein data. In contrast, large multimodal models have demonstrated potential capabilities in generating any-to-any content like text, images, and videos, thus enriching user interactions across various domains. Integrating these multimodal model technologies into protein research offers significant promise by potentially transforming how proteins are studied. To this end, we introduce HelixProtX, a system built upon the large multimodal model, aiming to offer a comprehensive solution to protein research by supporting any-to-any protein modality generation. Unlike existing methods, it allows for the transformation of any input protein modality into any desired protein modality. The experimental results affirm the advanced capabilities of HelixProtX, not only in generating functional descriptions from amino acid sequences but also in executing critical tasks such as designing protein sequences and structures from textual descriptions. Preliminary findings indicate that HelixProtX consistently achieves superior accuracy across a range of protein-related tasks, outperforming existing state-of-the-art models. By integrating multimodal large models into protein research, HelixProtX opens new avenues for understanding protein biology, thereby promising to accelerate scientific discovery.",Unreleased,,Unverified,China,,,,,,Industry,,,Industry,,,,Hardware
EvoBind2,Biology,Protein design,"Stockholm University,Science for Life Laboratory","Qiuzhen Li,  Efstathios Nikolaos Vlachos,  Patrick Bryant",2024-10-12,Design of linear and cyclic peptide binders of different lengths from protein sequence information,https://www.biorxiv.org/content/10.1101/2024.06.20.599739v2.abstract,,,,,,,,,,,,,,,,,,,"Structure prediction technology has revolutionised the field of protein design, but key questions such as how to design new functions remain. Many proteins exert their functions through interactions with other proteins, and a significant challenge is designing these interactions effectively. While most efforts have focused on larger, more stable proteins, shorter peptides offer advantages such as lower manufacturing costs, reduced steric hindrance, and the ability to traverse cell membranes when cyclized. However, less structural data is available for peptides and their flexibility makes them harder to design. Here, we present a method to design both novel linear and cyclic peptide binders of varying lengths based solely on a protein target sequence. Our approach does not specify a binding site or the length of the binder, making the procedure completely blind. We demonstrate that linear and cyclic peptide binders of different lengths can be designed with nM affinity in a single shot, and adversarial designs can be avoided through orthogonal in silico evaluation, tripling the success rate. Our protocol, EvoBind2 is freely available https://github.com/patrickbryant1/EvoBind.",,,Unverified,"Sweden,Sweden",,,,,,"Academia,Research collective",,,"Academia,Research collective",,,,
PepGLAD,Biology,Protein design,"Tsinghua University,Renmin University of China","Xiangzhe Kong, Yinjun Jia, Wenbing Huang, Yang Liu",2024-02-21,Full-Atom Peptide Design with Geometric Latent Diffusion,https://arxiv.org/abs/2402.13555,,,,,,25000000000000200000.00,"1. Hardware: 1x NVIDIA GeForce RTX 3090 (1.60e+14 FLOP/s)
2. Training duration: Estimated - (60 + 500 epochs) × 600s/epoch = 336,000s
3. Utilization: 40%
4. Final calculation: 1.60e+14 FLOP/s × 1 GPU × 336,000s × 0.4 = 2.5e+19 FLOPs",,,196601,"
PepBench Training: 4,157 entries × 10 residues = 41,570 residues
PepBDB Training: 8,434 entries × 10 residues = 84,340 residues
ProtFrag: 70,645 monomers × 1 residue = 70,645 residues

Total: 41,570 + 84,340 + 70,645 = 196,555 residues",,,,,,,,"Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable. Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences. In this paper, we propose a generative model for full-atom \textbf{Pep}tide design with \textbf{G}eometric \textbf{LA}tent \textbf{D}iffusion (PepGLAD) given the binding site. We first establish a benchmark consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature for systematic evaluation. We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry. To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes full-atom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space. For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes. Experimental Results show that our method not only improves diversity and binding affinity significantly in the task of sequence-structure co-design, but also excels at recovering reference structures for binding conformation generation.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
RNA-MSM,Biology,Protein or nucleotide language model (pLM/nLM),"Peking University,Shanghai AI Lab,Griffith University,Peng Cheng Laboratory,Shenzhen Bay Laboratory","Yikun Zhang, Mei Lang, Jiuhong Jiang, Zhiqiang Gao, Fan Xu, Thomas Litfin, Ke Chen, Jaswinder Singh, Xiansong Huang, Guoli Song, Yonghong Tian, Jian Zhan, Jie Chen, Yaoqi Zhou",2023-11-06,Multiple sequence alignment-based RNA language model and its application to structural inference,https://academic.oup.com/nar/article/52/1/e3/7369930,29.00,,,,10 blocks in the MSA transformer,,8 V100 32Gbs at FP32,,,64000001,"3932 (families) × 512 (sequences per family) × 32 (tokens per sequence) = 64,421,888 unique tokens

Total: 6.4e7 tokens",300.00,,,,,,,"Compared with proteins, DNA and RNA are more difficult languages to interpret because four-letter coded DNA/RNA sequences have less infor-
mation content than 20-letter coded protein sequences. While BERT (Bidirectional Encoder Representations from Transformers)-like language
models have been developed for RNA, they are ineffective at capturing the evolutionary information from homologous sequences because unlike
proteins, RNA sequences are less conserved. Here, we have developed an unsupervised multiple sequence alignment-based RNA language
model (RNA-MSM) by utilizing homologous sequences from an automatic pipeline, RNAcmap, as it can provide significantly more homologous se-
quences than manually annotated Rfam. We demonstrate that the resulting unsupervised, two-dimensional attention maps and one-dimensional
embeddings from RNA-MSM contain structural information. In fact, they can be directly mapped with high accuracy to 2D base pairing probabil-
ities and 1D solvent accessibilities, respectively. Further fine-tuning led to significantly improved performance on these two downstream tasks
compared with existing state-of-the-art techniques including SPOT-RNA2 and RNAsnap2. By comparison, RNA-FM, a BERT-based RNA language
model, performs worse than one-hot encoding with its embedding in base pair and solvent-accessible surface area prediction. We anticipate
that the pre-trained RNA-MSM model can be fine-tuned on many other tasks related to RNA structure and function.",Open weights (unrestricted),,Unverified,"China,China,Australia,China,China",,,,1,,"Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia",,,,
ERNIE-RNA,Biology,Protein or nucleotide language model (pLM/nLM),"Microsoft Research,Syngentech,Tsinghua University","Weijie Yin, Zhaoyu Zhang, Liang He, Rui Jiang, Shuo Zhang, Gan Liu, Xuegong Zhang, Tao Qin, Zhen Xie",2024-03-17,ERNIE-RNA: An RNA Language Model with Structure-enhanced Representations,https://www.biorxiv.org/content/10.1101/2024.03.17.585376v1.abstract,3.00,,,,,2.1000000000000016e+21,"1. Hardware: 24x V100 32GB GPUs (1.30e+14 FLOPs/s per GPU)
2. Training duration: 20 days (directly provided) = 1.728e+6 seconds
3. Utilization: 40%
4. Calculation: 
   24 GPUs × 1.30e+14 FLOPs/s × 1.728e+6 seconds × 0.4 utilization = 2.1e+21 FLOPs",,,20900000001,"Number of sequences (20.4M) × Maximum sequence length (1024) = 20,400,000 × 1,024 = 20,889,600,000 ≈ 2.09e10 datapoints",,,,,,,,"With large amounts of unlabeled RNA sequences data produced by high-throughput sequencing technologies, pre-trained RNA language models have been developed to estimate semantic space of RNA molecules, which facilities the understanding of grammar of RNA language. However, existing RNA language models overlook the impact of structure when modeling the RNA semantic space, resulting in incomplete feature extraction and suboptimal performance across various downstream tasks. In this study, we developed a RNA pre-trained language model named ERNIE-RNA (Enhanced Representations with base-pairing restriction for RNA modeling) based on a modified BERT (Bidirectional Encoder Representations from Transformers) by incorporating base-pairing restriction with no MSA (Multiple Sequence Alignment) information. We found that the attention maps from ERNIE-RNA with no fine-tuning are able to capture RNA structure in the zero-shot experiment more precisely than conventional methods such as fine-tuned RNAfold and RNAstructure, suggesting that the ERNIE-RNA can provide comprehensive RNA structural representations. Furthermore, ERNIE-RNA achieved SOTA (state-of-the-art) performance after fine-tuning for various downstream tasks, including RNA structural and functional predictions. In summary, our ERNIE-RNA model provides general features which can be widely and effectively applied in various subsequent research tasks. Our results indicate that introducing key knowledge-based prior information in the BERT framework may be a useful strategy to enhance the performance of other language models.",,,Unverified,"United States of America,United Kingdom of Great Britain and Northern Ireland,China,China",,,,,,"Industry,Industry,Academia",,,"Industry,Industry,Academia",,,,Hardware
RNA language models predict mutations that improve RNA function,Biology,Mutation prediction,"""NERSC, Lawrence Berkeley National Laboratory"",University of California San Francisco,University of California (UC) Berkeley","Yekaterina Shulgina, Marena I Trinidad, Conner J Langeberg, Hunter Nisonoff, Seyone Chithrananda, Petr Skopintsev, Amos J Nissley, Jaymin Patel, Ron S Boger, Honglue Shi, Peter H Yoon, Erin E Doherty, Tara Pande, Aditya M Iyer, Jennifer A Doudna, Jamie H D Cate",2024-09-16,RNA language models predict mutations that improve RNA function,https://pmc.ncbi.nlm.nih.gov/articles/PMC11014562/,3.00,,,,,,,,,363000001,"89,000,000 + 274,000,000 = 363,000,000 tokens

Dataset 1 (23S rRNA): 89M tokens
Dataset 2 (GARNET): 274M tokens
Total combined: 363M tokens (3.63e8)",,,,,,,,"Structured RNA lies at the heart of many central biological processes, from gene expression to catalysis. While advances in deep learning enable the prediction of accurate protein structural models, RNA structure prediction is not possible at present due to a lack of abundant high-quality reference data1. Furthermore, available sequence data are generally not associated with organismal phenotypes that could inform RNA function2–4. We created GARNET (Gtdb Acquired RNa with Environmental Temperatures), a new database for RNA structural and functional analysis anchored to the Genome Taxonomy Database (GTDB)5. GARNET links RNA sequences derived from GTDB genomes to experimental and predicted optimal growth temperatures of GTDB reference organisms. This enables construction of deep and diverse RNA sequence alignments to be used for machine learning. Using GARNET, we define the minimal requirements for a sequence- and structure-aware RNA generative model. We also develop a GPT-like language model for RNA in which overlapping triplet tokenization provides optimal encoding. Leveraging hyperthermophilic RNAs in GARNET and these RNA generative models, we identified mutations in ribosomal RNA that confer increased thermostability to the Escherichia coli ribosome. The GTDB-derived data and deep learning models presented here provide a foundation for understanding the connections between RNA sequence, structure, and function.",,,Unverified,"United States of America,United States of America,United States of America",,,,,,"Government,Academia,Academia",,,"Government,Academia,Academia",,,,
Uni-RNA-L8,Biology,Protein or nucleotide language model (pLM/nLM),DP Technology,"Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen",2023-07-12,UNI-RNA: UNIVERSAL PRE-TRAINED MODELS REVOLUTIONIZE RNA RESEARCH,https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1.abstract,20.00,,,25000000.00,Table 8: Model architecture parameters of different Uni-RNA models,,"""We trained our proposed network on 128 A100""",,,100000000,Table 8: Model architecture parameters of different Uni-RNA models,,,,,,,,"RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of life’s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",Unreleased,,Unverified,China,,,,,,Industry,,,Industry,,,,
RNAformer,Biology,RNA structure prediction,University of Freiburg,"Jörg K.H. Franke, Frederic Runge, Ryan Köksal,  Rolf Backofen, Frank Hutter",2024-12-01,RNAformer: A Simple Yet Effective Deep Learning Model for RNA Secondary Structure Prediction,https://www.biorxiv.org/content/10.1101/2024.02.12.579881v1.abstract,6.00,,,,,,,,,557219,"Total Data Points = 410,408 + 38,184 + 64,535 + 44,091 = 557,218

Components:
- Biophysical Model: 410,408
- bpRNA: 38,184
- Intra-family: 64,535
- Homology: 44,091

Final estimate: 557,218 samples",,,,,,,,"Traditional RNA secondary structure prediction methods, based on dynamic programming, often fall short in accuracy. Recent advances in deep learning have aimed to address this, but may not adequately learn the biophysical model of RNA folding. Many deep learning approaches are also too complex, incorporating multi-model systems, ensemble strategies, or requiring external data like multiple sequence alignments. In this study, we demonstrate that a single deep learning model, relying solely on RNA sequence input, can effectively learn a biophysical model and outperform existing deep learning methods in standard benchmarks, as well as achieve comparable results to methods that utilize multi-sequence alignments. We dub this model RNAformer and achieve these benefits by a two-dimensional latent space, axial attention, and recycling in the latent space. Further, we found that our model performance improves when we scale it up. We also demonstrate how to refine a pre-trained RNAformer with fine-tuning techniques, which are particularly efficient when applied to a limited amount of high-quality data. A further aspect of our work is addressing the challenges in dataset curation in deep learning, especially regarding data homology. We tackle this through an advanced data processing pipeline that allows for training and evaluation of our model across various levels of sequence similarity. Our models and datasets are openly accessible, offering a simplified yet effective tool for RNA secondary structure prediction.",,,Unverified,Germany,,,,,,Academia,,,Academia,,,,
RiboDiffusion,Biology,RNA design,"Beihang University,Nanjing University,Chinese University of Hong Kong (CUHK)","Han Huang, Ziqian Lin, Dongchen He, Liang Hong, Yu Li",2024-06-28,RiboDiffusion: tertiary structure-based RNA inverse folding with generative diffusion models,https://academic.oup.com/bioinformatics/article/40/Supplement_1/i347/7700903,3.00,,,,,,,,,3648001,"24,322 RNA chains × 150 nucleotides per chain = 3,648,300 data points ≈ 3.648 × 10⁶
(7,322 + 17,000 = 24,322 RNA chains)",,,,,,,,"Motivation
RNA design shows growing applications in synthetic biology and therapeutics, driven by the crucial role of RNA in various biological processes. A fundamental challenge is to find functional RNA sequences that satisfy given structural constraints, known as the inverse folding problem. Computational approaches have emerged to address this problem based on secondary structures. However, designing RNA sequences directly from 3D structures is still challenging, due to the scarcity of data, the nonunique structure-sequence mapping, and the flexibility of RNA conformation.

Results
In this study, we propose RiboDiffusion, a generative diffusion model for RNA inverse folding that can learn the conditional distribution of RNA sequences given 3D backbone structures. Our model consists of a graph neural network-based structure module and a Transformer-based sequence module, which iteratively transforms random sequences into desired sequences. By tuning the sampling weight, our model allows for a trade-off between sequence recovery and diversity to explore more candidates. We split test sets based on RNA clustering with different cut-offs for sequence or structure similarity. Our model outperforms baselines in sequence recovery, with an average relative improvement of 11% for sequence similarity splits and 16% for structure similarity splits. Moreover, RiboDiffusion performs consistently well across various RNA length categories and RNA types. We also apply in silico folding to validate whether the generated sequences can fold into the given 3D RNA backbones. Our method could be a powerful tool for RNA design that explores the vast sequence space and finds novel solutions to 3D structural constraints.",,,Unverified,"China,China,Hong Kong,China",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
RNADiffFold,Biology,RNA structure prediction,"Hangzhou Institute of Medicine,Zhejiang University,University of Chinese Academy of Sciences","Zhen Wang, Yizhen Feng, Qingwen Tian, Ziqi Liu, Pengju Yan, Xiaolin Li",2024-10-13,RNADiffFold: Generative RNA Secondary Structure Prediction using Discrete Diffusion Models,https://www.biorxiv.org/content/10.1101/2024.05.28.596177v2.abstract,0.00,,,,,10000000000000000000.00,"1. Hardware setup: 1x NVIDIA A40 PCIe GPU (1.50×10¹⁴ FLOP/s per GPU)

2. Training duration: Estimated 20 hours (72,000 seconds)
   Calculation: 100 epochs × (135,000 sequences/32 batch size) × 0.1s per step, rounded up

3. Utilization rate: 40% (0.4)

4. Final calculation:
   1.50×10¹⁴ FLOP/s × 1 GPU × 0.4 utilization × 72,000s = 4.32×10¹⁹ FLOPs
   (Rounded to 1.0×10¹⁹ FLOPs)",,,135001,"RNAStrAlign (30,451) + bpRNA TR0 (102,318) + Mutate-seq (2,717) = 135,486 unique sequences

30,451 + 102,318 + 2,717 = 135,486

Final result: 135,486 (1.35e5)",,,,,,,,"RNA molecules are essential macromolecules that perform diverse biological functions in living beings. Precise prediction
of RNA secondary structures is instrumental in deciphering their complex three-dimensional architecture and functionality.
Traditional methodologies for RNA structure prediction, including energy-based and learning-based approaches, often
depict RNA secondary structures from a static perspective and rely on stringent a priori constraints. Inspired by the
success of diffusion models, in this work, we introduce RNADiffFold, an innovative generative prediction approach
of RNA secondary structures based on multinomial diffusion. We reconceptualize the prediction of contact maps as
akin to pixel-wise segmentation and accordingly train a denoising model to refine the contact maps starting from a
noise-infused state progressively. We also devise a potent conditioning mechanism that harnesses features extracted from
RNA sequences to steer the model toward generating an accurate secondary structure. These features encompass one-hot
encoded sequences, probabilistic maps generated from a pre-trained scoring network, and embeddings and attention
maps derived from RNA-FM. Experimental results on both within- and cross-family datasets demonstrate RNADiffFold’s
competitive performance compared with current state-of-the-art methods. Additionally, RNADiffFold has shown a notable
proficiency in capturing the dynamic aspects of RNA structures, a claim corroborated by its performance on datasets
comprising multiple conformations.",,,Unverified,"China,China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
DiscDiff,Biology,Protein or nucleotide language model (pLM/nLM),Imperial College London,"Zehui Li, Yuhao Ni, William A V Beardall, Guoxuan Xia, Akashaditya Das, Guy-Bart Stan, Yiren Zhao",2024-02-08,DiscDiff: Latent Diffusion Model for DNA Sequence Generation,https://arxiv.org/abs/2402.06079,2.00,,,,,33999999999999880000.00,"1. Hardware setup:
- VAE stage: 1x NVIDIA RTX A6000 (3.87e13 FLOP/s)
- UNet stage: 1x NVIDIA A100 40GB (3.12e14 FLOP/s)

2. Training duration (provided directly):
- VAE: 24 GPU-hours (86,400 seconds)
- UNet: 72 GPU-hours (259,200 seconds)

3. Utilization rate: 40% for both stages

4. Calculation:
VAE: 3.87e13 FLOP/s × 86,400s × 0.4 = 1.34e18 FLOPs
UNet: 3.12e14 FLOP/s × 259,200s × 0.4 = 3.24e19 FLOPs
Total = 1.34e18 + 3.24e19 = 3.4e19 FLOPs",,,94000000001,"EPD-GenDNA (256 bases): 15,000,000 × 256 = 3.84 × 10^9
EPD-GenDNA (2048 bases): 15,000,000 × 2048 = 3.072 × 10^10
EPD-GenDNA Total: 3.84 × 10^9 + 3.072 × 10^10 = 3.456 × 10^10
EPDnew: 29,349,475 × 2048 = 5.9392 × 10^10
Final Total: 3.456 × 10^10 + 5.9392 × 10^10 = 9.3952 × 10^10",,,,,,,,"This paper introduces a novel framework for DNA sequence generation, comprising two key components: DiscDiff, a Latent Diffusion Model (LDM) tailored for generating discrete DNA sequences, and Absorb-Escape, a post-training algorithm designed to refine these sequences. Absorb-Escape enhances the realism of the generated sequences by correcting ‘round errors’ inherent in the conversion process between latent and input spaces. Our approach not only sets new standards in DNA sequence generation but also demonstrates superior performance over existing diffusion models, in generating both short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the first comprehensive, multi-species dataset for DNA generation, encompassing 160,000 unique sequences from 15 species. We hope this study will advance the generative modelling of DNA, with potential implications for gene therapy and protein production.",,,Unverified,United Kingdom of Great Britain and Northern Ireland,,,,,,Academia,,,Academia,,,,Hardware
DDPM,Biology,Gene expression profile generation,"University Paris-Saclay,Radboud University Medical Center","Alice Lacan, Romain André, Michele Sebag, Blaise Hanczar",2024-04-13,In Silico Generation of Gene Expression profiles using Diffusion Models,https://www.biorxiv.org/content/10.1101/2024.04.10.588825v1.abstract,0.00,,,,,900000000000000600.00,"1. Hardware setup: 1x NVIDIA A40 GPU (1.50×10¹⁴ FLOP/s per GPU)

2. Training duration: 15,000 seconds (directly provided - sum of TCGA training: 3,780s and GTEx training: 11,220s)

3. Utilization rate: 40%

4. Calculation: 1.50×10¹⁴ FLOP/s × 1 GPU × 15,000s × 0.4 = 9.0×10¹⁷ FLOPs",,,16000001,"TCGA: 6,499 × 978 = 6,356,022
GTEx: 9,796 × 974 = 9,541,304
Total: 6,356,022 + 9,541,304 = 15,897,326 ≈ 1.6e7 datapoints",,,,,,,,"Motivation RNA-seq data is used for precision medicine (e.g., cancer predictions), which benefits from deep learning approaches to analyze complex gene expression data. However, transcriptomics datasets often have few samples compared to deep learning standards. Synthetic data generation is thus being explored to address this data scarcity. So far, only deep generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) have been used for this aim. Considering the recent success of diffusion models (DM) in image generation, we propose the first generation pipeline that leverages the power of said diffusion models.

Results This paper presents two state-of-the-art diffusion models (DDPM and DDIM) and achieves their adaptation in the transcriptomics field. DM-generated data of L1000 landmark genes show better predictive performance over TCGA and GTEx datasets. We also compare linear and nonlinear reconstruction methods to recover the complete transcriptome. Results show that such reconstruction methods can boost the performances of diffusion models, as well as VAEs and GANs. Overall, the extensive comparison of various generative models using data quality indicators shows that diffusion models perform best and second-best, making them promising synthetic transcriptomics generators.",,,Unverified,"France,Netherlands",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
DDIM,Biology,Gene expression profile generation,"University Paris-Saclay,Radboud University Medical Center","Alice Lacan, Romain André, Michele Sebag, Blaise Hanczar",2024-04-13,In Silico Generation of Gene Expression profiles using Diffusion Models,https://www.biorxiv.org/content/10.1101/2024.04.10.588825v1.abstract,0.00,,,,,900000000000000600.00,"1. Hardware setup: 1x NVIDIA A40 GPU (1.50×10¹⁴ FLOP/s per GPU)

2. Training duration: 15,000 seconds (directly provided - sum of TCGA training: 3,780s and GTEx training: 11,220s)

3. Utilization rate: 40%

4. Calculation: 1.50×10¹⁴ FLOP/s × 1 GPU × 15,000s × 0.4 = 9.0×10¹⁷ FLOPs",,,16000001,"TCGA: 6,499 × 978 = 6,356,022
GTEx: 9,796 × 974 = 9,541,304
Total: 6,356,022 + 9,541,304 = 15,897,326 ≈ 1.6e7 datapoints",,,,,,,,"Motivation: RNA-seq data is used for precision medicine (e.g., cancer predictions), which benefits from deep learning approaches to analyze complex gene expression data. However, transcriptomics datasets often have few samples compared to deep learning standards. Synthetic data generation is thus being explored to address this data scarcity. So far, only deep generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) have been used for this aim. Considering the recent success of diffusion models (DM) in image generation, we propose the first generation pipeline that leverages the power of said diffusion models. Results: This paper presents two state-of-the-art diffusion models (DDPM and DDIM) and achieves their adaptation in the transcriptomics field. DM-generated data of L1000 landmark genes show better predictive performance over TCGA and GTEx datasets. We also compare linear and nonlinear reconstruction methods to recover the complete transcriptome. Results show that such reconstruction methods can boost the performances of diffusion models, as well as VAEs and GANs. Overall, the extensive comparison of various generative models using data quality indicators shows that diffusion models perform best and second-best, making them promising synthetic transcriptomics generators.",,,Unverified,"France,Netherlands",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
RNACG,Biology,Protein or nucleotide language model (pLM/nLM),Tsinghua University,"Letian Gao, Zhi John Lu",2024-07-29,RNACG: A Universal RNA Sequence Conditional Generation model based on Flow-Matching,https://arxiv.org/abs/2407.19838,0.00,,,,,,,,,,,,,,,,,,"RNA plays a crucial role in diverse life processes. In contrast to the rapid advancement of protein design methods, the work related to RNA is more demanding. Most current RNA design approaches concentrate on specified target attributes and rely on extensive experimental searches. However, these methods remain costly and inefficient due to practical limitations. In this paper, we characterize all sequence design issues as conditional generation tasks and offer parameterized representations for multiple problems. For these problems, we have developed a universal RNA sequence generation model based on flow matching, namely RNACG. RNACG can accommodate various conditional inputs and is portable, enabling users to customize the encoding network for conditional inputs as per their requirements and integrate it into the generation network. We evaluated RNACG in RNA 3D structure inverse folding, 2D structure inverse folding, family-specific sequence generation, and 5'UTR translation efficiency prediction. RNACG attains superior or competitive performance on these tasks compared with other methods. RNACG exhibits extensive applicability in sequence generation and property prediction tasks, providing a novel approach to RNA sequence design and potential methods for simulation experiments with large-scale RNA sequence data.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
PocketGen,Biology,Protein-ligand contact prediction,"University of Science and Technology of China,Hefei Comprehensive National Science Center,Harvard University,Broad Institute,Harvard Data Science Initiative","Zaixi Zhang, Wan Xiang Shen, Qi Liu, Marinka Zitnik",2024-09-23,Efficient Generation of Protein Pockets with PocketGen,https://www.biorxiv.org/content/10.1101/2024.02.25.581968v4.abstract,1.00,,,7900000.00,"""As a result, PocketGen requires significantly fewer trainable parameters than RFDiffusionAA [16] (7.9M versus 82.9M trainable parameters).""",21000000000000000000.00,"""It takes around 48 hours to finish training on 1 Tesla A100 GPU from scratch."" Assume 40% utilization, FP16 tensor precision. ",,,40000,"""Finally, we have 40k protein-ligand pairs for training, 100 pairs for validation, and 100 pairs for testing.""",,,,,,,,"Designing protein-binding proteins is critical for drug discovery. However, the AI-based design of such proteins is challenging due to the complexity of ligand-protein interactions, the flexibility of ligand molecules and amino acid side chains, and sequence-structure dependencies. We introduce PocketGen, a deep generative model that simultaneously produces both the residue sequence and atomic structure of the protein regions where ligand interactions occur. PocketGen ensures consistency between sequence and structure by using a graph transformer for structural encoding and a sequence refinement module based on a protein language model. The bilevel graph transformer captures interactions at multiple scales, including atom, residue, and ligand levels. To enhance sequence refinement, PocketGen integrates a structural adapter into the protein language model, ensuring that structure-based predictions align with sequence-based predictions. PocketGen can generate high-fidelity protein pockets with superior binding affinity and structural validity. It operates ten times faster than physics-based methods and achieves a 95% success rate, defined as the percentage of generated pockets with higher binding affinity than reference pockets. Additionally, it attains an amino acid recovery rate exceeding 64%.",,,Unverified,"China,China,United States of America,United States of America,United States of America",,,,,,"Academia,Research collective,Academia,Research collective,Academia",,,"Academia,Research collective,Academia,Research collective,Academia",,,,Hardware
FoldFlow,Biology,Protein generation,"McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Dreamfold,University of Montreal / Université de Montréal,University of Oxford","Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, Alexander Tong",2023-10-03,SE(3) Stochastic Flow Matching for Protein Backbone Generation,https://arxiv.org/abs/2310.02391,41.00,,,,,110000000000000080000.00,"1. Hardware: 4x NVIDIA A100-80GB GPUs (3.12e14 FLOP/s per GPU)
2. Training duration: 2.5 days = 216,000 seconds (directly provided)
3. Utilization rate: 40%
4. Calculation: 3.12e14 FLOP/s × 4 GPUs × 216,000s × 0.40 = 1.1e20 FLOP",,,16000001,"PDB Dataset: 22,248 proteins × 200 residues = 4,449,600 tokens
MD Dataset: 200,000 frames × 58 residues = 11,600,000 tokens
Total: 4,449,600 + 11,600,000 = 16,049,600 tokens (1.6 × 10⁷)",,,,,,,,"The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce FOLDFLOW a series of novel generative models of increasing modeling power based on the flow-matching paradigm over 3D rigid motions—i.e. the group SE(3)—enabling accurate modeling of protein backbones. We first introduce FOLDFLOW-BASE a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on SE(3). We next accelerate training by incorporating Riemannian optimal transport to create FOLDFLOW-OT leading to the construction of both more simple and stable flows. Finally, we design FOLDFLOW-SFM coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over SE(3). Our family of FOLDFLOW generative models offers several key advantages over previous approaches to the generative modeling of proteins: they are more stable and faster to train than diffusion-based approaches, and our models enjoy the ability to map any invariant source distribution to any invariant target distribution over SE(3). Empirically, we validate FOLDFLOW on protein backbone generation of up to 300 amino acids leading to high-quality designable, diverse, and novel samples.",,,Unverified,"Canada,Canada,Canada,Canada,United Kingdom of Great Britain and Northern Ireland",,,,,,"Academia,Academia,Industry,Academia,Academia",,,"Academia,Academia,Industry,Academia,Academia",,,,Hardware
GraphBP,Biology,Drug discovery,"Texas A&M,Fujitsu","Meng Liu, Youzhi Luo, Kanji Uchino, Koji Maruhashi, Shuiwang Ji",2022-04-19,Generating 3D Molecules for Target Protein Binding,https://arxiv.org/abs/2204.09410,109.00,,,,,,,,,15000001,"500,000 complexes × 30 atoms/complex = 15,000,000 total data points (1.5e7)",,,,,,,,,,,Unverified,"United States of America,Japan",,,,,,"Academia,Industry",,,"Academia,Industry",,,,
ProteinReDiff,Biology,Protein design,"FPT Software AI Center,University of Chicago,Indiana State University","Viet Thanh Duy Nguyen, Nhan D. Nguyen,  Truong Son Hy",2024-06-12,Complex-based Ligand-Binding Proteins Redesign by Equivariant Diffusion-based Generative Models,https://www.biorxiv.org/content/10.1101/2024.04.17.589997v3.abstract,1.00,,,,,,,,,8640001,"Total Samples: 9,430 (PDBBind) + 15,261 (CATH) = 24,691 samples
Tokens per Sample: 300 (protein) + 50 (SMILES) = 350 tokens
Total Data Points: 24,691 × 350 = 8,641,850",,,,,,,,"Proteins, serving as the fundamental architects of biological processes, interact with ligands to perform a myriad of functions essential for life. Designing functional ligand-binding proteins is pivotal for advancing drug development and enhancing therapeutic efficacy. In this study, we introduce ProteinReDiff, an efficient computational framework targeting the redesign of ligand-binding proteins. Using equivariant diffusion-based generative models, ProteinReDiff enables the creation of high-affinity ligand-binding proteins without the need for detailed structural information, leveraging instead the potential of initial protein sequences and ligand SMILES strings. Our evaluations across sequence diversity, structural preservation, and ligand binding affinity underscore ProteinReDiff’s potential to advance computational drug discovery and protein engineering. Our source code is publicly available at https: //github.com/HySonLab/Protein_Redesign.",,,Unverified,"Vietnam,United States of America,United States of America",,,,,,"Industry,Academia",,,"Industry,Academia",,,,
Alphaflow,Biology,Protein folding prediction,Massachusetts Institute of Technology (MIT),"Bowen Jing, Bonnie Berger, Tommi Jaakkola",2024-09-02,AlphaFold Meets Flow Matching for Generating Protein Ensembles,https://arxiv.org/abs/2402.04845,43.00,,,,,1.3400000000000016e+21,"1. Hardware setup: 8x NVIDIA A100 GPUs, 3.12e+14 FLOP/s per GPU
2. Training duration: 371 hours (267h AlphaFLOW + 104h ESMFLOW) = 1,335,600 seconds
3. Utilization rate: 40%
4. Final calculation: 8 GPUs × 3.12e+14 FLOP/s × 1,335,600s × 0.4 = 1.34e+21 FLOPs",,,530000001,"AlphaFLOW:
1.28M × 256 = 327,680,000
43K × 256 = 11,008,000
Total: 327,680,000 + 11,008,000 = 338,688,000

ESMFLOW:
720K × 256 = 184,320,000
27K × 256 = 6,912,000
Total: 184,320,000 + 6,912,000 = 191,232,000

Combined Total:
338,688,000 + 191,232,000 = 529,920,000 (5.3e8)",,,,,,,,,,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,Hardware
ESMFlow,Biology,Protein folding prediction,Massachusetts Institute of Technology (MIT),"Bowen Jing, Bonnie Berger, Tommi Jaakkola",2024-09-02,AlphaFold Meets Flow Matching for Generating Protein Ensembles,https://arxiv.org/abs/2402.04845,43.00,,,,,1.3400000000000016e+21,"1. Hardware setup: 8x NVIDIA A100 GPUs, 3.12e+14 FLOP/s per GPU
2. Training duration: 371 hours (267h AlphaFLOW + 104h ESMFLOW) = 1,335,600 seconds
3. Utilization rate: 40%
4. Final calculation: 8 GPUs × 3.12e+14 FLOP/s × 1,335,600s × 0.4 = 1.34e+21 FLOPs",,,530000001,"AlphaFLOW:
1.28M × 256 = 327,680,000
43K × 256 = 11,008,000
Total: 327,680,000 + 11,008,000 = 338,688,000

ESMFLOW:
720K × 256 = 184,320,000
27K × 256 = 6,912,000
Total: 184,320,000 + 6,912,000 = 191,232,000

Combined Total:
338,688,000 + 191,232,000 = 529,920,000 (5.3e8)",,,,,,,,"The biological functions of proteins often depend on dynamic structural ensembles. In this work, we develop a flow-based generative modeling approach for learning and sampling the conformational landscapes of proteins. We repurpose highly accurate single-state predictors such as AlphaFold and ESMFold and fine-tune them under a custom flow matching framework to obtain sequence-conditoned generative models of protein structure called AlphaFlow and ESMFlow. When trained and evaluated on the PDB, our method provides a superior combination of precision and diversity compared to AlphaFold with MSA subsampling. When further trained on ensembles from all-atom MD, our method accurately captures conformational flexibility, positional distributions, and higher-order ensemble observables for unseen proteins. Moreover, our method can diversify a static PDB structure with faster wall-clock convergence to certain equilibrium properties than replicate MD trajectories, demonstrating its potential as a proxy for expensive physics-based simulations. Code is available at this https URL.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,Reported
AutoDiff,Biology,Drug discovery,"Galixir Technologies,Rensselaer Polytechnic Institute,Massachusetts Institute of Technology (MIT)","Xinze Li, Penglei Wang, Tianfan Fu, Wenhao Gao, Chengtao Li, Leilei Shi, Junhong Liu",2024-04-02,AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug Design,https://arxiv.org/abs/2404.02003,1.00,,,,,,,,,22500001,"22.5M poses = 22.5M datapoints
= 22,500,000 datapoints

Final estimate = 22.5M",,,,,,,,"Structure-based drug design (SBDD), which aims to generate molecules that can bind tightly to the target protein, is an essential problem in drug discovery, and previous approaches have achieved initial success. However, most existing methods still suffer from invalid local structure or unrealistic conformation issues, which are mainly due to the poor leaning of bond angles or torsional angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based fragment-wise autoregressive generation model. Specifically, we design a novel molecule assembly strategy named conformal motif that preserves the conformation of local structures of molecules first, then we encode the interaction of the protein-ligand complex with an SE(3)-equivariant convolutional network and generate molecules motif-by-motif with diffusion modeling. In addition, we also improve the evaluation framework of SBDD by constraining the molecular weights of the generated molecules in the same range, together with some new metrics, which make the evaluation more fair and practical. Extensive experiments on CrossDocked2020 demonstrate that our approach outperforms the existing models in generating realistic molecules with valid structures and conformations while maintaining high binding affinity.",,,Unverified,"China,United States of America,United States of America",,,,,,"Industry,Academia,Academia",,,"Industry,Academia,Academia",,,,
BindDM,Biology,Drug discovery,"Peng Cheng Laboratory,Peking University,University of Science and Technology of China,ByteDance,Tsinghua University","Zhilin Huang, Ling Yang, Zaixi Zhang, Xiangxin Zhou, Yu Bao, Xiawu Zheng, Yuwei Yang, Yu Wang, Wenming Yang",2024-03-24,Binding-Adaptive Diffusion Models for Structure-Based Drug Design,https://ojs.aaai.org/index.php/AAAI/article/view/29162,4.00,,,,,,,,,100001,"100,000 protein-ligand pairs

Total datapoints = 100,000
Final result = 1.0e5",,,,,,,,"Structure-based drug design (SBDD) aims to generate 3D ligand molecules that bind to specific protein targets. Existing 3D deep generative models including diffusion models have shown great promise for SBDD. However, it is complex to capture the essential protein-ligand interactions exactly in 3D space for molecular generation. To address this problem, we propose a novel framework, namely Binding-Adaptive Diffusion Models (BindDM). In BindDM, we adaptively extract subcomplex, the essential part of binding sites responsible for protein-ligand interactions. Then the selected protein-ligand subcomplex is processed with SE(3)-equivariant neural networks, and transmitted back to each atom of the complex for augmenting the target-aware 3D molecule diffusion generation with binding interaction information. We iterate this hierarchical complex-subcomplex process with cross-hierarchy interaction node for adequately fusing global binding context between the complex and its corresponding subcomplex. Empirical studies on the CrossDocked2020 dataset show BindDM can generate molecules with more realistic 3D structures and higher binding affinities towards the protein targets, with up to -5.92 Avg. Vina Score, while maintaining proper molecular properties. Our code is available at https://github.com/YangLing0818/BindDM",,,Unverified,"China,China,China,China,China",,,,,,"Academia,Academia,Academia,Industry,Academia",,,"Academia,Academia,Academia,Industry,Academia",,,,
BADGER,Biology,Drug discovery,"NVIDIA,University of California (UC) Berkeley","Yue Jian, Curtis Wu, Danny Reidenbach, Aditi S. Krishnapriyan",2024-06-24,General Binding Affinity Guidance for Diffusion Models in Structure-Based Drug Design,https://arxiv.org/abs/2406.16821,1.00,,,,,,,,,100001,"100,000 (training) + 100 (testing) = 100,100 unique datapoints
20 epochs mentioned but only unique datapoints counted
Final estimate: 1.0e5",,,,,,,,"Structure-Based Drug Design (SBDD) focuses on generating valid ligands that strongly and specifically bind to a designated protein pocket. Several methods use machine learning for SBDD to generate these ligands in 3D space, conditioned on the structure of a desired protein pocket. Recently, diffusion models have shown success here by modeling the underlying distributions of atomic positions and types. While these methods are effective in considering the structural details of the protein pocket, they often fail to explicitly consider the binding affinity. Binding affinity characterizes how tightly the ligand binds to the protein pocket, and is measured by the change in free energy associated with the binding process. It is one of the most crucial metrics for benchmarking the effectiveness of the interaction between a ligand and protein pocket. To address this, we propose BADGER: Binding Affinity Diffusion Guidance with Enhanced Refinement. BADGER is a general guidance method to steer the diffusion sampling process towards improved protein-ligand binding, allowing us to adjust the distribution of the binding affinity between ligands and proteins. Our method is enabled by using a neural network (NN) to model the energy function, which is commonly approximated by AutoDock Vina (ADV). ADV’s energy function is non-differentiable, and estimates the affinity based on the interactions between a ligand and target protein receptor. By using a NN as a differentiable energy function proxy, we utilize the gradient of our learned energy function as a guidance method on top of any trained diffusion model. We show that our method improves the binding affinity of generated ligands to their protein receptors by up to 60%, significantly surpassing previous machine learning methods. We also show that our guidance method is flexible and can be easily applied to other diffusion-based SBDD frameworks.",,,Unverified,"United States of America,United States of America",,,,,,"Industry,Academia",,,"Industry,Academia",,,,
DecompDiff,Biology,Drug discovery,"University of Illinois Urbana-Champaign (UIUC),ByteDance,University of Chinese Academy of Sciences,Chinese Academy of Sciences,Tsinghua University","Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma, Qiang Liu, Liang Wang, Quanquan Gu",2024-02-26,DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design,https://arxiv.org/abs/2403.07902,41.00,,,,"The key/value/query embedding is obtained through a 2-layer MLP with LayerNorm and ReLU activation. Stacking these three layers as a block, our model consists of 6 blocks with hidden dim=128 and n heads=16.",18999999999999996000.00,"1. Hardware setup: 1x NVIDIA A100 GPU (3.12e14 FLOPs/s for FP16)
2. Training duration: 41.7 hours (provided directly) = 150,120 seconds
3. Utilization rate: 40% (assumed)
4. Calculation: 3.12e14 FLOPs/s × 1 GPU × 150,120s × 0.4 = 1.873e19 FLOPs",,,100001,"300,000 steps * 4 batch size = 1,200,000 total samples
1,200,000 total samples / 100,000 complexes = 12 epochs
Final count = 100,000 unique data points (first epoch only)",,,,,,,,"Designing 3D ligands within a target binding site is a fundamental task in drug discovery. Existing structured-based drug design methods treat all ligand atoms equally, which ignores different roles of atoms in the ligand for drug design and can be less efficient for exploring the large drug-like molecule space. In this paper, inspired by the convention in pharmaceutical practice, we decompose the ligand molecule into two parts, namely arms and scaffold, and propose a new diffusion model, DECOMPDIFF, with decomposed priors over arms and scaffold. In order to facilitate the decomposed generation and improve the properties of the generated molecules, we incorporate both bond diffusion in the model and additional validity guidance in the sampling phase. Extensive experiments on CrossDocked2020 show that our approach achieves state-of-the-art performance in generating high-affinity molecules while maintaining proper molecular properties and conformational stability, with up to −8.39 Avg. Vina Dock score and 24.5% Success Rate. The code is provided at https://github. com/bytedance/DecompDiff",,,Unverified,"United States of America,China,China,China,China",,,,,,"Academia,Industry,Academia,Academia,Academia",,,"Academia,Industry,Academia,Academia,Academia",,,,Hardware
Molecular Diffusion Models with Virtual Receptors,Biology,Drug discovery,Verily Research,"Matan Halfon, Eyal Rozenberg, Ehud Rivlin, Daniel Freedman",2024-06-26,Molecular Diffusion Models with Virtual Receptors,https://arxiv.org/abs/2406.18330,0.00,,,,,,,,,22500001,"Number of unique datapoints: 22.5 million ligand poses

Calculation: 22.5 million = 2.25 × 10^7 datapoints

Final result: 2.25e7",,,,,,,,"Machine learning approaches to Structure-Based Drug Design (SBDD) have proven quite fertile over the last few years. In particular, diffusion-based approaches to SBDD have shown great promise. We present a technique which expands on this diffusion approach in two crucial ways. First, we address the size disparity between the drug molecule and the target/receptor, which makes learning more challenging and inference slower. We do so through the notion of a Virtual Receptor, which is a compressed version of the receptor; it is learned so as to preserve key aspects of the structural information of the original receptor, while respecting the relevant group equivariance. Second, we incorporate a protein language embedding used originally in the context of protein folding. We experimentally demonstrate the contributions of both the virtual receptors and the protein embeddings: in practice, they lead to both better performance, as well as significantly faster computations.",,,Unverified,United States of America,,,,,,Industry,,,Industry,,,,
ShapeMol,Biology,Drug discovery,Ohio State University,"Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, Xia Ning
",2023-08-23,Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models,https://arxiv.org/abs/2308.11890,6.00,,,,,25999999999999800000.00,"1. Hardware: 1x Tesla V100 PCIe 32GB (1.30×10¹⁴ FLOP/s)
2. Training duration: directly provided - 140 hours total (80h Shape Encoder + 60h Diffusion Model) = 504,000 seconds
3. Utilization: 40%
4. Calculation: 1.30×10¹⁴ FLOP/s × 1 GPU × 504,000s × 0.4 = 2.60×10¹⁹ FLOPs",,,1600001,"Step-by-step summary:
1. Training dataset: 1,593,653 molecules
2. Training steps calculation: 900,000 steps × 32 batch size = 28,800,000 molecule instances
3. Epochs calculation: 28,800,000 / 1,593,653 ≈ 18.1 epochs
4. Final data points (unique molecules from first epoch): 1,593,653

Final estimate: 1.6e6 data points",,,,,,,,"Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.",,,Unverified,United States of America,,,,,,,,,,,,,Hardware
ProSST,Biology,Protein or nucleotide language model (pLM/nLM),"Shanghai Jiao Tong University,Shanghai AI Lab,East China University of Science and Technology","Mingchen Li, Pan Tan, Xinzhu Ma, Bozitao Zhong, Huiqun Yu, Ziyi Zhou, Wanli Ouyang, Bingxin Zhou, Liang Hong, Yang Tan",2024-05-17,ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention,https://www.biorxiv.org/content/10.1101/2024.04.15.589672v3.abstract,9.00,,,110000000.00,Explicitly denoted in Table 2,2.6e+21,"""All ProSST models is trained on a DGX-A800 GPU (8×80G) server in BF16 precision for about a month."" I assume 8 accelerators each at 311.84 TFLOPS (A800 is absent from compute calculator) at 40% utilization. ",,,5600000001,18.8M structures × 300 residues per structure = 5.64B data points ≈ 5.6B data points,,720.0,"""All ProSST models is trained [...] for about a month.""",NVIDIA A800,8,,,"Protein language models (PLMs) have shown remarkable capabilities in various protein function prediction tasks. However, while protein function is intricately tied to structure, most existing PLMs do not incorporate protein structure information. To address this issue, we introduce ProSST, a Transformer-based protein language model that seamlessly integrates both protein sequences and structures. ProSST incorporates a structure quantization module and a Transformer architecture with disentangled attention. The structure quantization module translates a 3D protein structure into a sequence of discrete tokens by first serializing the protein structure into residue-level local structures and then embeds them into dense vector space. These vectors are then quantized into discrete structure tokens by a pre-trained clustering model. These tokens serve as an effective protein structure representation. Furthermore, ProSST explicitly learns the relationship between protein residue token sequences and structure token sequences through the sequence-structure disentangled attention. We pre-train ProSST on millions of protein structures using a masked language model objective, enabling it to learn comprehensive contextual representations of proteins. To evaluate the proposed ProSST, we conduct extensive experiments on the zero-shot mutation effect prediction and several supervised downstream tasks, where ProSST achieves the state-of-the-art performance among all baselines. Our code and pretrained models are publicly available 2.",,,Unverified,"China,China,China",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,Hardware
SS-pLM,Biology,Protein or nucleotide language model (pLM/nLM),"Nostrum Biodiscovery,Barcelona Supercomputing Center,Institucio Catalana de Recerca i Estudis Avancçats","Yaiza Serrano, Sergi Roda, Victor Guallar, Alexis Molina",2023-08-06,Efficient and accurate sequence generation with small-scale protein language models,https://www.biorxiv.org/content/10.1101/2023.08.04.551626.abstract,3.00,,,14800000.00,From Table 3,20700000000000025000.00,"1. Hardware setup: 4x NVIDIA A30 GPUs (1.50×10¹⁴ FLOP/s per GPU in FP16)
2. Training duration: 1 day (86,400 seconds) - directly provided
3. Utilization rate: 40%
4. Calculation: 4 GPUs × 1.50×10¹⁴ FLOP/s × 86,400s × 0.40 = 2.07×10¹⁹ FLOPs",,,525000001,Total Datapoints = 1.75 × 10⁶ sequences × 300 tokens/sequence = 5.25 × 10⁸ tokens,,,,,,,,"Large Language Models (LLMs) have demonstrated exceptional capabilities in understanding contextual relationships, outperforming traditional methodologies in downstream tasks such as text generation and sentence classification. This success has been mirrored in the realm of protein language models (pLMs), where proteins are encoded as text via their amino acid sequences. However, the training of pLMs, which involves tens to hundreds of millions of sequences and hundreds of millions to billions of parameters, poses a significant computational challenge.

In this study, we introduce a Small-Scale Protein Language Model (SS-pLM), a more accessible approach that requires training on merely millions of representative sequences, reducing the number of trainable parameters to 14.8M. This model significantly reduces the computational load, thereby democratizing the use of foundational models in protein studies. We demonstrate that the performance of our model, when fine-tuned to a specific set of sequences for generation, is comparable to that of larger, more computationally demanding pLM.",,,Unverified,"Spain,Spain,Spain",,,,,,"Government,Research collective",,,"Government,Research collective",,,,Hardware
PLLaMa,Biology,Language modeling/generation,"University of California Santa Barbara (UCSB),University of Lincoln,Chinese Academy of Agricultural Sciences,Swedish University of Agricultural Sciences","Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson",2024-01-03,PLLaMa: An Open-source Large Language Model for Plant Science,https://arxiv.org/abs/2401.01600,20.00,,,,,300000000000000100000.00,"1. Hardware setup:
- Pretraining: 8x NVIDIA A100 80GB GPUs (3.12E14 FLOP/s per GPU)
- Instruction Tuning: 4x NVIDIA A100 80GB GPUs (3.12E14 FLOP/s per GPU)

2. Training duration (provided directly):
- Pretraining: 7B model (26h), 13B model (57h)
- Instruction Tuning: 7B model (1.3h), 13B model (2.7h)

3. Utilization rate: 40%

4. Final calculation:
Pretraining:
- 7B: 3.12E14 × 8 GPUs × 26h × 3600s × 0.4 = 9.36E19
- 13B: 3.12E14 × 8 GPUs × 57h × 3600s × 0.4 = 2.05E20
Instruction Tuning:
- 7B: 3.12E14 × 4 GPUs × 1.3h × 3600s × 0.4 = 2.34E18
- 13B: 3.12E14 × 4 GPUs × 2.7h × 3600s × 0.4 = 4.85E18
Total: 3.0E20 FLOPs",,,11660000001,"Number of text pieces: 2,278,433
Tokens per text piece: 5,120

Calculations:
2,278,433 × 5,000 = 11,392,165,000
2,278,433 × 120 = 273,411,960
11,392,165,000 + 273,411,960 = 11,665,576,960

Final result: 11,665,576,960 tokens (1.166 × 10¹⁰)",,,,,,,,"Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high accuracy, such as plant science, due to a lack of specific expertise in these fields.
This paper introduces PLLaMa, an open-source language model that evolved
from LLaMa-2. It’s enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in
plant and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves
its understanding of plant science-related topics. Moreover, we have formed
an international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying
the accuracy of PLLaMa’s responses to various academic inquiries, ensuring
its effective and reliable application in the field. To support further research
and development, we have made the model’s checkpoints and source codes
accessible to the scientific community. These resources are available for
download at https://github.com/Xianjun-Yang/PLLaMa.",,,Unverified,"United States of America,United Kingdom of Great Britain and Northern Ireland,China,Sweden",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,Hardware
METL,Biology,"Protein or nucleotide language model (pLM/nLM),Protein design","University of Wisconsin Madison,Morgridge Institute for Research","Sam Gelman, Bryce Johnson, Chase Freschlin, Sameer D’Costa, Anthony Gitter, Philip A. Romero",2024-04-17,Biophysics-based protein language models for protein engineering,https://www.biorxiv.org/content/10.1101/2024.03.15.585128v1.abstract,3.00,,,,,,,,,10000000001,"50M variants (20M + 30M) × 200 residues = 10 billion (1.0e10) tokens

20M from METL-Local
30M from METL-Global (148 proteins × 200k variants)
Average sequence length: 200 residues

Final estimate: 1.0e10 tokens",,,,,,,,"Protein language models trained on evolutionary data have emerged as powerful tools for predictive problems involving protein sequence, structure, and function. However, these models overlook decades of research into biophysical factors governing protein function. We propose Mutational Effect Transfer Learning (METL), a protein language model framework that unites advanced machine learning and biophysical modeling. Using the METL framework, we pretrain transformer-based neural networks on biophysical simulation data to capture fundamental relationships between protein sequence, structure, and energetics. We finetune METL on experimental sequence-function data to harness these biophysical signals and apply them when predicting protein properties like thermostability, catalytic activity, and fluorescence. METL excels in challenging protein engineering tasks like generalizing from small training sets and position extrapolation, although existing methods that train on evolutionary signals remain powerful for many types of experimental assays. We demonstrate METL’s ability to design functional green fluorescent protein variants when trained on only 64 examples, showcasing the potential of biophysics-based protein language models for protein engineering.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
Prot2Token,Biology,Protein or nucleotide language model (pLM/nLM),"University of Missouri,Politecnico di Milano","Mahdi Pourmirzaei, Farzaneh Esmaili, Mohammadreza Pourmirzaei, Duolin Wang, Dong Xu",2024-06-03,Prot2Token: A multi-task framework for protein language processing using autoregressive language modeling,https://www.biorxiv.org/content/10.1101/2024.05.31.596915v1.abstract,1.00,,,,,,,,,310000001,"1,024,055 = 8,678 + 53,571 + 21,446 + 15,550 + 29,898 + 12,312 + 29,215 + 23,604 + 35,669 + 8,716 + 300,700 + 6,391 + 16,436 + 22,841 + 10,400 + 428,628

307,216,500 = 1,024,055 × 300

Final estimate: 3.1 × 10⁸",,,,,,,,"This paper proposes a versatile tokenization method and introduces Prot2Token, a model that combines autoregressive language modeling with protein language models (PLMs) to tackle various protein prediction tasks using protein sequences. Leveraging our tokenization method, Prot2Token adapts existing PLMs for multiple tasks such as protein-level prediction, residue-level prediction, and protein-protein interaction prediction through next-token prediction of tokenized target label sequences. By incorporating prompt tokens into the decoder, Prot2Token enables multi-task training in a single end-to-end session. Our results demonstrate that Prot2Token not only matches the performance of specialized models across various tasks but also paves the way for integrating protein tasks with large language models (LLMs), representing an important step towards creating general-purpose PLMs for advanced protein language processing (PLP). Additionally, we use Prot2Token to develop S-ESM, a structure-aware version of the ESM model, which achieves competitive performance with state-of-the-art methods in 3D structure-related tasks using only protein sequences. Code is available at: https://github.com/mahdip72/prot2token.",,,Unverified,"United States of America,Italy",,,,,,Academia,,,Academia,,,,
ESM-NBR,Biology,Protein nucleotide interaction prediction,Hunan University,"Wenwu Zeng, Dafeng Lv, Xuan Liu, Guo Chen, Wenjuan Liu, Shaoliang Peng",2024-01-18,ESM-NBR: fast and accurate nucleic acid-binding residue prediction via protein language model feature representation and multi-task learning,https://ieeexplore.ieee.org/abstract/document/10385509?casa_token=3fdFlCTqsngAAAAA:e2xRjxP3NP4t39elNf9BYR-zw6AWHayrqqFsavdQYXyyVGp7Jpyu62satcyzrum7NQ-MSFK7Ug,4.00,,,,,,,,,13000000001,"Summary for ESM-NBR data estimate:

Pre-training (UniRef50):
43,000,000 proteins × 300 residues = 12,900,000,000 tokens (1.29e10)

Training (YK17-Tr + DRNATr-1068):
2,068 proteins × 300 residues = 620,400 tokens (6.2e5)

Total: 12,900,000,000 + 620,400 ≈ 1.29e10 tokens

Final estimate: 1.3e10 data points",,,,,,,,"Protein-nucleic acid interactions play a very important role in a variety of biological activities. Accurate identification of nucleic acid-binding residues is a critical step in understanding the interaction mechanisms. Although many computationally based methods have been developed to predict nucleic acid-binding residues, challenges remain. In this study, a fast and accurate sequence-based method, called ESM-NBR, is proposed. In ESM-NBR, we first use the large protein language model ESM2 to extract discriminative biological properties feature representation from protein primary sequences; then, a multi-task deep learning model composed of stacked bidirectional long short-term memory (BiLSTM) and multi-layer perceptron (MLP) networks is employed to explore common and private information of DNA- and RNA-binding residues with ESM2 feature as input. Experimental results on benchmark data sets demonstrate that the prediction performance of ESM2 feature representation comprehensively outperforms evolutionary information-based hidden Markov model (HMM) features. Meanwhile, the ESM-NBR obtains the MCC values for DNA-binding residues prediction of 0.427 and 0.391 on two independent test sets, which are 18.61 and 10.45% higher than those of the second-best methods, respectively. Moreover, by completely discarding the time-cost multiple sequence alignment process, the prediction speed of ESM-NBR far exceeds that of existing methods (5.52s for a protein sequence of length 500, which is about 16 times faster than the second-fastest method). A user-friendly standalone package and the data of ESM-NBR are freely available for academic use at: https://github.com/wwzll123/ESM-NBR.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
scFormer,Biology,Representation learning,"University of Toronto,Vector Institute,University Health Network,Microsoft Research","Haotian Cui, Chloe Wang, Hassaan Maan, Nan Duan, Bo Wang",2022-11-22,scFormer: A Universal Representation Learning Approach for Single-Cell Data Using Transformers,https://www.biorxiv.org/content/10.1101/2022.11.20.517285v1.abstract,3.00,,,,,,,,,1040000001,"Datapoint calculation breakdown:

Cortex: 3,005 × 19,972 = 60,099,860
PBMC 8K: 7,982 × 3,346 = 26,707,772
Spleen 17K: 17,001 × 13,553 = 230,414,553
Immune Human: 33,506 × 12,303 = 412,878,918
Pancreas: 16,382 × 19,093 = 312,934,046

Total: 60,099,860 + 26,707,772 + 230,414,553 + 412,878,918 + 312,934,046 = 1.04 × 10⁹",30.00,,,,,,,,,,Unverified,"Canada,Canada,Canada,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,"Academia,Academia,Industry",,,"Academia,Academia,Industry",,,,
HelixFold3,Biology,Protein folding prediction,"Tecorigin LTD,Tsinghua University","Lihang Liu, Shanzhuo Zhang, Yang Xue, Xianbin Ye, Kunrui Zhu, Yuxin Li, Yang Liu, Wenlai Zhao, Hongkun Yu, Zhihua Wu, Xiaonan Zhang, Xiaomin Fang",2024-08-30,Technical Report of HelixFold3 for Biomolecular Structure Prediction,https://arxiv.org/abs/2408.16975,,,,,,,,,,,,,,,,,,,"The AlphaFold series has transformed protein structure prediction with remarkable accuracy, often matching experimental methods. AlphaFold2, AlphaFold-Multimer, and the latest AlphaFold3 represent significant strides in predicting single protein chains, protein complexes, and biomolecular structures. While AlphaFold2 and AlphaFold-Multimer are open-sourced, facilitating rapid and reliable predictions, AlphaFold3 remains partially accessible through a limited online server and has not been open-sourced, restricting further development. To address these challenges, the PaddleHelix team is developing HelixFold3, aiming to replicate AlphaFold3's capabilities. Using insights from previous models and extensive datasets, HelixFold3 achieves an accuracy comparable to AlphaFold3 in predicting the structures of conventional ligands, nucleic acids, and proteins. The initial release of HelixFold3 is available as open source on GitHub for academic research, promising to advance biomolecular research and accelerate discoveries. We also provide online service at PaddleHelix website at this https URL.",,,Unverified,"China,China",,,,,,"Industry,Academia",,,"Industry,Academia",,,,
LBSTER,Biology,Protein or nucleotide language model (pLM/nLM),"Prescient Design,Genentech","Nathan C. Frey, Taylor Joren, Aya Abdelsalam Ismail, Allen Goodman, Richard Bonneau, Kyunghyun Cho, Vladimir Gligorijević",2024-05-15,Cramming Protein Language Model Training in 24 GPU Hours,https://www.biorxiv.org/content/10.1101/2024.05.14.594108v1.abstract,1.00,,,67000000.00,"""we are able to train a 67 million parameter model""",2000000000000000000.00,"1 day on 1 A100, assumed FP16 precision due to paper mentioning ""Using 8-bit floating point mixed precision training and other recent advances in efficient
transformer training are also promising avenues for future work."" Assuming 30% utilization rate.",UniRef50,"While listing rules of general challenge the paper is trying to complete, they list ""The training, validation, and test data splits are from UniRef50 [...]""",13000000001,"UniRef50 dataset size calculation:
43,000,000 sequences × 300 tokens/sequence = 1.29 × 10¹⁰ tokens

Training process calculation: 
1,048,576 tokens/step × 50,000 steps = 5.24288 × 10¹⁰ total tokens

Epochs calculation:
5.24288 × 10¹⁰ / 1.29 × 10¹⁰ = 4.06 epochs

Final unique tokens (first epoch only): 1.3 × 10¹⁰ tokens",,24.0,"""we are able to train a 67 million parameter model in a single day",NVIDIA A100 SXM4 80 GB,1,,,"Protein language models (pLMs) are ubiquitous across biological machine learning research, but state-of-the-art models like ESM2 take hundreds of thousands of GPU hours to pre-train on the vast protein universe. Resource requirements for scaling up pLMs prevent fundamental investigations into how optimal modeling choices might differ from those used in natural language. Here, we define a “cramming” challenge for pLMs and train performant models in 24 hours on a single GPU. By re-examining many aspects of pLM training, we are able to train a 67 million parameter model in a single day that achieves comparable performance on downstream protein fitness landscape inference tasks to ESM-3B, a model trained for over 15, 000× more GPU hours than ours. We open source our library1 for training and inference, LBSTER: Language models for Biological Sequence Transformation and Evolutionary Representation.",,,Unverified,"United States of America,United States of America",,,,,,"Industry,Industry",,,"Industry,Industry",,,882.2701758545828,Hardware
OPUS-Design,Biology,Protein design,"Fudan University,Shanghai AI Lab,Harcam Biomedicines","Gang Xu, Yulu Yang, Yiqiu Zhang, Qinghua Wang, Jianpeng Ma",2024-08-29,OPUS-Design: Designing Protein Sequence from Backbone Structure with 3DCNN and Protein Language Model,https://www.biorxiv.org/content/10.1101/2024.08.20.608889v2.abstract,0.00,,,,,,,,,,,,,,,,,,"Protein sequence design, also known as protein inverse folding, is a crucial task in protein engineering and design. Despite the recent advancements in this field, which have facilitated the identification of amino acid sequences based on backbone structures, achieving higher levels of accuracy in sequence recovery rates remains challenging. It this study, we introduce a two-stage protein sequence design method named OPUS-Design. Our evaluation on recently released targets from CAMEO and CASP15 shows that OPUS-Design significantly surpasses several other leading methods on both monomer and oligomer targets in terms of sequence recovery rate. Furthermore, by utilizing its finetune version OPUS-Design-ft and our previous work OPUS-Mut, we have successfully designed a thermal-tolerant double-point mutant of T4 lysozyme that demonstrates a residual enzyme activity exceeding that of the wild-type T4 by more than twofold when both are subjected to extreme heat treatment at 70°C. Importantly, this accomplishment is achieved through the experimental verification of less than 10 mutant candidates, thus significantly alleviating the burden of experimental verification process.",,,Unverified,"China,China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
Mdgen,"Biology,Materials science",Molecular simulation,Massachusetts Institute of Technology (MIT),"Bowen Jing, Hannes Stärk, Tommi Jaakkola, Bonnie Berger",2024-09-26,Generative Modeling of Molecular Dynamics Trajectories,https://arxiv.org/abs/2409.17808,3.00,,,,,,,,,280000001,"1,000 tokens/simulation = 250 frames × 4 residues
1,250 tokens/simulation = 250 frames × 5 residues
64,000 tokens/simulation = 250 frames × 256 residues

Explicit tetrapeptides: 3,109,000 = 3,109 × 1,000
Implicit tetrapeptides: 2,646,000 = 2,646 × 1,000
Implicit pentapeptides: 6,493,750 = 5,195 × 1,250
Protein simulations: 266,880,000 = 4,170 × 64,000

Total: 278,128,750 = 3,109,000 + 2,646,000 + 6,493,750 + 266,880,000",,,,,,,,"Molecular dynamics (MD) is a powerful technique for studying microscopic phenomena, but its computational cost has driven significant interest in the development of deep learning-based surrogate models. We introduce generative modeling of molecular trajectories as a paradigm for learning flexible multi-task surrogate models of MD from data. By conditioning on appropriately chosen frames of the trajectory, we show such generative models can be adapted to diverse tasks such as forward simulation, transition path sampling, and trajectory upsampling. By alternatively conditioning on part of the molecular system and inpainting the rest, we also demonstrate the first steps towards dynamics-conditioned molecular design. We validate the full set of these capabilities on tetrapeptide simulations and show that our model can produce reasonable ensembles of protein monomers. Altogether, our work illustrates how generative modeling can unlock value from MD data towards diverse downstream tasks that are not straightforward to address with existing methods or even MD itself. Code is available at this https URL.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
ProteinMPNN,Biology,"Protein or nucleotide language model (pLM/nLM),Protein design",,"J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan, B. Koepnick, H. Nguyen, A. Kang, B. Sankaran, A. K. Bera, N. P. King, D. Baker",2022-09-15,Robust deep learning–based protein sequence design using ProteinMPNN,https://www.science.org/doi/full/10.1126/science.add2187,793.00,,,,,,,,,6000001,"19,700 proteins × 300 residues/protein = 5,910,000 residues ≈ 6.0 × 10⁶ datapoints",,,,,,,,"While deep learning has revolutionized protein structure prediction, almost all experimentally characterized de novo protein designs have been generated using physically based approaches such as Rosetta. Here we describe a deep learning based protein sequence design method, ProteinMPNN, with outstanding performance in both in silico and experimental tests. The amino acid sequence at different positions can be coupled between single or multiple chains, enabling application to a wide range of current protein design challenges. On native protein backbones, ProteinMPNN has a sequence recovery of 52.4%, compared to 32.9% for Rosetta. Incorporation of noise during training improves sequence recovery on protein structure models, and produces sequences which more robustly encode their structures as assessed using structure prediction algorithms. We demonstrate the broad utility and high accuracy of ProteinMPNN using X-ray crystallography, cryoEM and functional studies by rescuing previously failed designs, made using Rosetta or AlphaFold, of protein monomers, cyclic homo-oligomers, tetrahedral nanoparticles, and target binding proteins.",,,Unverified,,,,,,,,,,,,,,
Innovative Drug-like Molecule Generation from,Biology,Drug discovery,"University of Pittsburgh,Carnegie Mellon University (CMU)","Haotian Zhang, Linxiaoyi Wan",2022-11-12,Innovative Drug-like Molecule Generation from Flow-based Generative Model,https://arxiv.org/abs/2211.06566,,,,,,,,,,1000001,"Total data points = 50,000 binding structures × 20 atoms/ligand = 1,000,000 data points
[Final estimate: 1.0e6]",,,,,,,,"To design a drug given a biological molecule by using deep learning methods, there are many successful models published recently. People commonly used generative models to design new molecules given certain protein. LiGAN was regarded as the baseline of deep learning model which was developed on convolutional neural networks. Recently, GraphBP showed its ability to predict innovative ""real"" chemicals that the binding affinity outperformed with traditional molecular docking methods by using a flow-based generative model with a graph neural network and multilayer perception. However, all those methods regarded proteins as rigid bodies and only include a very small part of proteins related to binding. However, the dynamics of proteins are essential for drug binding. Based on GraphBP, we proposed to generate more solid work derived from protein data bank. The results will be evaluated by validity and binding affinity by using a computational chemistry algorithm.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary,Biology,"Protein or nucleotide language model (pLM/nLM),Protein folding prediction",Henan University,"Wei Yang, Chun Liu, Zheng Li",2023-03-23,Lightweight Fine-tuning a Pretrained Protein Language Model for Protein Secondary Structure Prediction,https://www.biorxiv.org/content/10.1101/2023.03.22.530066v1.abstract,,,,,,6050000000000008000.00,"1. Hardware setup: 1x NVIDIA GeForce RTX 3090 Ti (1.60 x 10^14 FP16 FLOP/s)

2. Training duration: Directly provided - 2,200 seconds/epoch × 43 epochs = 94,600 seconds

3. Utilization rate: 40%

4. Final calculation: 1.60 × 10^14 FLOP/s × 94,600 seconds × 0.4 = 6.05 × 10^18 FLOPs",,,10000001,"25,792 training chains × 400 residues = 10,316,800 tokens ≈ 1.0e7",,,,,,,,"Pretrained large-scale protein language models, such as ESM-1b and ProtTrans, are becoming the fundamental infrastructure for various protein-related biological modeling tasks. Existing works use mainly pretrained protein language models in feature extraction. However, the knowledge contained in the embedding features directly extracted from a pretrained model is task-agnostic. To obtain task-specific feature representations, a reasonable approach is to fine-tune a pretrained model based on labeled datasets from downstream tasks. To this end, we investigate the fine-tuning of a given pretrained protein language model for protein secondary structure prediction tasks. Specifically, we propose a novel end-to-end protein secondary structure prediction framework involving the lightweight fine-tuning of a pretrained model. The framework first introduces a few new parameters for each transformer block in the pretrained model, then updates only the newly introduced parameters, and then keeps the original pretrained parameters fixed during training. Extensive experiments on seven test sets, namely, CASP12, CASP13, CASP14, CB433, CB634, TEST2016, and TEST2018, show that the proposed framework outperforms existing predictors and achieves new state-of-the-art prediction performance. Furthermore, we also experimentally demonstrate that lightweight fine-tuning significantly outperforms full model fine-tuning and feature extraction in enabling models to predict secondary structures. Further analysis indicates that only a few top transformer blocks need to introduce new parameters, while skipping many lower transformer blocks has little impact on the prediction accuracy of secondary structures.",,,Unverified,China,,,,,,Academia,,,Academia,,,,Hardware
Beyond ESM2: Graph-Enhanced Protein Sequence Modeling with Efficient,Biology,Protein or nucleotide language model (pLM/nLM),"Huazhong University of Science and Technology,Fudan University,Northwestern Polytechnical University","Shujian Jiao, Bingxuan Li, Lei Wang, Xiaojin Zhang, Wei Chen, Jiajie Peng, Zhongyu Wei",2024-04-24,Beyond ESM2: Graph-Enhanced Protein Sequence Modeling with Efficient Clustering,https://arxiv.org/abs/2404.15805,,,,,,,,,,86000000001,"Data points = 86 billion (8.6e10)
Calculation: 250 million protein sequences containing 86 billion amino acids total
86,000,000,000 = 8.6e10",,,,,,,,"Proteins are essential to life's processes, underpinning evolution and diversity. Advances in sequencing technology have revealed millions of proteins, underscoring the need for sophisticated pre-trained protein models for biological analysis and AI development. Facebook's ESM2, the most advanced protein language model to date, leverages a masked prediction task for unsupervised learning, crafting amino acid representations with notable biochemical accuracy. Yet, it lacks in delivering functional protein insights, signaling an opportunity for enhancing representation this http URL study addresses this gap by incorporating protein family classification into ESM2's this http URL approach, augmented with Community Propagation-Based Clustering Algorithm, improves global protein representations, while a contextual prediction task fine-tunes local amino acid accuracy. Significantly, our model achieved state-of-the-art results in several downstream experiments, demonstrating the power of combining global and local methodologies to substantially boost protein representation quality.",,,Unverified,"China,China,China",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
RENNAISSANCE,Biology,Cell Biology,"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),University of Cambridge,Harvard Medical School","Subham Choudhury, Bharath Narayanan, Michael Moret, Vassily Hatzimanikatis, Ljubisa Miskovic",2024-08-30,Generative machine learning produces kinetic models that accurately characterize intracellular metabolic states,https://www.nature.com/articles/s41929-024-01220-6.pdf,10.00,,,,,,,,,,,,,,,,,,"Generating large omics datasets has become routine for gaining insights into cellular processes, yet deciphering these datasets to determine metabolic states remains challenging. Kinetic models can help integrate omics data by explicitly linking metabolite concentrations, metabolic fluxes and enzyme levels. Nevertheless, determining the kinetic parameters that underlie cellular physiology poses notable obstacles to the widespread use of these mathematical representations of metabolism. Here we present RENAISSANCE, a generative machine learning framework for efficiently parameterizing large-scale kinetic models with dynamic properties matching experimental observations. Through seamless integration of diverse omics data and other relevant information, including extracellular medium composition, physicochemical data and expertise of domain specialists, RENAISSANCE accurately characterizes intracellular metabolic states in Escherichia coli. It also estimates missing kinetic parameters and reconciles them with sparse experimental data, substantially reducing parameter uncertainty and improving accuracy. This framework will be valuable for researchers studying metabolic variations involving changes in metabolite and enzyme levels and enzyme activity in health and biotechnology.",,,Unverified,"Switzerland,United Kingdom of Great Britain and Northern Ireland,United States of America",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
AntiFormer,Biology,Protein protein binding affinity prediction,"University of Florida,Sichuan University,Shihezi University,University of Macau,University of Texas Health Science Center","Qing Wang, Yuzhou Feng, Yanfei Wang, Bo Li, Jianguo Wen, Xiaobo Zhou, Qianqian Song",2024-08-20,AntiFormer: graph enhanced large language model for binding affinity prediction ,https://academic.oup.com/bib/article/25/5/bbae403/7736247,0.00,,,24670596.00,Loaded provided model file from Zenodo page into PyTorch. ,1710000000000014800.00,"1. Hardware setup: 1x NVIDIA A100 GPU (3.12x10^14 FLOP/s using FP16 Tensor Core)

2. Training duration: Directly provided - 0.76 hours per epoch x 5 folds = 3.8 hours (13,680 seconds)

3. Utilization rate: 40%

4. Calculation: 3.12x10^14 FLOP/s × 13,680s × 1 GPU × 0.4 = 1.71×10^18 FLOPs",,,,,,,"""AntiBERTy adopts the multiple
instance learning and results in much longer training time (1.46 h)
than Transformer-12 L (0.63 h) and Transformer-6 L (0.38 h).""",,,,,"Antibodies play a pivotal role in immune defense and serve as key therapeutic agents. The process of affinity maturation, wherein
antibodies evolve through somatic mutations to achieve heightened specificity and affinity to target antigens, is crucial for effective
immune response. Despite their significance, assessing antibody–antigen binding affinity remains challenging due to limitations in
conventional wet lab techniques. To address this, we introduce AntiFormer, a graph-based large language model designed to predict
antibody binding affinity. AntiFormer incorporates sequence information into a graph-based framework, allowing for precise prediction
of binding affinity. Through extensive evaluations, AntiFormer demonstrates superior performance compared with existing methods,
offering accurate predictions with reduced computational time. Application of AntiFormer to severe acute respiratory syndrome
coronavirus 2 patient samples reveals antibodies with strong neutralizing capabilities, providing insights for therapeutic development
and vaccination strategies. Furthermore, analysis of individual samples following inf luenza vaccination elucidates differences in
antibody response between young and older adults. AntiFormer identifies specific clonotypes with enhanced binding affinity postvaccination, particularly in young individuals, suggesting age-related variations in immune response dynamics. Moreover, our findings
underscore the importance of large clonotype category in driving affinity maturation and immune modulation. Overall, AntiFormer is
a promising approach to accelerate antibody-based diagnostics and therapeutics, bridging the gap between traditional methods and
complex antibody maturation processes.",Open weights (unrestricted),,Unverified,"United States of America,China,China,Macao,United States of America",,,,64,"""The training batch size is set to 64, the test batch size to 128, and the learning rate is 0.0001. AntiFormer is trained on one NVIDIA A100 TENSOR CORE GPU with 40 GB memory"" from Model training paragraph","Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia",,,,Hardware
ChatMol,Biology,Drug discovery,"Tsinghua University,PingAn Technology,Beijing University of Posts and Telecommunications","Zheni Zeng, Bangchen Yin, Shipeng Wang, Jiarui Liu, Cheng Yang, Haishen Yao, Xingzhi Sun, Maosong Sun, Guotong Xie, Zhiyuan Liu",2024-09-02,ChatMol: interactive molecular discovery with natural language ,https://academic.oup.com/bioinformatics/article/40/9/btae534/7747661,13.00,,,,,,,,,3072001,"Summary of calculations:

ChEBI-20: 26,407 samples × 143 tokens/sample = 3,775,701 tokens
PCdes: 10,500 samples × 162 tokens/sample = 1,701,000 tokens
ChEBI-dia: 7,361 dialogs × 200 tokens/dialog = 1,472,200 tokens
Total dataset tokens: 3,775,701 + 1,701,000 + 1,472,200 = 7,948,901 tokens
Training tokens: 12,000 steps × 256 tokens/step = 3,072,000 tokens
Final data estimate: 3.072 × 10^6 tokens",,,,,,,,"Motivation: Natural language is poised to become a key medium for human–machine interactions in the era of large language models. In the
field of biochemistry, tasks such as property prediction and molecule mining are critically important yet technically challenging. Bridging molecular expressions in natural language and chemical language can significantly enhance the interpretability and ease of these tasks. Moreover, it
can integrate chemical knowledge from various sources, leading to a deeper understanding of molecules.
Results: Recognizing these advantages, we introduce the concept of conversational molecular design, a novel task that utilizes natural language
to describe and edit target molecules. To better accomplish this task, we develop ChatMol, a knowledgeable and versatile generative pretrained
model. This model is enhanced by incorporating experimental property information, molecular spatial knowledge, and the associations between
natural and chemical languages. Several typical solutions including large language models (e.g. ChatGPT) are evaluated, proving the challenge of
conversational molecular design and the effectiveness of our knowledge enhancement approach. Case observations and analysis offer insights
and directions for further exploration of natural-language interaction in molecular discovery.",,,Unverified,"China,China,China",,,,,,"Academia,Industry,Academia",,,"Academia,Industry,Academia",,,,
ParetoDrug,Biology,Drug discovery,"Chinese University of Hong Kong (CUHK),Zhejiang Lab,Zhejiang University,Huawei Noah's Ark Lab","Yaodong Yang, Guangyong Chen, Jinpeng Li, Junyou Li, Odin Zhang, Xujun Zhang, Lanqing Li, Jianye Hao, Ercheng Wang, Pheng-Ann Heng",2024-09-02,Enabling target-aware molecule generation to follow multi objectives with Pareto MCTS,https://www.nature.com/articles/s42003-024-06746-w,1.00,,,,,,,,,,,,,,,,,,"Target-aware drug discovery has greatly accelerated the drug discovery process to design smallmolecule ligands with high binding affinity to disease-related protein targets. Conditioned on targeted proteins, previous works utilize various kinds of deep generative models and have shown great potential in generating molecules with strong protein-ligand binding interactions. However, beyond binding affinity, effective drug molecules must manifest other essential properties such as high druglikeness, which are not explicitly addressed by current target-aware generative methods. In this article, aiming to bridge the gap of multi-objective target-aware molecule generation in the field of deep learning-based drug discovery, we propose ParetoDrug, a Pareto Monte Carlo Tree Search (MCTS) generation algorithm. ParetoDrug searches molecules on the Pareto Front in chemical space using MCTS to enable synchronous optimization of multiple properties. Specifically, ParetoDrug utilizes pretrained atom-by-atom autoregressive generative models for the exploration guidance to desired molecules during MCTS searching. Besides, when selecting the next atom symbol, a scheme named ParetoPUCT is proposed to balance exploration and exploitation. Benchmark experiments and case studies demonstrate that ParetoDrug is highly effective in traversing the large and complex chemical space to discover novel compounds with satisfactory binding affinities and drug-like properties for various multi-objective target-aware drug discovery tasks.",,,Unverified,"Hong Kong,China,China,China,China",,,,,,"Academia,Academia,Industry",,,"Academia,Academia,Industry",,,,
DTI-LM,Biology,Protein-ligand binding affinity prediction,University of Central Florida,"Khandakar Tanvir Ahmed, Md. Istiaq Ansari, Wei Zhang",2024-09-02,DTI-LM: language model powered drug–target interaction prediction,https://academic.oup.com/bioinformatics/article-abstract/40/9/btae533/7747660,0.00,,,,,,,,,15056,"DrugBank: 6,041
BindingDB: 4,040
Yamanishi_08: 3,448
Luo's Dataset: 1,526

Total = 6,041 + 4,040 + 3,448 + 1,526 = 15,055 datapoints",,,,,,,,"Motivation: The identification and understanding of drug–target interactions (DTIs) play a pivotal role in the drug discovery and development
process. Sequence representations of drugs and proteins in computational model offer advantages such as their widespread availability, easier
input quality control, and reduced computational resource requirements. These make them an efficient and accessible tools for various computational biology and drug discovery applications. Many sequence-based DTI prediction methods have been developed over the years. Despite
the advancement in methodology, cold start DTI prediction involving unknown drug or protein remains a challenging task, particularly for
sequence-based models. Introducing DTI-LM, a novel framework leveraging advanced pretrained language models, we harness their exceptional context-capturing abilities along with neighborhood information to predict DTIs. DTI-LM is specifically designed to rely solely on sequence
representations for drugs and proteins, aiming to bridge the gap between warm start and cold start predictions.
Results: Large-scale experiments on four datasets show that DTI-LM can achieve state-of-the-art performance on DTI predictions. Notably, it
excels in overcoming the common challenges faced by sequence-based models in cold start predictions for proteins, yielding impressive
results. The incorporation of neighborhood information through a graph attention network further enhances prediction accuracy. Nevertheless,
a disparity persists between cold start predictions for proteins and drugs. A detailed examination of DTI-LM reveals that language models exhibit contrasting capabilities in capturing similarities between drugs and proteins.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
Reinvent 4,Biology,Drug discovery,AstraZeneca,"Hannes H. Loeffler, Jiazhen He, Alessandro Tibo, Jon Paul Janet, Alexey Voronov, Lewis H. Mervin, Ola Engkvist",2024-02-21,Reinvent 4: Modern AI–driven generative molecule design,https://link.springer.com/article/10.1186/s13321-024-00812-5,50.00,,,,,,,,,20000000000001,"Prior Model: 1M molecules × 50 tokens/molecule = 5 × 10^7 tokens
Mol2Mol Agent: 200B pairs × 2 molecules/pair × 50 tokens/molecule = 2 × 10^13 tokens
Total: 5 × 10^7 + 2 × 10^13 ≈ 2.0 × 10^13 tokens",,,,,,,,"REINVENT 4 is a modern open-source generative AI framework for the design of small molecules. The software utilizes recurrent neural networks and transformer architectures to drive molecule generation. These generators are seamlessly embedded within the general machine learning optimization algorithms, transfer learning, reinforcement learning and curriculum learning. REINVENT 4 enables and facilitates de novo design, R-group replacement, library design, linker design, scaffold hopping and molecule optimization. This contribution gives an overview of the software and describes its design. Algorithms and their applications are discussed in detail. REINVENT 4 is a command line tool which reads a user configuration in either TOML or JSON format. The aim of this release is to provide reference implementations for some of the most common algorithms in AI based molecule generation. An additional goal with the release is to create a framework for education and future innovation in AI based molecular design. The software is available from https://github.com/MolecularAI/REINVENT4 and released under the permissive Apache 2.0 license. Scientific contribution. The software provides an open–source reference implementation for generative molecular design where the software is also being used in production to support in–house drug discovery projects. The publication of the most common machine learning algorithms in one code and full documentation thereof will increase transparency of AI and foster innovation, collaboration and education.",,,Unverified,"Sweden,United Kingdom of Great Britain and Northern Ireland",,,,,,Industry,,,Industry,,,,
DrugFormer,Biology,Drug Sensitivity Prediction,"University of Florida,University of Texas Health Science Center,H. Lee Moffitt Cancer Center and Research Institute","Xiaona Liu, Qing Wang, Minghao Zhou, Yanfei Wang, Xuefeng Wang, Xiaobo Zhou, Qianqian Song",2024-08-29,DrugFormer: Graph‐Enhanced Language Model to Predict Drug Sensitivity,https://onlinelibrary.wiley.com/doi/full/10.1002/advs.202405861,,,,,,,,,,,,,,,,,,,"Drug resistance poses a crucial challenge in healthcare, with response rates to chemotherapy and targeted therapy remaining low. Individual patient’s resistance is exacerbated by the intricate heterogeneity of tumor cells, presenting significant obstacles to effective treatment. To address this challenge, DrugFormer, a novel graph-augmented large language model designed to predict drug resistance at single-cell level is proposed. DrugFormer integrates both serialized gene tokens and gene-based knowledge graphs for the accurate predictions of drug response. After training on comprehensive single-cell data with drug response information, DrugFormer model presents outperformance, with higher F1, precision, and recall in predicting drug response. Based on the scRNA-seq data from refractory multiple myeloma (MM) and acute myeloid leukemia (AML) patients, DrugFormer demonstrates high efficacy in identifying resistant cells and uncovering underlying molecular mechanisms. Through pseudotime trajectory analysisunique drug-resistant cellular states associated with poor patient outcomes are revealed. Furthermore, DrugFormer identifies potential therapeutic targets, such as COX8A, for overcoming drug resistance across different cancer types. In conclusion, DrugFormer represents a significant advancement in the field of drug resistance prediction, offering a powerful tool for unraveling the heterogeneity of cellular response to drugs and guiding personalized treatment strategies.",,,Unverified,"United States of America,United States of America,United States of America",,,,,,Academia,,,Academia,,,,
CHIEF,Biology,Cancer diagnosis,"Harvard Medical School,Massachusetts Institute of Technology (MIT),Sichuan University,Sun Yat-sen University,Shenzhen Maternity & Child Healthcare Hospital,Chongqing University Cancer Hospital,Harvard University,University of Pennsylvania,Cedars-Sinai Medical Center,Broad Institute,Dana-Farber Cancer Institute,Brigham and Women's Hospital,Tencent,Massachusettes General Hospital,Pennsylvania State University,Jinan University,Stanford University","Xiyue Wang, Junhan Zhao, Eliana Marostica, Wei Yuan, Jietian Jin, Jiayu Zhang, Ruijiang Li, Hongping Tang, Kanran Wang, Yu Li, Fang Wang, Yulong Peng, Junyou Zhu, Jing Zhang, Christopher R. Jackson, Jun Zhang, Deborah Dillon, Nancy U. Lin, Lynette Sholl, Thomas Denize, David Meredith, Keith L. Ligon, Sabina Signoretti, Shuji Ogino, Jeffrey A. Golden, MacLean P. Nasrallah, Xiao Han, Sen Yang, Kun-Hsing Yu",2024-09-04,A Pathology Foundation Model for Cancer Diagnosis and Prognosis Prediction ,https://www.nature.com/articles/s41586-024-07894-z,,,,,,,,,,225000001,"(44 TB × 10^12 bytes) ÷ 60,530 WSIs = 7.26 × 10^8 bytes/WSI
(256 × 256 × 3) = 196,608 bytes per tile
(7.26 × 10^8 bytes) ÷ (2 × 10^5 bytes/tile) = 3,630 tiles/WSI
60,530 WSIs × 3,630 tiles/WSI = 2.19 × 10^8 tiles
1.5 × 10^7 + 2.19 × 10^8 = 2.34 × 10^8 total datapoints

Final estimate: 2.25 × 10^8 datapoints",,,,,,,,"Histopathology image evaluation is indispensable for cancer diagnoses and subtype classification. Standard artificial intelligence methods for histopathology image analyses have focused on optimizing specialized models for each diagnostic task1,2. Although such methods have achieved some success, they often have limited generalizability to images generated by different digitization protocols or samples collected from different populations3. Here, to address this challenge, we devised the Clinical Histopathology Imaging Evaluation Foundation (CHIEF) model, a general-purpose weakly supervised machine learning framework to extract pathology imaging features for systematic cancer evaluation. CHIEF leverages two complementary pretraining methods to extract diverse pathology representations: unsupervised pretraining for tile-level feature identification and weakly supervised pretraining for whole-slide pattern recognition. We developed CHIEF using 60,530 whole-slide images spanning 19 anatomical sites. Through pretraining on 44 terabytes of high-resolution pathology imaging datasets, CHIEF extracted microscopic representations useful for cancer cell detection, tumour origin identification, molecular profile characterization and prognostic prediction. We successfully validated CHIEF using 19,491 whole-slide images from 32 independent slide sets collected from 24 hospitals and cohorts internationally. Overall, CHIEF outperformed the state-of-the-art deep learning methods by up to 36.1%, showing its ability to address domain shifts observed in samples from diverse populations and processed by different slide preparation methods. CHIEF provides a generalizable foundation for efficient digital pathology evaluation for patients with cancer.",,,Unverified,"United States of America,United States of America,China,China,China,China,United States of America,United States of America,United States of America,United States of America,United States of America,United States of America,China,United States of America,United States of America,China,United States of America",,,,,,"Academia,Academia,Academia,Academia,Academia,Academia,Research collective,Industry,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia,Academia,Academia,Research collective,Industry,Academia,Academia,Academia",,,,
Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-Terminal Coding Sequences,Biology,Gene expression enhancement,"National University of Singapore,Jiangnan University","Zhanglu Yan, Weiran Chu, Yuhua Sheng, Kaiwen Tang, Shida Wang, Yanfeng Liu, Weng-Fai Wong",2024-09-04,Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-Terminal Coding Sequences,https://pubs.acs.org/doi/abs/10.1021/acssynbio.4c00371,0.00,,,,,,"All experiments were conducted on an Intel Xeon E5-2680 server, boasting 256GB DRAM and running 64-bit Linux 4.15.0. This server was equipped with both an Nvidia Tesla P100 GPU and a GeForce RT 3090 GPU.",,,1996,"Initial sequences: 73 × 15 = 1,095 codons
New sequences: 6 × 10 × 15 = 900 codons
Total: 1,095 + 900 = 1,995 codons
Final estimate: 1.995e3",,,,,,,,"N-terminal coding sequence (NCS) influences gene expression by impacting the translation initiation rate. The NCS optimization problem is to find an NCS that maximizes gene expression. The problem is important in genetic engineering. However, current methods for NCS optimization such as rational design and statistics-guided approaches are labor-intensive yield only relatively small improvements. This paper introduces a deep learning/synthetic biology co-designed few-shot training workflow for NCS optimization. Our method utilizes k-nearest encoding followed by word2vec to encode the NCS, then performs feature extraction using attention mechanisms, before constructing a time-series network for predicting gene expression intensity, and finally a direct search algorithm identifies the optimal NCS with limited training data. We took green fluorescent protein (GFP) expressed by Bacillus subtilis as a reporting protein of NCSs, and employed the fluorescence enhancement factor as the metric of NCS optimization. Within just six iterative experiments, our model generated an NCS (MLD62) that increased average GFP",,,Unverified,"Singapore,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
µFormer,Biology,Protein design,Microsoft Research AI for Science,"Haoran Sun, Liang He, Pan Deng, Guoqing Liu, Haiguang Liu, Chuan Cao, Fusong Ju, Lijun Wu, Tao Qin, Tie-Yan Liu",2024-09-05,Accelerating protein engineering with fitness landscape modeling and reinforcement learning,https://www.biorxiv.org/content/10.1101/2023.11.16.565910v3.abstract,0.00,,,,,,,,,9000000001,"30,000,000 proteins × 300 amino acids/protein = 9,000,000,000 datapoints

Total datapoints: 9.0e9",,,,,,,,"Protein engineering plays a pivotal role in designing novel proteins with desired functions, yet the rugged fitness landscape of proteins within their mutant space presents a major challenge, limiting the effective discovery of optimal sequences. To address this, we introduce µFormer, a deep learning framework that combines a pre-trained protein language model with custom-designed scoring modules to predict the mutational effects of proteins. µFormer achieves state-of-the-art performance in predicting high-order mutants, modeling epistatic interactions, and handling insertion. By integrating µFormer with a reinforcement learning framework, we enable efficient exploration of vast mutant spaces, encompassing trillions of mutation candidates, to design protein variants with enhanced activity. Remarkably, we successfully predicted mutants that exhibited a 2000-fold increase in bacterial growth rate due to enhanced enzymatic activity. These results highlight the effectiveness of our approach in identifying impactful mutations across diverse protein targets and fitness metrics, offering a powerful tool for optimizing proteins with significantly higher success rates.",,,Unverified,"United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany",,,,,,Industry,,,Industry,,,,
EnOpt,Biology,Protein-ligand binding affinity prediction,University of Pittsburgh,"Roshni Bhatt, Ann Wang, Jacob D. Durrant",2024-09-05,Teaching old docks new tricks with machine learning enhanced ensemble docking,https://www.nature.com/articles/s41598-024-71699-3,0.00,,,,,,,,,,,,,,,,,,"We here introduce Ensemble Optimizer (EnOpt), a machine-learning tool to improve the accuracy and interpretability of ensemble virtual screening (VS). Ensemble VS is an established method for predicting protein/small-molecule (ligand) binding. Unlike traditional VS, which focuses on a single protein conformation, ensemble VS better accounts for protein flexibility by predicting binding to multiple protein conformations. Each compound is thus associated with a spectrum of scores (one score per protein conformation) rather than a single score. To effectively rank and prioritize the molecules for further evaluation (including experimental testing), researchers must select which protein conformations to consider and how best to map each compound’s spectrum of scores to a single value, decisions that are system-specific. EnOpt uses machine learning to address these challenges. We perform benchmark VS to show that for many systems, EnOpt ranking distinguishes active compounds from inactive or decoy molecules more effectively than traditional ensemble VS methods. To encourage broad adoption, we release EnOpt free of charge under the terms of the MIT license.

",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
GearBind,Biology,Protein design,"BioGeometry,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),University of Montreal / Université de Montréal,Fudan University,HEC Montreal","Huiyu Cai, Zuobai Zhang, Mingkai Wang, Bozitao Zhong, Quanxiao Li, Yuxuan Zhong, Yanling Wu, Tianlei Ying, Jian Tang",2024-09-06,Pretrainable geometric graph neural network for antibody affinity maturation,https://www.nature.com/articles/s41467-024-51563-8,4.00,,,,,,"""The model was trained using the Adam optimizer with a learning rate of 1e-4 and a batch size of 8. The training process is performed on 1 A100 GPU for 40 epochs. For pretraining, we use the same architecture with 4-layer GearBind model with a hidden dimension of 128. The pretraining was conducted using the Adam optimizer with a learning rate of 5e-4 and a batch size of 8, employing 4 A100 GPUs for 10 epochs.""",,,,,,,,,,,,"Increasing the binding affinity of an antibody to its target antigen is a crucial task in antibody therapeutics development. This paper presents a pretrainable geometric graph neural network, GearBind, and explores its potential in in silico affinity maturation. Leveraging multi-relational graph construction, multi-level geometric message passing and contrastive pretraining on massscale, unlabeled protein structural data, GearBind outperforms previous stateof-the-art approaches on SKEMPI and an independent test set. A powerful ensemble model based on GearBind is then derived and used to successfully enhance the binding of two antibodies with distinct formats and target antigens. ELISA EC50 values of the designed antibody mutants are decreased by up to 17 fold, and KD values by up to 6.1 fold. These promising results underscore the utility of geometric deep learning and effective pretraining in macromolecule interaction modeling tasks.",,,Unverified,"China,Canada,Canada,China,Canada",,,,,,"Industry,Academia,Academia,Academia,Academia",,,"Industry,Academia,Academia,Academia,Academia",,,,
ProtSSN,Biology,Protein design,"Shanghai Jiao Tong University,East China University of Science and Technology,Shanghai AI Lab","Yang Tan, Bingxin Zhou, Lirong Zheng, Guisheng Fan, Liang Hong",2024-09-06,Semantical and Geometrical Protein Encoding Toward Enhanced Bioactivity and Thermostability,https://www.biorxiv.org/content/10.1101/2023.12.01.569522v3.abstract,7.00,,,,,,,,,12000001,"30,948 proteins * 300 amino acids/protein = 9,284,400 tokens ≈ 1.2e7 tokens

30,948 * 300 = 9,284,400",,,,,,,,"Protein engineering is a pivotal aspect of synthetic biology, involving the modification of amino acids within existing protein sequences to achieve novel or enhanced functionalities and physical properties. Accurate prediction of protein variant effects requires a thorough understanding of protein sequence, structure, and function. Deep learning methods have demonstrated remarkable performance in guiding protein modification for improved functionality. However, existing approaches predominantly rely on protein sequences, which face challenges in efficiently encoding the geometric aspects of amino acids’ local environment and often fall short in capturing crucial details related to protein folding stability, internal molecular interactions, and bio-functions. Furthermore, there lacks a fundamental evaluation for developed methods in predicting protein thermostability, although it is a key physical property that is frequently investigated in practice. To address these challenges, this paper introduces a novel pre-training framework that integrates sequential and geometric encoders for protein primary and tertiary structures. This framework guides mutation directions toward desired traits by simulating natural selection on wild-type proteins and evaluates variant effects based on their fitness to perform specific functions. We assess the proposed approach using three benchmarks comprising over 300 deep mutational scanning assays. The prediction results showcase exceptional performance across extensive experiments when compared to other zero-shot learning methods, all while maintaining a minimal cost in terms of trainable parameters. This study not only proposes an effective framework for more accurate and comprehensive predictions to facilitate efficient protein engineering, but also enhances the in silico assessment system for future deep learning models to better align with empirical requirements. The PyTorch implementation are available at https://github.com/tyang816/ProtSSN.",,,Unverified,"China,China,China",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation,Biology,Drug discovery,"ETH Zurich,University of Zurich,ETH AI Center","Heath Arthur-Loui, Amina Mollaysa, Michael Krauthammer",2024-08-19,Rethinking Molecular Design: Integrating Latent Variable and Auto-Regressive Models for Goal Directed Generation,https://arxiv.org/abs/2409.00046,0.00,,,,,4999999999999999000.00,"1. Hardware setup: 1x NVIDIA RTX 4090 (3.30×10¹⁴ FP16 FLOP/s)

2. Training duration: 7.9 hours (estimated)
   * 364,000 total samples
   * 128 batch size
   * 200 epochs
   * 50ms per step
   * Total: 28,430 seconds

3. Utilization rate: 40%

4. Final calculation:
   3.30×10¹⁴ FLOP/s × 28,430s × 0.4 = 5.0×10¹⁸ FLOPs",,,35000001,"QM9 Dataset: 113,885 molecules × 40 tokens = 4,555,400 tokens
ZINC250k Dataset: 250,000 molecules × 120 tokens = 30,000,000 tokens
Total: 4,555,400 + 30,000,000 = 34,555,400 tokens ≈ 3.5 × 10^7 tokens",,,,NVIDIA GeForce RTX 4090,1,,,"De novo molecule design has become a highly active research area, advanced significantly through the use of state-of-the-art generative models. Despite these advances, several fundamental questions remain unanswered as the field increasingly focuses on more complex generative models and sophisticated molecular representations as an answer to the challenges of drug design. In this paper, we return to the simplest representation of molecules, and investigate overlooked limitations of classical generative approaches, particularly Variational Autoencoders (VAEs) and auto-regressive models. We propose a hybrid model in the form of a novel regularizer that leverages the strengths of both to improve validity, conditional generation, and style transfer of molecular sequences. Additionally, we provide an in depth discussion of overlooked assumptions of these models' behaviour.",,,Unverified,"Switzerland,Switzerland,Switzerland",,,,,,"Academia,Academia,Research collective",,,"Academia,Academia,Research collective",,,991.6710179292822,Hardware
AF2RAVE,Biology,Drug discovery,University of Maryland,"Xinyu Gu, Akashnathan Aranganathan, Pratyush Tiwary",2024-04-10,Empowering AlphaFold2 for Protein Conformation-Selective Drug Discovery with AlphaFold2-RAVE,https://arxiv.org/abs/2404.07102v3,4.00,,,,,,,,,16000000001,"Initial simulations: 12 × (48.5×10^-9 / 2×10^-15) = 291×10^6 steps

Umbrella sampling: 12 × (11×11) × (90×10^-9 / 2×10^-15) = 6.534×10^10 steps

Total: 291×10^6 + 6.534×10^10 = 6.563×10^10 steps

Conservative estimate: 1.6×10^10 data points",,,,,,,,"Small molecule drug design hinges on obtaining co-crystallized ligand-protein structures. Despite AlphaFold2’s strides in protein native structure prediction, its focus on apo structures overlooks ligands and associated holo structures. Moreover, designing selective drugs often benefits from the targeting of diverse metastable conformations. Therefore, direct application of AlphaFold2 models in virtual screening and drug discovery remains tentative. Here, we demonstrate an AlphaFold2 based framework combined with all-atom enhanced sampling molecular dynamics and induced fit docking, named AF2RAVE-Glide, to conduct computational model based small molecule binding of metastable protein kinase conformations, initiated from protein sequences. We demonstrate the AF2RAVE-Glide workflow on three different protein kinases and their type I and II inhibitors, with special emphasis on binding of known type II kinase inhibitors which target the metastable classical DFG-out state. These states are not easy to sample from AlphaFold2. Here we demonstrate how with AF2RAVE these metastable conformations can be sampled for different kinases with high enough accuracy to enable subsequent docking of known type II kinase inhibitors with more than 50% success rates across docking calculations. We believe the protocol should be deployable for other kinases and more proteins generally.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
EvolMPNN,Biology,Mutation prediction,Aarhus university,"Zhiqiang Zhong, Davide Mottin",2024-08-22,Efficiently Predicting Mutational Effect on Homologous Proteins by Evolution Encoding,https://link.springer.com/chapter/10.1007/978-3-031-70368-3_24,,,,,,,,,,29000001,"Total proteins = 82,583 + 8,733 + 54,025 = 145,341 proteins
Residues per protein (avg) = 200
Total datapoints = 145,341 × 200 = 29,068,200 ≈ 2.9 × 10^7",,,,,,,,"Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, an efficient model to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way, EvolMPNN can efficiently utilise a novel message-passing method to capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 
 better than state-of-the-art methods and attains 
 inference speedup in comparison with large pre-trained models. Code and models are available at https://github.com/zhiqiangzhongddu/EvolMPNN.",,,Unverified,Denmark,,,,,,Academia,,,Academia,,,,
MolMVC,Biology,Molecular representation learning,"Central South University,Singapore Agency for Science","Zhijian Huang, Ziyu Fan, Siyuan Shen, Min Wu, Lei Deng",2024-09-04,MolMVC: Enhancing molecular representations for drug-related tasks through multi-view contrastive learning,https://academic.oup.com/bioinformatics/article/40/Supplement_2/ii190/7749083,0.00,,,,,,,,,3400001,"Total Datapoints = 3.4M molecular instances = 3.4e6

(Direct value from dataset size, no calculations needed)",,,,,,,,"Motivation
Effective molecular representation is critical in drug development. The complex nature of molecules demands comprehensive multi-view representations, considering 1D, 2D, and 3D aspects, to capture diverse perspectives. Obtaining representations that encompass these varied structures is crucial for a holistic understanding of molecules in drug-related contexts.

Results
In this study, we introduce an innovative multi-view contrastive learning framework for molecular representation, denoted as MolMVC. Initially, we use a Transformer encoder to capture 1D sequence information and a Graph Transformer to encode the intricate 2D and 3D structural details of molecules. Our approach incorporates a novel attention-guided augmentation scheme, leveraging prior knowledge to create positive samples tailored to different molecular data views. To align multi-view molecular positive samples effectively in latent space, we introduce an adaptive multi-view contrastive loss (AMCLoss). In particular, we calculate AMCLoss at various levels within the model to effectively capture the hierarchical nature of the molecular information. Eventually, we pre-train the encoders via minimizing AMCLoss to obtain the molecular representation, which can be used for various down-stream tasks. In our experiments, we evaluate the performance of our MolMVC on multiple tasks, including molecular property prediction (MPP), drug-target binding affinity (DTA) prediction and cancer drug response (CDR) prediction. The results demonstrate that the molecular representation learned by our MolMVC can enhance the predictive accuracy on these tasks and also reduce the computational costs. Furthermore, we showcase MolMVC’s efficacy in drug repositioning across a spectrum of drug-related applications.",,,Unverified,"China,Singapore",,,,,,"Academia,Government",,,"Academia,Government",,,,
SAAMBE-MEM,Biology,"Mutation prediction,Protein protein binding affinity prediction","Clemson University,Central China Normal University","Prawin Rimal, Shailesh Kumar Panday, Wang Xu, Yunhui Peng, Emil Alexov",2024-09-06,SAAMBE-MEM: A Sequence-Based Method for Predicting Binding Free Energy Change upon Mutation in Membrane Protein-Protein Complexes,https://academic.oup.com/bioinformatics/article/40/9/btae544/7750356,0.00,,,,,,,,,1041,"Dataset: MPAD_Clean
Unique datapoints = 1040 mutations = 1.04e3

Calculations:
1040 = 1.04 × 10³

Final result: 1.04e3 datapoints",,,,,,,,"Motivation
Mutations in protein–protein interactions can affect the corresponding complexes, impacting function and potentially leading to disease. Given the abundance of membrane proteins, it is crucial to assess the impact of mutations on the binding affinity of these proteins. Although several methods exist to predict the binding free energy change due to mutations in protein–protein complexes, most require structural information of the protein complex and are primarily trained on the SKEMPI database, which is composed mainly of soluble proteins.

Results
A novel sequence-based method (SAAMBE-MEM) for predicting binding free energy changes (ΔΔG) in membrane protein–protein complexes due to mutations has been developed. This method utilized the MPAD database, which contains binding affinities for wild-type and mutant membrane protein complexes. A machine learning model was developed to predict ΔΔG by leveraging features such as amino acid indices and position-specific scoring matrices (PSSM). Through extensive dataset curation and feature extraction, SAAMBE-MEM was trained and validated using the XGBoost regression algorithm. The optimal feature set, including PSSM-related features, achieved a Pearson correlation coefficient of 0.64, outperforming existing methods trained on the SKEMPI database. Furthermore, it was demonstrated that SAAMBE-MEM performs much better when utilizing evolution-based features in contrast to physicochemical features.",,,Unverified,"United States of America,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
DDGemb,Biology,"Mutation prediction,Protein stability prediction",University of Bologna,"Castrense Savojardo, Matteo Manfredi, Pier Luigi Martelli, Rita Casadio",2024-09-07,DDGemb: predicting protein stability change upon single- and multi-point variations with embeddings and deep learning,https://www.biorxiv.org/content/10.1101/2024.09.05.611455v1.abstract,,,,,,,,,,5099,"2648 + 2450 = 5098 unique data points
5098 data points = 5.098e3",,,,,,,,"The knowledge of protein stability upon residue variation is an important step for functional protein design and for understanding how protein variants can promote disease onset. Computational methods are important to complement experimental approaches and allow a fast screening of large datasets of variations. In this work we present DDGemb, a novel method combining protein language model embeddings and transformer architectures to predict protein 𝚫𝚫G upon both single- and multipoint variations. DDGemb has been trained on a high-quality dataset derived from literature and tested on available benchmark datasets of single- and multi-point variations. DDGemb performs at the state of the art in both single- and multi-point variations.",,,Unverified,Italy,,,,,,Academia,,,Academia,,,,
RiboCode,Biology,RNA design,"Sun Yat-sen University,Rhegen Biotechnology,Chinese Academy of Sciences","Yupeng Li, Fan Wang, Jiaqi Yang, Zirong Han, Linfeng Chen, Wenbing Jiang, Hao Zhou, Tong Li, Zehua Tang, Jianxiang Deng, Xin He, Gaofeng Zha, Jiekai Hu, Yong Hu, Linping Wu, Changyou Zhan, Caijun Sun, Yao He, Zhi Xie",2024-09-08,Deep Generative Optimization of mRNA Codon Sequences for Enhanced Protein Production and Therapeutic Efficacy,https://www.biorxiv.org/content/10.1101/2024.09.06.611590v1,,,,,,,,,,3200001,"320 datasets × 10,000 mRNAs/dataset = 3,200,000 data points
(3.2 × 10^6 data points)",,,,,,,,"Messenger RNA (mRNA) therapeutics show immense promise, but their efficacy is limited by suboptimal protein expression. Here, we present RiboCode, a deep learning framework that generates mRNA codon sequences for enhanced protein production. RiboCode introduces several advances, including direct learning from large-scale ribosome profiling data, context-aware mRNA optimization and generative exploration of a large sequence space. In silico analysis demonstrate RiboCode’s robust predictive accuracy for unseen genes and cellular environments. In vitro experiments show substantial improvements in protein expression, with up to a 72-fold increase, significantly outperforming past methods. In addition, RiboCode achieves cell-type specific expression and demonstrates robust performance across different mRNA formats, including m1Ψ-modified and circular mRNAs, an important feature for mRNA therapeutics. In vivo mouse studies show that optimized influenza hemagglutinin mRNAs induce ten times stronger neutralizing antibody responses against influenza virus compared to the unoptimized sequence. In an optic nerve crush model, optimized nerve growth factor mRNAs achieve equivalent neuroprotection of retinal ganglion cells at one-fifth the dose of the unoptimized sequence. Collectively, RiboCode represents a paradigm shift from rule-based to data-driven, context-sensitive approach for mRNA therapeutic applications, enabling the development of more potent and dose-efficient treatments.",,,Unverified,"China,China,China",,,,,,"Academia,Industry,Academia",,,"Academia,Industry,Academia",,,,
Pro-PRIME,Biology,Protein design,"Shanghai Jiao Tong University,Shanghai AI Lab,East China University of Science and Technology,Shanghai Tech University,Guangzhou Inernational Bio Island,Chinese Academy of Sciences,Shanghai Academy of Experimental Medicine","Fan Jiang, Mingchen Li, Jiajun Dong, Yuanxi Yu, Xinyu Sun, Banghao Wu, Jin Huang, Liqi Kang, Yufeng Pei, Liang Zhang, Shaojie Wang, Wenxue Xu, Jingyao Xin, Wanli Ouyang, Guisheng Fan, Lirong Zheng, Yang Tan, Zhiqiang Hu, Yi Xiong, Yan Feng, Guangyu Yang, Qian Liu, Jie Song, Jia Liu, Liang Hong, Pan Tan",2024-10-28,Pro-PRIME: A general Temperature-Guided Language model to engineer enhanced Stability and Activity in Proteins,https://arxiv.org/abs/2307.12682,1.00,,,,,42999999999999880000.00,"1. Hardware setup: 8x NVIDIA A100 80G GPUs (3.12e14 FLOP/s per GPU)

2. Training duration: 12 hours (estimated based on 200,000 update steps at ~1 second per step, rounded up from 6.94 hours for conservative estimate)

3. Utilization rate: 40%

4. Calculation: 3.12e14 FLOP/s × 8 GPUs × 43,200 seconds × 0.4 = 4.31e19 FLOPs",,,28800000001,"96,000,000 sequences × 300 residues/sequence = 28,800,000,000 tokens (2.88 × 10¹⁰)",,,,,,,,"Designing protein mutants of both high stability and activity is a critical yet challenging task in protein engineering. Here, we introduce PRIME, a deep learning model, which can suggest protein mutants of improved stability and activity without any prior experimental mutagenesis data of the specified protein. Leveraging temperature-aware language modeling, PRIME demonstrated superior predictive power compared to current state-of-the-art models on the public mutagenesis dataset over 283 protein assays. Furthermore, we validated PRIME's predictions on five proteins, examining the top 30-45 single-site mutations' impact on various protein properties, including thermal stability, antigen-antibody binding affinity, and the ability to polymerize non-natural nucleic acid or resilience to extreme alkaline conditions. Remarkably, over 30% of the AI-recommended mutants exhibited superior performance compared to their pre-mutation counterparts across all proteins and desired properties. Moreover, we have developed an efficient, and successful method based on PRIME to rapidly obtain multi-site mutants with enhanced activity and stability. Hence, PRIME demonstrates the general applicability in protein engineering.",,,Unverified,"China,China,China,China,China,China,China",,,,,,"Academia,Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia,Academia",,,,Hardware
DiffForce,Biology,Protein design,"University of Cambridge,Shanghai Jiao Tong University,University of New South Wales","Paulina Kulytė, Francisco Vargas, Simon Valentin Mathis, Yu Guang Wang, José Miguel Hernández-Lobato, Pietro Liò",2024-09-09,Improving Antibody Design with Force-Guided Sampling in Diffusion Models,https://arxiv.org/abs/2406.05832,1.00,,,,,,,,,,,,,,,,,,"Antibodies, crucial for immune defense, primarily rely on complementarity-determining regions (CDRs) to bind and neutralize antigens, such as viruses. The design of these CDRs determines the antibody's affinity and specificity towards its target. Generative models, particularly denoising diffusion probabilistic models (DDPMs), have shown potential to advance the structure-based design of CDR regions. However, only a limited dataset of bound antibody-antigen structures is available, and generalization to out-of-distribution interfaces remains a challenge. Physics based force-fields, which approximate atomic interactions, offer a coarse but universal source of information to better mold designs to target interfaces. Integrating this foundational information into diffusion models is, therefore, highly desirable. Here, we propose a novel approach to enhance the sampling process of diffusion models by integrating force field energy-based feedback. Our model, DiffForce, employs forces to guide the diffusion sampling process, effectively blending the two distributions. Through extensive experiments, we demonstrate that our method guides the model to sample CDRs with lower energy, enhancing both the structure and sequence of the generated antibodies.",,,Unverified,"United Kingdom of Great Britain and Northern Ireland,China,Australia",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
Precious3GPT,Biology,Drug discovery,"Insilico Medicine AI,Harvard Medical School","Fedor Galkin, Vladimir Naumov, Stefan Pushkov, Denis Sidorenko, Anatoly Urban, Diana Zagirova, Khadija M. Alawi, Alex Aliper, Ruslan Gumerov, Aleksandr Kalashnikov, Sabina Mukba, Aleksandra Pogorelskaya, Feng Ren, Anastasia Shneyderman, Qiuqiong Tang, Deyong Xiao, Alexander Tyshkovskiy, Kejun Ying, Vadim N. Gladyshev, Alex Zhavoronkov",2024-07-05,Precious3GPT: Multimodal Multi-Species Multi-Omics Multi-Tissue Transformer for Aging Research and Drug Discovery,https://www.biorxiv.org/content/10.1101/2024.07.25.605062v1.abstract,1.00,,,,,,,,,63377,"63,376 = 25,332 (genes) + 22,241 (compounds) + 13,439 (pathways) + 661 (mechanisms of action) + 635 (conditions) + 394 (age groups) + 300 (tissues) + 269 (cell lines)",,,,,,,,"We present a multimodal multi-species multi-omics multi-tissue transformer for aging research and drug discovery capable of performing multiple tasks such as age prediction across species, target discovery, tissue, sex, and disease sample classification, drug sensitivity prediction, replication of omics response and prediction of biological and phenotypic response to compound treatment. This model combines textual, tabular, and knowledge graph-derived representations of biological experiments to provide insights into molecular-level biological processes. We demonstrate that P3GPT has developed an intuition for the interactions between compounds, pathologies, and gene regulation in the context of multiple species and tissues. In these areas, it outperforms existing LLMs and we highlight its utility in diverse case studies. P3GPT is a general model that may be used as a target identification tool, aging clock, digital laboratory, and scientific assistant. The model is intended as a community resource available open source as well as via a Discord server.",,,Unverified,"Hong Kong,China,United States of America",,,,,,"Industry,Academia",,,"Industry,Academia",,,,
Chemistry42,Biology,Drug discovery,Insilico Medicine AI,"Yan A. Ivanenkov, Daniil Polykovskiy, Dmitry Bezrukov, Bogdan Zagribelnyy, Vladimir Aladinskiy, Petrina Kamya, Alex Aliper, Feng Ren, Alex Zhavoronkov",2023-02-02,Chemistry42: An AI-Driven Platform for Molecular Design and Optimization,https://pubs.acs.org/doi/full/10.1021/acs.jcim.2c01191,,,,,,,,,,,,,,,,,,,"Chemistry42 is a software platform for de novo small molecule design and optimization that integrates Artificial Intelligence (AI) techniques with computational and medicinal chemistry methodologies. Chemistry42 efficiently generates novel molecular structures with optimized properties validated in both in vitro and in vivo studies and is available through licensing or collaboration. Chemistry42 is the core component of Insilico Medicine’s Pharma.ai drug discovery suite. Pharma.ai also includes PandaOmics for target discovery and multiomics data analysis, and inClinico�a data-driven multimodal forecast of a clinical trial’s probability of success (PoS). In this paper, we demonstrate how the platform can be used to efficiently find novel molecular structures against DDR1 and CDK20",,,Unverified,"Hong Kong,China",,,,,,Industry,,,Industry,,,,
ProteinMPNN-DDG,Biology,Protein inverse folding,Peptone,"Oliver Dutton, Sandro Bottaro, Michele Invernizzi, Istvan Redl, Albert Chung, Falk Hoffmann, Louie Henderson, Stefano Ruschetta, Fabio Airoldi, Benjamin M J Owens, Patrik Foerch, Carlo Fisicaro, Kamil Tamiola",2024-09-09,Improving Inverse Folding models at Protein Stability Prediction without additional Training or Data,https://www.biorxiv.org/content/10.1101/2024.06.15.599145v4.abstract,0.00,,,,,,,,,5600001,"Total Residues = 5,615,050 = 5.6e6",,,,,,,,Deep learning protein sequence models have shown outstanding performance at de novo protein design and variant effect prediction. We substantially improve performance without further training or use of additional experimental data by introducing a second term derived from the models themselves which align outputs for the task of stability prediction. On a task to predict variants which increase protein stability the absolute success probabilities of ProteinMPNN and ESMif are improved by 11% and 5% respectively. We term these models ProteinMPNN-ddG and ESMif-ddG.,,,Unverified,United Kingdom of Great Britain and Northern Ireland,,,,,,Industry,,,Industry,,,,
ESMIF-DDG,Biology,Protein inverse folding,Peptone,"Oliver Dutton, Sandro Bottaro, Michele Invernizzi, Istvan Redl, Albert Chung, Falk Hoffmann, Louie Henderson, Stefano Ruschetta, Fabio Airoldi, Benjamin M J Owens, Patrik Foerch, Carlo Fisicaro, Kamil Tamiola",2024-09-09,Improving Inverse Folding models at Protein Stability Prediction without additional Training or Data,https://www.biorxiv.org/content/10.1101/2024.06.15.599145v4.abstract,0.00,,,,,,,,,5600001,"Total Residues = 5,615,050 = 5.6e6",,,,,,,,Deep learning protein sequence models have shown outstanding performance at de novo protein design and variant effect prediction. We substantially improve performance without further training or use of additional experimental data by introducing a second term derived from the models themselves which align outputs for the task of stability prediction. On a task to predict variants which increase protein stability the absolute success probabilities of ProteinMPNN and ESMif are improved by 11% and 5% respectively. We term these models ProteinMPNN-ddG and ESMif-ddG.,,,Unverified,United Kingdom of Great Britain and Northern Ireland,,,,,,Industry,,,Industry,,,,
PDFII,Biology,Protein function prediction,Sichuan University,"Wanyi Yang, Qingsong Du, Xunyu Zhou, Chuanfang Wu, Jinku Bao",2024-09-09,PDFll: Predictors of Disorder and Function of Proteins from the Language of Life,https://www.liebertpub.com/doi/abs/10.1089/cmb.2024.0506,0.00,,,,,,,,,,,,,,,,,,"The identification of intrinsically disordered proteins and their functional roles is largely dependent on the performance of computational predictors, necessitating a high standard of accuracy in these tools. In this context, we introduce a novel series of computational predictors, termed PDFll (Predictors of Disorder and Function of proteins from the Language of Life), which are designed to offer precise predictions of protein disorder and associated functional roles based on protein sequences. PDFll is developed through a two-step process. Initially, it leverages large-scale protein language models (pLMs), trained on an extensive dataset comprising billions of protein sequences. Subsequently, the embeddings derived from pLMs are integrated into streamlined, yet sophisticated, deep-learning models to generate predictions. These predictions notably surpass the performance of existing state-of-the-art predictors, particularly those that forecast disorder and function without utilizing evolutionary information.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
ESM-DBP,Biology,Protein or nucleotide language model (pLM/nLM),Hunan University,"Wenwu Zeng, Yutao Dou, Liangrui Pan, Liwen Xu, Shaoliang Peng ",2024-09-07,Improving prediction performance of general protein language model by domain-adaptive pretraining on DNA-binding protein,https://www.nature.com/articles/s41467-024-52293-7,0.00,,,650000000.00,"""so the ESM2 model with 650 million parameters is used to continue training here.""",,,UniProtKB,Subset of UniProtKB consisting of proteins which bind DNA.,170264,"""170,264 non-redundant DBP sequences (abbreviate as UniDBP40) are used as the pretraining data set.""",,72.0,"""The model is trained for approximately 51k steps and took about 3 days on four Tesla V100 GPUs with 16 G memory."" ",,4,,,"DNA-protein interactions exert the fundamental structure of many pivotal biological processes, such as DNA replication, transcription, and gene regulation. However, accurate and efficient computational methods for identifying these interactions are still lacking. In this study, we propose a method ESM-DBP through refining the DNA-binding protein sequence repertory and domain-adaptive pretraining based the general protein language model. Our method considers the lacking exploration of general language model for DNA-binding protein domain-specific knowledge, so we screen out 170,264 DNA-binding protein sequences to construct the domain-adaptive language model. Experimental results on four downstream tasks show that ESM-DBP provides a better feature representation of DNA-binding protein compared to the original language model, resulting in improved prediction performance and outperforming the state-of-the-art methods. Moreover, ESM-DBP can still perform well even for those sequences with only a few homologous sequences. ChIP-seq on two predicted cases further support the validity of the proposed method.",,,Unverified,China,ESM2-650M,13000000000000000000,"""The model is trained for approximately 51k steps and took about 3 days on four Tesla V100 GPUs with 16 G memory."" Assume FP16 precision and 40% utilization.",100,"""The batch size is set to 100.""",Academia,,,Academia,,,,
EpiScan,Biology,Antibody epitope prediction,"Sun Yat-sen University,Guangzhou National Laboratory","Chuan Wang, Jiangyuan Wang, Wenjun Song, Guanzheng Luo, Taijiao Jiang",2024-09-09,EpiScan: accurate high-throughput mapping of antibody-specific epitopes using sequence information ,https://www.nature.com/articles/s41540-024-00432-7,,,,288915.00,Loaded model from https://github.com/ gzBiomedical/EpiScan and counted parameters.,,,,,,,,,,NVIDIA GeForce RTX 2080 Ti,1,,,"The identification of antibody-specific epitopes on virus proteins is crucial for vaccine development and drug design. Nonetheless, traditional wet-lab approaches for the identification of epitopes are both costly and labor-intensive, underscoring the need for the development of efficient and costeffective computational tools. Here, EpiScan, an attention-based deep learning framework for predicting antibody-specific epitopes, is presented. EpiScan adopts a multi-input and single-output strategy by designing independent blocks for different parts of antibodies, including variable heavy chain (VH), variable light chain (VL), complementary determining regions (CDRs), and framework regions (FRs). The block predictions are weighted and integrated for the prediction of potential epitopes. Using multiple experimental data samples, we show that EpiScan, which only uses antibody sequence information, can accurately map epitopes on specific antigen structures. The antibodyspecific epitopes on the receptor binding domain (RBD) of SARS coronavirus 2 (SARS-CoV-2) were located by EpiScan, and the potentially valuable vaccine epitope was identified. EpiScan can expedite the epitope mapping process for high-throughput antibody sequencing data, supporting vaccine design and drug development. Availability: For the convenience of related wet-experimental researchers, the source code and web server of EpiScan are publicly available at https://github.com/ gzBiomedical/EpiScan.",,,Unverified,"China,China",,,,,,"Academia,Government",,,"Academia,Government",,,550.8217654598325,
VespaG,Biology,"Protein property prediction,Protein or nucleotide language model (pLM/nLM)","Technical University of Munich,Sorbonne University,Institute for Advanced Study,Université Paris Cité,Institut Universitaire de France (IUF)","Céline Marquet, Julius Schlensok, Marina Abakarova, Burkhard Rost, Elodie Laine",2024-09-09,Expert-guided protein Language Models enable accurate and blazingly fast fitness prediction,https://www.biorxiv.org/content/10.1101/2024.04.24.590982v2.abstract,,,,,,,,,,39000001,"Total datapoints = 39,000,000

39,000,000 = 3.9 x 10^7

Final data estimate: 3.9e7",,,,,,,,"Exhaustive experimental annotation of the effect of all known protein variants remains daunting and expensive, stressing the need for scalable effect predictions. We introduce VespaG, a blazingly fast missense amino acid variant effect predictor, leveraging protein Language Model (pLM) embeddings as input to a minimal deep learning model. To overcome the sparsity of experimental training data, we created a dataset of 39 million single amino acid variants from the human proteome applying the multiple sequence alignment-based effect predictor GEMME as a pseudo standard-of-truth. This setup increases interpretability compared to the baseline pLM and is easily retrainable with novel or updated pLMs. Assessed against the ProteinGym benchmark (217 multiplex assays of variant effect - MAVE - with 2.5 million variants), VespaG achieved a mean Spearman correlation of 0.48±0.02, matching top-performing methods evaluated on the same data. VespaG has the advantage of being orders of magnitude faster, predicting all mutational landscapes of all proteins in proteomes such as Homo sapiens or Drosophila melanogaster in under 30 minutes on a consumer laptop (12-core CPU, 16 GB RAM).",,,Unverified,"Germany,France,United States of America,France,France",,,,,,"Academia,Academia,Academia,Academia,Research collective",,,"Academia,Academia,Academia,Academia,Research collective",,,,
AlphaLink,Biology,Protein folding prediction,"Technische Universitat Berlin,Research Cluster of Excellence,University of Edinburgh","Kolja Stahl, Andrea Graziadei, Therese Dau, Oliver Brock, Juri Rappsilber",2023-04-20,Protein structure prediction with in-cell photo-crosslinking mass spectrometry and deep learning,https://www.nature.com/articles/s41587-023-01704-z,,,,93000000.00,"""To avoid training OpenFold from scratch, we start with the AlphaFold2 2.0 (https://github.com/deepmind/alphafold/releases/tag/v2.0.0) weights provided by Deepmind""",,,trRosetta,"""We train for five epochs on five GPUs, which takes roughly 5 days.""",13000,"refine the network on 13,000 proteins from the trRosetta38 training set with simulated photo-AA crosslinking data.",5.00,120.0,"""We train for five epochs on five GPUs, which takes roughly 5 days.""",,5,,,"While AlphaFold2 can predict accurate protein structures from the primary sequence, challenges remain for proteins that undergo conformational changes or for which few homologous sequences are known. Here we introduce AlphaLink, a modified version of the AlphaFold2 algorithm that incorporates experimental distance restraint information into its network architecture. By employing sparse experimental contacts as anchor points, AlphaLink improves on the performance of AlphaFold2 in predicting challenging targets. We confirm this experimentally by using the noncanonical amino acid photo-leucine to obtain information on residueresidue contacts inside cells by crosslinking mass spectrometry. The program can predict distinct conformations of proteins on the basis of the distance restraints provided, demonstrating the value of experimental data in driving protein structure prediction. The noise-tolerant framework for integrating data in protein structure prediction presented here opens a path to accurate characterization of protein structures from in-cell data.",,,Unverified,"Germany,Germany,United Kingdom of Great Britain and Northern Ireland",AlphaFold 2,,,,,"Research collective,Academia",,,"Research collective,Academia",,,,
MMAPLE,Biology,Protein-ligand binding affinity prediction,"City University of New York,Cornell University","You Wu, Li Xie, Yang Liu, Lei Xie",2024-09-09,Semi-supervised meta-learning elucidates understudied molecular interactions,https://www.nature.com/articles/s42003-024-06797-z,,,,,,,,,,,,,,,,,,,"Many biological problems are understudied due to experimental limitations and human biases. Although deep learning is promising in accelerating scientific discovery, its power compromises when applied to problems with scarcely labeled data and data distribution shifts. We develop a deep learning framework—Meta Model Agnostic Pseudo Label Learning (MMAPLE)—to address these challenges by effectively exploring out-of-distribution (OOD) unlabeled data when conventional transfer learning fails. The uniqueness of MMAPLE is to integrate the concept of meta-learning, transfer learning and semi-supervised learning into a unified framework. The power of MMAPLE is demonstrated in three applications in an OOD setting where chemicals or proteins in unseen data are dramatically different from those in training data: predicting drug-target interactions, hidden human metabolite-enzyme interactions, and understudied interspecies microbiome metabolite-human receptor interactions. MMAPLE achieves 11% to 242% improvement in the prediction-recall on multiple OOD benchmarks over various base models. Using MMAPLE, we reveal novel interspecies metabolite-protein interactions that are validated by activity assays and fill in missing links in microbiome-human interactions. MMAPLE is a general framework to explore previously unrecognized biological domains beyond the reach of present experimental and computational techniques.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
KinoML,Biology,Drug discovery,"Charité-Universitätsmedizin Berlin,Saarland University,Memorial Sloan Kettering Cancer Center","Raquel López-Ríos de Castro, Jaime Rodríguez-Guerra, David Schaller, Talia B Kimber, Corey Taylor, Jessica B White, Michael Backenköhler, Alexander Payne, Ben Kaminow, Iván Pulido, Sukrit Singh, Paula Linh Kramer, Guillermo Pérez-Hernández, Andrea Volkamer, John D Chodera",2024-09-10,"Lessons learned during the journey of data: from experiment to model for predicting kinase affinity, selectivity, polypharmacology, and resistance",https://pmc.ncbi.nlm.nih.gov/articles/PMC11419124/,,,,,,,,,,617001,"Total Datapoints = KLIFS + KinCo + ChEMBL + PKIS2
= 6,667 + 137,778 + (159,823 + 15,578 + 11,412) + 261,870
= 6,667 + 137,778 + 186,813 + 261,870
= 592,128 ≈ 6.17 × 10⁵",,,,,,,,"Recent advances in machine learning (ML) are reshaping drug discovery. Structure-based ML methods use physically-inspired models to predict binding affinities from protein:ligand complexes. These methods promise to enable the integration of data for many related targets, which addresses issues related to data scarcity for single targets and could enable generalizable predictions for a broad range of targets, including mutants. In this work, we report our experiences in building KinoML, a novel framework for ML in target-based small molecule drug discovery with an emphasis on structure-enabled methods. KinoML focuses currently on kinases as the relative structural conservation of this protein superfamily, particularly in the kinase domain, means it is possible to leverage data from the entire superfamily to make structure-informed predictions about binding affinities, selectivities, and drug resistance. Some key lessons learned in building KinoML include: the importance of reproducible data collection and deposition, the harmonization of molecular data and featurization, and the choice of the right data format to ensure reusability and reproducibility of ML models. As a result, KinoML allows users to easily achieve three tasks: accessing and curating molecular data; featurizing this data with representations suitable for ML applications; and running reproducible ML experiments that require access to ligand, protein, and assay information to predict ligand affinity. Despite KinoML focusing on kinases, this framework can be applied to other proteins. The lessons reported here can help guide the development of platforms for structure-enabled ML in other areas of drug discovery.

",,,Unverified,"Germany,Germany,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
CPDiffusion,Biology,Protein design,"Shanghai Jiao Tong University,University of New South Wales,University of Cambridge,Shanghai AI Lab","Bingxin Zhou, Lirong Zheng, Banghao Wu, Kai Yi, Bozitao Zhong, Yang Tan, Qian Liu, Pietro Liò, Liang Hong",2024-09-10,A conditional protein diffusion model generates artificial programmable endonuclease sequences with enhanced activity ,https://www.nature.com/articles/s41421-024-00728-2,1.00,,,4000000.00,"CPDiffusion trains a denoising diffusion model with 4 million
learnable parameters from natural protein structures. ",,,,,5600001,"CATH: 20,000 × 250 = 5,000,000
pAgo: 694 × 800 = 555,200
Total: 5,000,000 + 555,200 = 5,555,200 ≈ 5.6 × 10^6 datapoints",,,,,,,,"Deep learning-based methods for generating functional proteins address the growing need for novel biocatalysts, allowing for precise tailoring of functionalities to meet specific requirements. This advancement leads to the development of highly efficient and specialized proteins with diverse applications across scientific, technological, and biomedical fields. This study establishes a pipeline for protein sequence generation with a conditional protein diffusion model, namely CPDiffusion, to create diverse sequences of proteins with enhanced functions. CPDiffusion accommodates protein-specific conditions, such as secondary structures and highly conserved amino acids. Without relying on extensive training data, CPDiffusion effectively captures highly conserved residues and sequence features for specific protein families. We applied CPDiffusion to generate artificial sequences of Argonaute (Ago) proteins based on the backbone structures of wild-type (WT) Kurthia massiliensis Ago (KmAgo) and Pyrococcus furiosus Ago (PfAgo), which are complex multi-domain programmable endonucleases. The generated sequences deviate by up to nearly 400 amino acids from their WT templates. Experimental tests demonstrated that the majority of the generated proteins for both KmAgo and PfAgo show unambiguous activity in DNA cleavage, with many of them exhibiting superior activity as compared to the WT. These findings underscore CPDiffusion’s remarkable success rate in generating novel sequences for proteins with complex structures and functions in a single step, leading to enhanced activity. This approach facilitates the design of enzymes with multi-domain molecular structures and intricate functions through in silico generation and screening, all accomplished without the need for supervision from labeled data.",,,Unverified,"China,Australia,United Kingdom of Great Britain and Northern Ireland,China",,,,,,"Academia,Academia,Academia,Academia",,,"Academia,Academia,Academia,Academia",,,,
IEV2MOL,Biology,Drug discovery,Tokyo Institute of Technology,"Mami Ozawa, Shogo Nakamura, Nobuaki Yasuo, Masakazu Sekijima",2024-10-10,IEV2Mol: Molecular Generative Model Considering Protein-Ligand Interaction Energy Vectors,https://pubs.acs.org/doi/full/10.1021/acs.jcim.4c00842,,,,,,100000000000000000000.00,"1. Hardware: 1x RTX 4090 (3.30×10¹⁴ FLOP/s) + 4x TESLA P100 (1.5×10¹⁴ FLOP/s each)
2. Training duration: 2 days (172,800s) - estimated based on training phases
3. Utilization: 40%
4. Calculation: Total hardware (9.3×10¹⁴ FLOP/s) × Utilization (0.4) × Duration (172,800s) = 6.43×10¹⁹ FLOPs (rounded to 1.0×10²⁰)",,,2960001,"Main dataset: 981,139 compounds × 3 proteins = 2,943,417 points
Additional datasets: 8,350 + 6,640 + 3,576 = 18,566 points
Total: 2,943,417 + 18,566 = 2,961,983 points ≈ 2.96e6",,,,,,,,"Generating drug candidates with desired protein–ligand interactions is a significant challenge in structure-based drug design. In this study, a new generative model, IEV2Mol, is proposed that incorporates interaction energy vectors (IEVs) between proteins and ligands obtained from docking simulations, which quantitatively capture the strength of each interaction type, such as hydrogen bonds, electrostatic interactions, and van der Waals forces. By integrating this IEV into an end-to-end variational autoencoder (VAE) framework that learns the chemical space from SMILES and minimizes the reconstruction error of the SMILES, the model can more accurately generate compounds with the desired interactions. To evaluate the effectiveness of IEV2Mol, we performed benchmark comparisons with randomly selected compounds, unconstrained VAE models (JT-VAE), and compounds generated by RNN models based on interaction fingerprints (IFP-RNN). The results show that the compounds generated by IEV2Mol retain a significantly greater percentage of the binding mode of the query structure than those of the other methods. Furthermore, IEV2Mol was able to generate compounds with interactions similar to those of the input compounds, regardless of structural similarity. The source code and trained models for IEV2Mol, JT-VAE, and IFP-RNN designed for generating compounds active against the DRD2, AA2AR, and AKT1, as well as the data sets (DM-QP-1M, active compounds to each protein, and ChEMBL33) utilized in this study, are released under the MIT License and available at https://github.com/sekijima-lab/IEV2Mol.",,,Unverified,Japan,,,,,,Academia,,,Academia,,,,Hardware
AbGPT,Biology,Protein design,Carnegie Mellon University (CMU),"Desmond Kuan, Amir Barati Farimani",2024-09-09,AbGPT: De Novo Antibody Design via Generative Language Modeling,https://arxiv.org/abs/2409.06090,,,,,,10000000000000000000.00,"1. Hardware setup: 1x NVIDIA RTX A6000 GPU (3.87 × 10¹³ FLOP/s)

2. Training duration: Estimated 20.8 days (calculated from total FLOPs divided by effective FLOP/s)

3. Utilization rate: 40%

4. Final calculation:
   * Steps: (57M/1024) × 5 epochs = 278,515 steps
   * 1 × 10¹⁴ FLOPs/step × 278,515 steps = 2.785 × 10¹⁹ total FLOPs",,,3200000001,"Sequences: 50M + 57M = 107M total sequences
Per sequence: 120 amino acids average
Total amino acids: 107M * 120 = 1.284 × 10¹⁰
Tokens (4 amino acids each): 1.284 × 10¹⁰ ÷ 4 = 3.2 × 10⁹
Final estimate: 3.2 × 10⁹ datapoints",,,,,,,,"The adaptive immune response, largely mediated by B-cell receptors (BCRs), plays a crucial role for effective pathogen neutralization due to its diversity and antigen specificity. Designing BCRs de novo, or from scratch, has been challenging because of their complex structure and diverse binding requirements. Protein language models (PLMs) have shown remarkable performance in contextualizing and performing various downstream tasks without relying on structural information. However, these models often lack a comprehensive understanding of the entire protein space, which limits their application in antibody design. In this study, we introduce Antibody Generative Pretrained Transformer (AbGPT), a model fine-tuned from a foundational PLM to enable a more informed design of BCR sequences. Using a custom generation and filtering pipeline, AbGPT successfully generated a high-quality library of 15,000 BCR sequences, demonstrating a strong understanding of the intrinsic variability and conserved regions within the antibody repertoire. https://github.com/deskk/AbGPT",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
MPDF,Biology,Drug discovery,"Chinese University of Hong Kong (CUHK),Lanzhou University,Zhejiang Lab,Zhejiang University","Chunbin Gu, Mutian He, Hanqun Cao, Guangyong Chen, Chang-yu Hsieh, Pheng Ann Heng",2024-09-07,Unlocking Potential Binders: Multimodal Pretraining DEL-Fusion for Denoising DNA-Encoded Libraries,https://arxiv.org/abs/2409.05916,2.00,,,,,1400000000000012000.00,"1. Hardware: 1x NVIDIA GeForce RTX 3090, 1.60 x 10^14 FLOP/s (FP16)

2. Training duration: 6 hours (max of 1-6 hour range) = 21,600 seconds

3. Utilization: 40%

4. Calculation: 
1.60 x 10^14 FLOP/s × 1 GPU × 21,600s × 0.4 = 1.38 x 10^18 FLOPs",,,,,,,"""with training times ranging from 1 to 6 hours, depending on the complexity and size of the different building blocks.""",NVIDIA GeForce RTX 3090,1,,,"In the realm of drug discovery, DNA-encoded library (DEL) screening technology has emerged as an efficient method for identifying high-affinity compounds. However, DEL screening faces a significant challenge: noise arising from nonspecific interactions within complex biological systems. Neural networks trained on DEL libraries have been employed to extract compound features, aiming to denoise the data and uncover potential binders to the desired therapeutic target. Nevertheless, the inherent structure of DEL, constrained by the limited diversity of building blocks, impacts the performance of compound encoders. Moreover, existing methods only capture compound features at a single level, further limiting the effectiveness of the denoising strategy. To mitigate these issues, we propose a Multimodal Pretraining DEL-Fusion model (MPDF) that enhances encoder capabilities through pretraining and integrates compound features across various scales. We develop pretraining tasks applying contrastive objectives between different compound representations and their text descriptions, enhancing the compound encoders' ability to acquire generic features. Furthermore, we propose a novel DEL-fusion framework that amalgamates compound information at the atomic, submolecular, and molecular levels, as captured by various compound encoders. The synergy of these innovations equips MPDF with enriched, multi-scale features, enabling comprehensive downstream denoising. Evaluated on three DEL datasets, MPDF demonstrates superior performance in data processing and analysis for validation tasks. Notably, MPDF offers novel insights into identifying high-affinity molecules, paving the way for improved DEL utility in drug discovery.",,,Unverified,"Hong Kong,China,China,China,China",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,771.1646664431432,Hardware
"PocketVec
",Biology,Drug discovery,"Barcelona Institute of Science and Technology,Universitat de Barcelona,Institució Catalana de Recerca i Estudis Avançats (ICREA)","Arnau Comajuncosa-Creus, Guillem Jorba, Xavier Barril, Patrick Aloy",2024-03-16,Comprehensive detection and characterization of human druggable pockets through binding site descriptors,https://www.biorxiv.org/content/10.1101/2024.03.14.584971v1.abstract,,,,,,,,,,1,"0.0

Final calculation: 0.0
(No additions or multiplications performed as there is no training data involved)",,,,,,,,"Druggable pockets are protein regions that have the ability to bind organic small molecules, and their characterization is essential in target-based drug discovery. However, strategies to derive pocket descriptors are scarce and usually exhibit limited applicability. Here, we present PocketVec, a novel approach to generate pocket descriptors for any protein binding site of interest through the inverse virtual screening of lead-like molecules. We assess the performance of our descriptors in a variety of scenarios, showing that it is on par with the best available methodologies, while overcoming some important limitations. In parallel, we systematically search for druggable pockets in the folded human proteome, using experimentally determined protein structures and AlphaFold2 models, identifying over 32,000 binding sites in more than 20,000 protein domains. Finally, we derive PocketVec descriptors for each small molecule binding site and run an all-against-all similarity search, exploring over 1.2 billion pairwise comparisons. We show how PocketVec descriptors facilitate the identification of druggable pocket similarities not revealed by structure- or sequence-based comparisons. Indeed, our analyses unveil dense clusters of similar pockets in distinct proteins for which no inhibitor has yet been crystalized, opening the door to strategies to prioritize the development of chemical probes to cover the druggable space.

",,,Unverified,"Spain,Spain,Spain",,,,,,Academia,,,Academia,,,,
EITLEM-Kinetics,Biology,Mutation prediction,Beijing University of Chemical Technology,"Xiaowei Shen, Ziheng Cui, Jianyu Long, Shiding Zhang, Biqiang Chen, Tianwei Tan",2024-09-19,EITLEM-Kinetics: A Deep-Learning Framework for Kinetic Parameter Prediction of Mutant Enzymes,https://www.cell.com/chem-catalysis/abstract/S2667-1093(24)00266-5,,,,,,,,,,61185,"34,429 × 0.8 = 27,543
28,664 × 0.8 = 22,931
13,388 × 0.8 = 10,710
27,543 + 22,931 + 10,710 = 61,184",,,,,,,,"The core issue in implementing in silico enzyme screening lies in accurately evaluating the merits of mutants. The best solution to this problem would undoubtedly be the precise prediction of kinetic parameters for mutant enzymes to directly assess the catalytic efficiency and activity of enzymes. Previously developed models of this type are mostly limited to predictions for wild-type enzymes and tend to exhibit poorer generalization capabilities. Here, a novel deep-learning model framework and an ensemble iterative transfer learning strategy for enzyme mutant kinetics parameter (kcat, Km, and KKm) prediction (EITLEM-Kinetics) were developed. This approach is designed to overcome the limitations imposed by sparse training samples on the model’s predictive performance and accurately predict the kinetic parameters of various mutants. This development is set to provide significant assistance in future endeavors to construct virtual screening methods aimed at enhancing enzyme activity and offer innovative solutions for researchers grappling with similar challenges.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
Automated design of multi-target ligands by generative deep learning,Biology,Drug discovery,"Goethe University Frankfurt,Ludwig-Maximilians-Universität München,Fraunhofer Institute for Translational Medicine and Pharmacology","Laura Isigkeit, Tim Hörmann, Espen Schallmayer, Katharina Scholz, Felix F. Lillich, Johanna H. M. Ehrler, Benedikt Hufnagel, Jasmin Büchner, Julian A. Marschner, Jörg Pabel, Ewgenij Proschak, Daniel Merk",2024-09-11,Automated design of multi-target ligands by generative deep learning,https://www.nature.com/articles/s41467-024-52060-8,,,,5820515.00,"""The model was based on a recurrent neural network with long short-term memory (LSTM) cells and consisted of four layers with a total of 5,820,515
parameters: layer 1, BatchNormalization; layer 2, LSTM with 1024 units;
layer 3, LSTM with 256 units; layer 4, BatchNormalization.""",,,,,510000001,"365,000 molecules × 10 augmentations = 3,650,000 total molecules
3,650,000 molecules × 140 tokens/molecule = 511,000,000 (5.11 × 10⁸) tokens
Final estimate: 5.1e8 tokens",,,,,,,,"Generative deep learning models enable data-driven de novo design of molecules with tailored features. Chemical language models (CLM) trained on string representations of molecules such as SMILES have been successfully employed to design new chemical entities with experimentally confirmed activity on intended targets. Here, we probe the application of CLM to generate multi-target ligands for designed polypharmacology. We capitalize on the ability of CLM to learn from small fine-tuning sets of molecules and successfully bias the model towards designing drug-like molecules with similarity to known ligands of target pairs of interest. Designs obtained from CLM after pooled fine-tuning are predicted active on both proteins of interest and comprise pharmacophore elements of ligands for both targets in one molecule. Synthesis and testing of twelve computationally favored CLM designs for six target pairs reveals modulation of at least one intended protein by all selected designs with up to double-digit nanomolar potency and confirms seven compounds as designed dual ligands. These results corroborate CLM for multi-target de novo design as source of innovation in drug discovery.",,,Unverified,"Germany,Germany,Germany",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment,Biology,Protein folding prediction,"Fudan University,Shanghai Academy of Artificial Intelligence for Science,Nanjing University","Kaihui Cheng, Ce Liu, Qingkun Su, Jun Wang, Liwei Zhang, Yining Tang, Yao Yao, Siyu Zhu, Yuan Qi",2024-09-12,"4D Diffusion for Dynamic Protein Structure Prediction with Reference Guided Motion Alignment
",https://arxiv.org/abs/2408.12419,,,,,,13000000000000110000.00,"1. Hardware: NVIDIA A100 GPU (3.12 x 10^14 FP16 FLOP/s)

2. Training duration (estimated):
   - 764 proteins, batch size 4, 550 epochs
   - 191 batches/epoch × 550 epochs × 1 sec/step ≈ 29.2 hours

3. Utilization: 40%

4. Final calculation:
   3.12 × 10^14 FLOP/s × 0.4 × (105,050 seconds) = 1.31 × 10^19 FLOPs",,,4900001,"Dataset 1: 758 proteins × 200 residues/protein = 151,600 residues
Dataset 2: 6 proteins × 256 residues/protein = 1,536 residues
Total residues: 151,600 + 1,536 = 153,136
Final datapoints: 153,136 residues × 32 time steps = 4,899,552",,,,NVIDIA A100,1,,,"Protein structure prediction is pivotal for understanding the structure-function relationship of proteins, advancing biological research, and facilitating pharmaceutical development and experimental design. While deep learning methods and the expanded availability of experimental 3D protein structures have accelerated structure prediction, the dynamic nature of protein structures has received limited attention. This study introduces an innovative 4D diffusion model incorporating molecular dynamics (MD) simulation data to learn dynamic protein structures. Our approach is distinguished by the following components: (1) a unified diffusion model capable of generating dynamic protein structures, including both the backbone and side chains, utilizing atomic grouping and side-chain dihedral angle predictions; (2) a reference network that enhances structural consistency by integrating the latent embeddings of the initial 3D protein structures; and (3) a motion alignment module aimed at improving temporal structural coherence across multiple time steps. To our knowledge, this is the first diffusion-based model aimed at predicting protein trajectories across multiple time steps simultaneously. Validation on benchmark datasets demonstrates that our model exhibits high accuracy in predicting dynamic 3D structures of proteins containing up to 256 amino acids over 32 time steps, effectively capturing both local flexibility in stable states and significant conformational changes.",,,Unverified,"China,China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,881.2904978153572,Hardware
IDPFold,Biology,Protein folding prediction,"Shandong University,BioMap Research,Fuzhou University,Shanghai Jiao Tong University","Junjie Zhu, Zhengxin Li, Zhuoqi Zheng, Bo Zhang, Bozitao Zhong, Jie Bai, Xiaokun Hong, Taifeng Wang, Ting Wei, Jianyi Yang, Hai-Feng Chen",2024-09-13,Precise Generation of Conformational Ensembles for Intrinsically Disordered Proteins via Fine-tuned Diffusion Models,https://www.biorxiv.org/content/10.1101/2024.05.05.592611.abstract,,,,17800000.00,Taken from Table 2,259999999999998000000.00,"1. Hardware setup: 1x NVIDIA A100 GPU with 3.12×10¹⁴ FLOP/s (fp16_tensor)

2. Training duration: Directly provided as 24 GPU days (9 days initial + 15 days second phase) = 2.0736×10⁶ seconds

3. Utilization rate: 40%

4. Final calculation:
3.12×10¹⁴ FLOP/s × 1 GPU × 2.0736×10⁶ s × 0.4 = 2.6×10²⁰ FLOP",,,7500001,"Initial Phase: 25,495 sequences × 200 residues = 5,099,000 points
Second Phase: 3,880 sequences × 300 residues = 1,164,000 points
Total: 5,099,000 + 1,164,000 = 6,263,000 points
Rounded: 7.5 million data points",,,,NVIDIA A100,,,,"Intrinsically disordered proteins (IDPs) play pivotal roles in various biological functions and are closely linked to many human diseases including cancer, diabetes and Alzheimer disease. Structural investigations of IDPs typically involve a combination of molecular dynamics (MD) simulations and experimental data to correct for intrinsic biases in simulation methods. However, these simulations are hindered by their high computational cost and a scarcity of experimental data, severely limiting their applicability. Despite the recent advancements in structure prediction for structured proteins, understanding the conformational properties of IDPs remains challenging partly due to the poor conservation of disordered protein sequences and limited experimental characterization. Here, we introduce IDPFold, a method capable of generating conformational ensembles for IDPs directly from their sequences using fine-tuned diffusion models. IDPFold bypasses the need for Multiple Sequence Alignments (MSA) or experimental data, achieving accurate predictions of ensemble properties across numerous IDPs. By sampling conformations at the backbone level, IDPFold provides more detailed structural features and more precise property estimation compared to other state-of-the-art methods. IDPFold is ready to be used in the elucidate the sequence-disorder-function paradigm of IDPs.",,,Unverified,"China,China,China,China",,,,,,"Academia,Industry,Academia,Academia",,,"Academia,Industry,Academia,Academia",,,,Hardware
Novae,Biology,Spatial Transcriptomics,"CentraleSupelec,Gustave Roussy,Université Paris Cité","Quentin Blampey, Hakim Benkirane, Nadège Bercovici, Fabrice André, Paul-Henry Cournède",2024-09-13,Novae: a graph-based foundation model for spatial transcriptomics data,https://www.biorxiv.org/content/10.1101/2024.09.09.612009v1.abstract,,,,,,11000000000000000000.00,"""Novae was trained on a Nvidia HGX A100 GPU for 24 hours."" Assume FP16 tensor precision and 40% utilization. ",,,30000000,"""This allowed us  169 to train Novae on a dataset composed of nearly 30 million cells using a GPU with 40GB of RAM (see  170 subsection 4.14 for more details).""",,,,,,,,"Spatial transcriptomics is advancing molecular biology by providing high-resolution insights into gene expression within the spatial context of tissues. This context is essential for identifying spatial domains, enabling the understanding of micro-environment organizations and their implications for tissue function and disease progression. To improve current model limitations on multiple slides, we have designed Novae (https://github.com/MICS-Lab/novae), a graph-based foundation model that extracts representations of cells within their spatial contexts. Our model was trained on a large dataset of nearly 30 million cells across 18 tissues, allowing Novae to perform zero-shot domain inference across multiple gene panels, tissues, and technologies. Unlike other models, it also natively corrects batch effects and constructs a nested hierarchy of spatial domains. Furthermore, Novae supports various downstream tasks, including spatially variable gene or pathway analysis and spatial domain trajectory analysis. Overall, Novae provides a robust and versatile tool for advancing spatial transcriptomics and its applications in biomedical research.",,,Unverified,"France,France,France",,,,,,Academia,,,Academia,,,,Hardware
CodonTransformer,Biology,Codon optimization,"Vector Institute,University of Toronto,Université Paris Cité","Adibvafa Fallahpour, Vincent Gureghian, Guillaume J. Filion, Ariel B. Lindner, Amir Pandi",2024-09-13,CodonTransformer: a multispecies codon optimizer using context-aware neural networks,https://www.biorxiv.org/content/10.1101/2024.09.13.612903v1.abstract,,,,89600000.00,"""total number of parameters to 89.6 million.""",10000000000000000000.00,"1. Hardware setup:
- Pretraining: 16x NVIDIA V100 GPUs @ 1.25e14 FLOP/s per GPU
- Fine-tuning: 4x NVIDIA V100 GPUs @ 1.25e14 FLOP/s per GPU

2. Training duration: Estimated 1.28 hours based on FLOPs calculation and hardware setup

3. Utilization rate: 40%

4. Final calculation:
Total FLOPs = Pretraining (8.96e7 params × 1.02e10 tokens × 100 = 9.14e17) + Fine-tuning (8.96e7 params × 3.07e9 tokens × 100 = 2.75e18) = 3.66e18 FLOPs",,,1000000001,"1,001,197 sequences × 1,000 tokens/sequence = 1.001197 × 10⁹ tokens ≈ 1.0 × 10⁹ tokens",5.00,,,NVIDIA V100,16,,,"The genetic code is degenerate allowing a multitude of possible DNA sequences to encode the same protein. This degeneracy impacts the efficiency of heterologous protein production due to the codon usage preferences of each organism. The process of tailoring organism-specific synonymous codons, known as codon optimization, must respect local sequence patterns that go beyond global codon preferences. As a result, the search space faces a combinatorial explosion that makes exhaustive exploration impossible. Nevertheless, throughout the diverse life on Earth, natural selection has already optimized the sequences, thereby providing a rich source of data allowing machine learning algorithms to explore the underlying rules. Here, we introduce CodonTransformer, a multispecies deep learning model trained on over 1 million DNA-protein pairs from 164 organisms spanning all kingdoms of life. The model demonstrates context-awareness thanks to the attention mechanism and bidirectionality of the Transformers we used, and to a novel sequence representation that combines organism, amino acid, and codon encodings. CodonTransformer generates host-specific DNA sequences with natural-like codon distribution profiles and with negative cis-regulatory elements. This work introduces a novel strategy of Shared Token Representation and Encoding with Aligned Multi-masking (STREAM) and provides a state-of-the-art codon optimization framework with a customizable open-access model and a user-friendly interface.",,,Unverified,"Canada,Canada,France",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,10575.388688566123,
Text2Protein,Biology,Protein design,"University of California San Diego,Brown University","Ramtin Hosseini, Siyang Zhang, Pengtao Xie",2024-09-13,Text2Protein: A Generative Model for Designated Protein Design on Given Description,https://www.researchsquare.com/article/rs-4868665/v1,,,,,,55000000000000030000.00,"1. Hardware setup: 1x NVIDIA RTX 3090 (1.60e14 FLOP/s FP16 tensor)

2. Training duration: 10 days provided directly (864,000 seconds)

3. Utilization rate: 40%

4. Calculation: 1.60e14 FLOP/s × 1 GPU × 864,000s × 0.4 = 5.5e19 FLOPs",,,1000001,"Training set: 10,898 * 0.95 = 10,353 proteins
Average residues per protein: 100
Total datapoints = 10,353 * 100 = 1,035,300

Range validation:
Minimum (40 residues): 10,353 * 40 = 414,120
Maximum (256 residues): 10,353 * 256 = 2,648,768

Final estimate: ~1.0e6 datapoints",,240.0,"""We trained the diffusion model for approximately 500,000 iterations with a batch size of 248. The entire training process, conducted on an NVIDIA RTX 3090 GPU, took about 10 days to complete.""",NVIDIA GeForce RTX 3090,,,,"Designing protein structures from text is challenging in computational biology. We propose Text2Protein, a pipeline combining large language models (LLMs) with diffusion models to generate full-atomic protein structures from text. Using a conditional diffusion model and the Vicuna-7B language model, we learn data distributions of 6D interresidue coordinates, refined into full-atomic structures with PyRosetta. Trained on a curated RCSB-PDB dataset, Text2Protein focuses on single-chain proteins with 40-256 residues. Our extensive experiments validate Text2Protein’s effectiveness by generating high-fidelity protein structures similar to ground truth proteins using raw texts. We evaluate Text2Protein using multiple metrics, including Mean Square Error (MSE) of 6D coordinates, Rosetta Energy Units (REU), and TM-score. Our results show that 5% of the generated proteins have a TM-score greater than 0.5, indicating similar folds in SCOP/CATH. Additionally, 16% of pairs have a TM-score greater than 0.4, 89% have a TM-score greater than 0.3, and none have a TM-score less than 0.17, below the threshold for unrelated proteins. Text2Protein presents a promising framework for automated protein design, potentially accelerating novel protein discovery. This work opens new avenues for integrating natural language understanding with protein structure generation, with implications in drug discovery, enzyme engineering, and material science.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
PLTNUM,Biology,Protein property prediction,"Kyoto University,National Institute of Biomedical Innovation,RIKEN","Tatsuya Sagawa, Eisuke Kanao, Kosuke Ogata, Koshi Imami, Yasushi Ishihama",2024-09-14,Prediction of Protein Half-lives from Amino Acid Sequences by Protein Language Models,https://www.biorxiv.org/content/10.1101/2024.09.10.612367v1.abstract,,,,,,,,,,2100001,"Total Datapoints = Number of Proteins × Sequence Length
Total Datapoints = 4,162 × 510 = 2,122,620 tokens
Final result ≈ 2.1 × 10^6 tokens",,,,,,,,"We developed a protein half-life prediction model, PLTNUM, based on a protein language model using an extensive dataset of protein sequences and protein half-lives from the NIH3T3 mouse embryo fibroblast cell line as a training set. PLTNUM achieved an accuracy of 71% on validation data and showed robust performance with an ROC of 0.73 when applied to a human cell line dataset. By incorporating Shapley Additive Explanations (SHAP) into PLTNUM, we identified key factors contributing to shorter protein half-lives, such as cysteine-containing domains and intrinsically disordered regions. Using SHAP values, PLTNUM can also predict potential degron sequences that shorten protein half-lives. This model provides a platform for elucidating the sequence dependency of protein half-lives, while the uncertainty in predictions underscores the importance of biological context in influencing protein half-lives.

",,,Unverified,"Japan,Japan,Japan",,,,,,"Academia,Government",,,"Academia,Government",,,,
LEGO,Biology,Protein-ligand binding affinity prediction,"Chinese Academy of Sciences,Beijing Academy of Artificial Intelligence / BAAI,University of Chinese Academy of Sciences","Yuancheng Sun, Kai Chen, Kang Liu, Qiwei Ye",2024-09-14,3D Molecular Pretraining via Localized Geometric Generation,https://www.biorxiv.org/content/10.1101/2024.09.10.612249v1,,,,,,,,,,,,,,,,,,,"Self-supervised learning on 3D molecular structures is gaining importance in data-driven scientific research and applications due to the high costs of annotating bio-chemical data. However, the strategic selection of semantic units for modeling 3D molecular structures remains underexplored, despite its crucial role in effective pre-training—a concept well-established in language processing and computer vision. We introduce Localized Geometric Generation (LEGO), a novel approach that treats tetrahedrons within 3D molecular structures as fundamental building blocks, leveraging their geometric simplicity and widespread presence across chemical functional patterns. Inspired by masked modeling, LEGO perturbs tetrahedral local structures and learns to reconstruct them in a self-supervised manner. Experimental results demonstrate LEGO consistently enhances molecular representations across biochemistry and quantum property prediction benchmarks. Additionally, the tetrahedral modeling and pretraining generalize from small molecules to larger molecular systems, validating by protein-ligand affinity prediction. Our results highlight the potential of selecting semantic units to build more expressive and interpretable neural networks for scientific AI applications.",,,Unverified,"China,China,China",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
MolSnapper,Biology,Drug discovery,University of Oxford,"Yael Ziv, Brian Marsden, Charlotte M. Deane",2024-09-14,MolSnapper: Conditioning Diffusion for Structure Based Drug Design ,https://www.biorxiv.org/content/10.1101/2024.03.28.586278v2.abstract,,,,,,,,,,140001,"CrossDocked2020 (100,000) + Binding MOAD (40,344) = 140,344 total data points

100,000 + 40,344 = 140,344 ≈ 1.4e5",,,,,,,,"Generative models have emerged as potentially powerful methods for molecular design, yet challenges persist in generating molecules that effectively bind to the intended target. The ability to control the design process and incorporate prior knowledge would be highly beneficial for better tailoring molecules to fit specific binding sites. In this paper, we introduce MolSnapper, a novel tool that is able to condition diffusion models for structure-based drug design by seamlessly integrating expert knowledge in the form of 3D pharmacophores. We demonstrate through comprehensive testing on both the CrossDocked and Binding MOAD datasets, that our method generates molecules better tailored to fit a given binding site, achieving high structural and chemical similarity to the original molecules. It also, when compared to alternative methods, yields approximately twice as many valid molecules.",,,Unverified,United Kingdom of Great Britain and Northern Ireland,,,,,,Academia,,,Academia,,,,
UdanDTI,Biology,"Protein-ligand contact prediction,Drug discovery",Tsinghua University,"Pei-Dong Zhang, Jianzhu Ma, Ting Chen",2024-09-15,Escaping the Drug-Bias Trap: Using Debiasing Design to Improve Interpretability and Generalization of Drug-Target Interaction Prediction,https://www.biorxiv.org/content/10.1101/2024.09.12.612771v1.abstract,,,,,,3500000000000028700.00,"1. Hardware setup: 1x NVIDIA A40 Tensor Core GPU (1.50×10¹⁴ FLOP/s)

2. Training duration: Estimated 16.4 hours (59,000 seconds) - revised from initial calculation of 11,641.67 hours which was deemed unrealistic

3. Utilization rate: 40%

4. Final calculation: 
1.50×10¹⁴ FLOP/s × 1 GPU × 59,000 s × 0.4 = 3.54×10¹⁸ FLOPs",,,75739,"BindingDB:
20,675 + 20,675 = 41,350

BioSNAP:
13,830 + 13,830 = 27,660",,,,NVIDIA A40 PCIe,1,,,"Considering the high cost associated with determining reaction affinities through in-vitro experiments, virtual screening of potential drugs bound with specific protein pockets from vast compounds is critical in AI-assisted drug discovery. Deep-leaning approaches have been proposed for Drug-Target Interaction (DTI) prediction. However, they have shown overestimated accuracy because of the drug-bias trap, a challenge that results from excessive reliance on the drug branch in the traditional drug-protein dual-branch network approach. This casts doubt on the interpretability and generalizability of existing Drug-Target Interaction (DTI) models. Therefore, we introduce UdanDTI, an innovative deep-learning architecture designed specifically for predicting drug-protein interactions. UdanDTI applies an unbalanced dual-branch system and an attentive aggregation module to enhance interpretability from a biological perspective. Across various public datasets, UdanDTI demonstrates outstanding performance, outperforming state-of-the-art models under in-domain, cross-domain, and structural interpretability settings. Notably, it demonstrates exceptional accuracy in predicting drug responses of two crucial subgroups of Epidermal Growth Factor Receptor (EGFR) mutations associated with non-small cell lung cancer, consistent with experimental results. Meanwhile, UdanDTI could complement the advanced molecular docking software DiffDock. The codes and datasets of UdanDTI are available at https://github.com/CQ-zhang-2016/UdanDTI.",,,Unverified,China,,,,,,Academia,,,Academia,,,660.9496344885652,Hardware
FvFold,Biology,Protein folding prediction,Jeonbuk National University,"Pasang Sherpa, Kil To Chong, Hilal Tayara",2024-11-01,FvFold: A model to predict antibody Fv structure using protein language model with residual network and Rosetta minimization,https://www.sciencedirect.com/science/article/pii/S0010482524012137,,,,9209788.00,"""In total, the model contains about 9,209,788 trainable parameters.""",,"""The training was performed on an NVIDIA A100 80 GB PCIe GPU with 40 CPU cores and 754 GB RAM capacity. [...] The model was trained
on a nonredundant set of 2253 Fv structures from the Structural
Antibody Database (SAbDab) [14,15] at 99% sequence identity.""",,,,,,,,NVIDIA A100 PCIe,1,,,"The immune system depends on antibodies (Abs) to recognize and attach to a wide range of antigens, playing a pivotal role in immunity. The precise prediction of the variable fragment (Fv) region of antibodies is vital for the progress of therapeutic and commercial applications, particularly in the treatment of diseases such as cancer. Although deep learning models exist for accurate antibody structure prediction, challenges persist, particularly in modeling complementarity-determining regions (CDRs) and the overall antibody Fv structures. Introducing the FvFold model, a deep learning approach harnessing the capabilities of the ProtT5-XL-UniRef50 protein language model which is capable of predicting accurate antibody Fv structure. Through evaluations on various benchmarks, our model outperforms existing models, demonstrating superior accuracy by achieving lower Root Mean Square Deviation (RMSD) in almost all loops and Orientational Coordinate Distance (OCD) values in the RosettaAntibody benchmark, Therapeutic benchmark and IgFold benchmark compared to the previous top-performing model.",Open weights (unrestricted),,Unverified,Korea (Republic of),,,,,,Academia,,,Academia,,,660.6647152283579,
MolPhenix,Biology,Cell Biology,"Valence Labs,University of British Columbia (UBC),Vector Institute,University of Toronto,University of Montreal / Université de Montréal,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)","Philip Fradkin, Puria Azadi, Karush Suri, Frederik Wenkel, Ali Bashashati, Maciej Sypetkowski, Dominique Beaini",2024-09-10,How Molecules Impact Cells: Unlocking Contrastive PhenoMolecular Retrieval,https://arxiv.org/abs/2409.08302,,,,38700000.00,"Taken from table 7, ""model size"" row; ""medium (38.7M)""",1100000000000000000.00,"""We utilized an NVIDIA A100 GPU to train Molphenix using Phenom1 and MolGPS embeddings, which takes approximately ∼4.75 hours each."" Assume FP16 precision, 40% utilization. ",,,2150001,"2,150,000 datapoints

Based on:
- 1,316,283 molecule-concentration pairs
- >2,150,000 phenomic experiments 
Final count = 2,150,000 unique datapoints
(2.15e6)",100.00,9.5,"""We utilized an NVIDIA A100 GPU to train Molphenix using Phenom1 and MolGPS embeddings, which takes approximately ∼4.75 hours each.""",NVIDIA A100,1,,,"Predicting molecular impact on cellular function is a core challenge in therapeutic design. Phenomic experiments, designed to capture cellular morphology, utilize microscopy based techniques and demonstrate a high throughput solution for uncovering molecular impact on the cell. In this work, we learn a joint latent space between molecular structures and microscopy phenomic experiments, aligning paired samples with contrastive learning. Specifically, we study the problem ofContrastive PhenoMolecular Retrieval, which consists of zero-shot molecular structure identification conditioned on phenomic experiments. We assess challenges in multi-modal learning of phenomics and molecular modalities such as experimental batch effect, inactive molecule perturbations, and encoding perturbation concentration. We demonstrate improved multi-modal learner retrieval through (1) a uni-modal pre-trained phenomics model, (2) a novel inter sample similarity aware loss, and (3) models conditioned on a representation of molecular concentration. Following this recipe, we propose MolPhenix, a molecular phenomics model. MolPhenix leverages a pre-trained phenomics model to demonstrate significant performance gains across perturbation concentrations, molecular scaffolds, and activity thresholds. In particular, we demonstrate an 8.1x improvement in zero shot molecular retrieval of active molecules over the previous state-of-the-art, reaching 77.33% in top-1% accuracy. These results open the door for machine learning to be applied in virtual phenomics screening, which can significantly benefit drug discovery applications.",,,Unverified,"Canada,Canada,Canada,Canada,Canada,Canada",,,,,,"Industry,Academia,Academia,Academia,Academia,Academia",,,"Industry,Academia,Academia,Academia,Academia,Academia",,,881.3067148261938,Hardware
MassiveFold,Biology,Protein folding prediction,"Université de Lille,Linköping University,Universite de Technologie de Compiègne – CNRS","Nessim Raouraoua, Claudio Mirabello, Thibaut Véry, Christophe Blanchet, Bjorn Wallner, Marc Lensink, Guillaume Brysbaert",2024-11-11,MassiveFold: unveiling AlphaFold’s hidden potential with optimized and parallelized massive sampling,https://www.researchsquare.com/article/rs-4319486/v1,,,,,,,,,,,,,,,,,,,"Massive sampling in AlphaFold enables access to increased structural diversity; in combination with its powerful confidence ranking, this unlocks elevated modeling capabilities for monomeric structures and foremost for protein assemblies. However, the approach struggles with GPU cost and data storage. MassiveFold removes these restraints as an optimized and customizable version of AlphaFold that runs predictions in parallel, offering the full benefit of enhanced sampling for protein structure and assembly modeling.",,,Unverified,"France,Sweden,France",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
NeoaPred,Biology,Immunogenic Neoantigen Prediction,"South China University of Technology,Jinan University","Dawei Jiang, Binbin Xi, Wenchong Tan, Zixi Chen, Jinfen Wei, Meiling Hu, Xiaoyun Lu, Dong Chen, Hongmin Cai, Hongli Du",2024-09-14,NeoaPred: A deep-learning framework for predicting immunogenic neoantigen based on surface and structural features of peptide-HLA complexes,https://academic.oup.com/bioinformatics/article/40/9/btae547/7758065,,,,,,,,,,39001,"PDB structures: 1,018 × 0.90 = 916
Self-distillation: 7,860
PepConf total: 916 + 7,860 = 8,776 ≈ 8.8 × 10³

PepFore samples: 32,962 × 0.90 = 29,666 ≈ 2.97 × 10⁴

Final total: 8.8 × 10³ + 2.97 × 10⁴ = 3.85 × 10⁴ ≈ 3.9 × 10⁴ datapoints",,,,,,,,"Motivation
Neoantigens, derived from somatic mutations in cancer cells, can elicit anti-tumor immune responses when presented to autologous T cells by human leukocyte antigen. Identifying immunogenic neoantigens is crucial for cancer immunotherapy development. However, the accuracy of current bioinformatic methods remains unsatisfactory. Surface and structural features of peptide–HLA class I (pHLA-I) complexes offer valuable insight into the immunogenicity of neoantigens.

Results
We present NeoaPred, a deep-learning framework for neoantigen prediction. NeoaPred accurately constructs pHLA-I complex structures, with 82.37% of the predicted structures showing an RMSD of < 1 Å. Using these structures, NeoaPred integrates differences in surface, structural, and atom group features between the mutant peptide and its wild-type counterpart to predict a foreignness score. This foreignness score is an effective factor for neoantigen prediction, achieving an AUROC (Area Under the Receiver Operating Characteristic Curve) of 0.81 and an AUPRC (Area Under the Precision-Recall Curve) of 0.54 in the test set, outperforming existing methods.

Availability and implementation
The source code is released under an Apache v2.0 license and is available at the GitHub repository (https://github.com/Dulab2020/NeoaPred).",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
ConfRank,Biology,Drug discovery,"University of Bonn,Institute for Numerical Simulation,Fraunhofer Institute for Algorithms and Scientific Computing","Christian Hölzer, Rick Oerderm, Stefan Grimme, Jan Hamaekers,",2024-11-24,ConfRank: Improving GFN-FF Conformer Ranking with Pairwise Training,https://pubs.acs.org/doi/abs/10.1021/acs.jcim.4c01524,,,,,,,,,,1400001,"7,349 molecular ensembles × 20 conformers = 146,980 conformers
Pairs per ensemble = (20 × 19) ÷ 2 = 190 pairs
Total pairs = 7,349 ensembles × 190 pairs = 1,396,310 datapoints",,,,,,,,"Conformer ranking is a crucial task for drug discovery, with methods for generating conformers often based on molecular (meta)dynamics or sophisticated sampling techniques. These methods are constrained by the underlying force computation regarding runtime and energy ranking accuracy, limiting their effectiveness for large-scale screening applications. To address these ranking limitations, we introduce ConfRank, a machine learning-based approach that enhances conformer ranking using pairwise training. We demonstrate its performance using GFN-FF-generated conformer ensembles, leveraging the DimeNet++ architecture trained on pairs of 159 760 uncharged organic compounds from the GEOM data set with r2SCAN-3c reference level. Instead of predicting only on single molecules, this approach captures relative energy differences between conformers, leading to a significant improvement of the overall conformational ranking, outperforming GFN-FF and GFN2-xTB. Thereby, the pairwise RMSD of the relative energy difference of two conformers can be reduced from 5.65 to 0.71 kcal mol–1 on the test data set, allowing to correctly identify up to 81% of all lowest lying conformers correctly (GFN-FF: 10%, GFN2-xTB: 47%). The ConfRank approach is cost-effective, allowing for scalable deployment on both CPU and GPU, achieving runtime accelerations by up to 2 orders of magnitude compared to GFN2-xTB. Out-of-sample investigations on CREST-generated conformer ensembles from the QM9 data set and conformers taken from an extended GMTKN55 data set show promising results for the robustness of this approach. Thereby, ranking correlation coefficient such as Spearman can be improved to 0.90 (GFN-FF: 0.39, GFN2-xTB: 0.84) reducing the probability of an incorrect sign flip in pairwise energy comparison from 32 to 7%. On the extended GMTKN55 subsets the pairwise MAD (RMSD) could be reduced on almost all subsets by up to 62% (58%) with an average improvement of 30% (29%). Moreover, an exemplary case study on vancomycin shows similar performance, indicating applicability to larger (bio)molecular structures. Furthermore, we motivate the usage of the pairwise training approach from a theoretical perspective, highlighting that while pairwise training can lead to a decline in single sample prediction of absolute energies for ML models, it significantly enhances conformer ranking performance. The data and models used in this study are available at https://github.com/grimme-lab/confrank.",,,Unverified,"Germany,Germany,Germany",,,,,,Academia,,,Academia,,,,
DeepRelax,Biology,Protein folding prediction,"National University of Singapore,Sun Yat-sen University,Peking University,China Medical University Hospital,Asia university,Guangdong L-Med Biotechnology Company","Ziduo Yang, Yi-Ming Zhao, Xian Wang, Xiaoqing Liu, Xiuying Zhang, Yifan Li, Qiujie Lv, Calvin Yu-Chian Chen & Lei Shen",2024-09-17,Scalable crystal structure relaxation using an iteration-free deep generative model with uncertainty quantification,https://www.nature.com/articles/s41467-024-52378-3,,,,,,,,,,,,,,,,,,,"In computational molecular and materials science, determining equilibrium structures is the crucial first step for accurate subsequent property calculations. However, the recent discovery of millions of new crystals and super large twisted structures has challenged traditional computational methods, both ab initio and machine-learning-based, due to their computationally intensive iterative processes. To address these scalability issues, here we introduce DeepRelax, a deep generative model capable of performing geometric crystal structure relaxation rapidly and without iterations. DeepRelax learns the equilibrium structural distribution, enabling it to predict relaxed structures directly from their unrelaxed ones. The ability to perform structural relaxation at the millisecond level per structure, combined with the scalability of parallel processing, makes DeepRelax particularly useful for large-scale virtual screening. We demonstrate DeepRelax’s reliability and robustness by applying it to five diverse databases, including oxides, Materials Project, two-dimensional materials, van der Waals crystals, and crystals with point defects. DeepRelax consistently shows high accuracy and efficiency, validated by density functional theory calculations. Finally, we enhance its trustworthiness by integrating uncertainty quantification. This work significantly accelerates computational workflows, offering a robust and trustworthy machine-learning method for material discovery and advancing the application of AI for science.

",,,Unverified,"Singapore,China,China,China,Taiwan,China",,,,,,"Academia,Academia,Academia,Academia,Industry",,,"Academia,Academia,Academia,Academia,Industry",,,,
GraphEC,Biology,Enzyme function prediction,"Sun Yat-sen University,National Supercomputing Center in Shenzhen,Chongqing University,Key Laboratory of Machine Intelligence and Advanced Computing","Yidong Song, Qianmu Yuan, Sheng Chen, Yuansong Zeng, Huiying Zhao, Yuedong Yang",2024-09-18,Accurately predicting enzyme functions through geometric graph learning on ESMFold-predicted structures,https://www.nature.com/articles/s41467-024-52533-w,,,,,,,,,,78001,"588 + 74,487 + 3,297 = 78,372 ≈ 7.8e4 datapoints

GraphEC training data from:
- Active Site: 588
- EC Number: 74,487
- Optimum pH: 3,297",,,,,,,,"Enzymes are crucial in numerous biological processes, with the Enzyme Commission (EC) number being a commonly used method for defining enzyme function. However, current EC number prediction technologies have not fully recognized the importance of enzyme active sites and structural characteristics. Here, we propose GraphEC, a geometric graph learning-based EC number predictor using the ESMFold-predicted structures and a pretrained protein language model. Specifically, we first construct a model to predict the enzyme active sites, which is utilized to predict the EC number. The prediction is further improved through a label diffusion algorithm by incorporating homology information. In parallel, the optimum pH of enzymes is predicted to reflect the enzyme-catalyzed reactions. Experiments demonstrate the superior performance of our model in predicting active sites, EC numbers, and optimum pH compared to other state-of-the-art methods. Additional analysis reveals that GraphEC is capable of extracting functional information from protein structures, emphasizing the effectiveness of geometric graph learning. This technology can be used to identify unannotated enzyme functions, as well as to predict their active sites and optimum pH, with the potential to advance research in synthetic biology, genomics, and other fields.",,,Unverified,"China,China,China,China",,,,,,"Academia,Government,Academia",,,"Academia,Government,Academia",,,,
DeepUrfold,Biology,Protein folding prediction,University of Virginia,"Eli J. Draizen, Stella Veretnik, Cameron Mura, Philip E. Bourne ",2024-09-16,Deep generative models of protein structure uncover distant relationships across a continuous fold space,https://www.nature.com/articles/s41467-024-52020-2,,,,110000000.00,"""Our final network has ≈ 110M parameters in total and all of the SF-level networks were trained for 30 epochs, using a batch size of 255""",,,,,,,,,,NVIDIA RTX A6000,,,,"Our views of fold space implicitly rest upon many assumptions that impact how we analyze, interpret and understand protein structure, function and evolution. For instance, is there an optimal granularity in viewing protein structural similarities (e.g., architecture, topology or some other level)? Similarly, the discrete/continuous dichotomy of fold space is central, but remains unresolved. Discrete views of fold space bin similar folds into distinct, nonoverlapping groups; unfortunately, such binning can miss remote relationships. While hierarchical systems like CATH are indispensable resources, less heuristic and more conceptually flexible approaches could enable more nuanced explorations of fold space. Building upon an Urfold model of protein structure, here we present a deep generative modeling framework, termed DeepUrfold, for analyzing protein relationships at scale. DeepUrfold’s learned embeddings occupy high-dimensional latent spaces that can be distilled for a given protein in terms of an amalgamated representation uniting sequence, structure and biophysical properties. This approach is structure-guided, versus being purely structure-based, and DeepUrfold learns representations that, in a sense, define superfamilies. Deploying DeepUrfold with CATH reveals evolutionarily-remote relationships that evade existing methodologies, and suggests a mostly-continuous view of fold space—a view that extends beyond simple geometric similarity, towards the realm of integrated sequence ↔ structure ↔ function properties.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
pKALM,Biology,Protein property prediction,Hokkaido University,"Shijie Xu,  Akira Onoda",2024-09-19,Accurate and Rapid Prediction of Protein pKa: Protein Language Models Reveal the Sequence-pKa Relationship,https://www.biorxiv.org/content/10.1101/2024.09.16.613101v1.abstract,,,,5083905.00,Loaded checkpoint and counted parameters.,6800000000000.00,"5083905 connections, 1712 training examples, 130 epochs.",,,1712,"""The revised data set now includes 1,450  pKa values for 165 wild-type proteins and 262 pKa values for 47 mutant proteins."" 1712 is sum of values.",130.00,,,NVIDIA GeForce RTX 3090 Ti,2,,,"Protein pKa prediction is a key challenge in computational biology. In this study, we present pKALM, a novel deep learning-based method for high-throughput protein pKa prediction. pKALM uses a protein language model (PLM) to capture the complex sequence-structure relationship of proteins. While traditionally considered a structure-based problem, our results show that a PLM pre-trained on large-scale protein sequence databases can effectively learn this relationship and achieve state-of-the-art performance. pKALM accurately predicts the pKa values of six residues (Asp, Glu, His, Lys, Cys, and Tyr) and two termini with high precision and efficiency. It excels at predicting both exposed and buried residues, which often deviate from standard pKa values measured in solvent. We demonstrate a novel finding that predicted protein isoelectric points (pI) can be used to improve the accuracy of pKa prediction. High-throughput pKa prediction of the human proteome using pKALM achieves a speed of 4,965 pKa predictions per second, which is several orders of magnitude faster than existing state-of-the-art methods. The case studies illustrate the efficacy of pKALM in estimating pKa values and the constraints of the method. pKALM will thus be a valuable tool for researchers in the fields of biochemistry, biophysics, and drug design.",,,Unverified,Japan,,,,,,Academia,,,Academia,,,1982.775977443404,Operation counting
GeoSeqBuilder,Biology,Protein design,Peking University,"Jiale Liu, Zheng Guo, Hantian You, Changsheng Zhang, Luhua Lai",2024-09-19,All-Atom Protein Sequence Design Based on Geometric Deep Learning,https://onlinelibrary.wiley.com/doi/abs/10.1002/ange.202411461?casa_token=oCX9uTFcpvQAAAAA%3AiGc_jXpH-dzeUXL0LgRnySqIJorSnOaPwaPGpjf8PbI9etcMdXXoXzwrEOX-wrQdbRsjcAEkQO0C6w,1.00,,,,,6480000000000066000.00,"1. Hardware: 1x NVIDIA A30 GPU (1.50×10¹⁴ FLOP/s using fp16 tensor cores)

2. Training duration: Directly provided - 15 epochs × 2 hours/epoch = 30 hours = 108,000 seconds

3. Utilization rate: 40%

4. Calculation:
   1.50×10¹⁴ FLOP/s × 1 GPU × 108,000s × 0.40 = 6.48×10¹⁸ FLOP",,,4700001,"Training datapoints = Training proteins × Average residues per protein
23,500 × 200 = 4,700,000 datapoints

Final estimate: 4.7 million datapoints",,,,,,,,"Designing sequences for specific protein backbones is a key step in creating new functional proteins. Here, we introduce GeoSeqBuilder, a deep learning framework that integrates protein sequence generation with side chain conformation prediction to produce the complete all-atom structures for designed sequences. GeoSeqBuilder uses spatial geometric features from protein backbones and explicitly includes three-body interactions of neighboring residues. GeoSeqBuilder achieves native residue type recovery rate of 51.6 %, comparable to ProteinMPNN and other leading methods, while accurately predicting side chain conformations. We first used GeoSeqBuilder to design sequences for thioredoxin and a hallucinated three-helical bundle protein. All the 15 tested sequences expressed as soluble monomeric proteins with high thermal stability, and the 2 high-resolution crystal structures solved closely match the designed models. The generated protein sequences exhibit low similarity (minimum 23 %) to the original sequences, with significantly altered hydrophobic cores. We further redesigned the hydrophobic core of glutathione peroxidase 4, and 3 of the 5 designs showed improved enzyme activity. Although further testing is needed, the high experimental success rate in our testing demonstrates that GeoSeqBuilder is a powerful tool for designing novel sequences for predefined protein structures with atomic details. GeoSeqBuilder is available at https://github.com/PKUliujl/GeoSeqBuilder.",,,Unverified,China,,,,,,Academia,,,Academia,,,,Hardware
McMLP,Biology,Human physiology,"Harvard Medical School,University of Illinois Urbana-Champaign (UIUC),Harvard TH Chan School of Public Health","Tong Wang, Hannah D Holscher, Sergei Maslov, Frank B Hu, Scott T Weiss, Yang-Yu Liu",2024-09-19,Predicting metabolite response to dietary intervention using deep learning,https://pmc.ncbi.nlm.nih.gov/articles/PMC10054958/,,,,,"""Model detail: Each MLP model (for either the top or the bottom MLP in Supplementary Fig. 1) has 6 hidden layers in the middle, sandwiched by input and output variables. Each hidden layer has a fixed hidden layer dimension of 2048.""",,,,,333,"Training data points calculation:
Synthetic: 250 * 0.8 = 200 points
Avocado study: 132 points
Total = 200 + 132 = 332 points (3.32e2)",,,,,,,,"Due to highly personalized biological and lifestyle characteristics, different individuals may have different metabolite responses to specific foods and nutrients. In particular, the gut microbiota, a collection of trillions of microorganisms living in the gastrointestinal tract, is highly personalized and plays a key role in the metabolite responses to foods and nutrients. Accurately predicting metabolite responses to dietary interventions based on individuals’ gut microbial compositions holds great promise for precision nutrition. Existing prediction methods are typically limited to traditional machine learning models. Deep learning methods dedicated to such tasks are still lacking. Here we develop a method McMLP (Metabolite response predictor using coupled Multilayer Perceptrons) to fill in this gap. We provide clear evidence that McMLP outperforms existing methods on both synthetic data generated by the microbial consumer-resource model and real data obtained from six dietary intervention studies. Furthermore, we perform sensitivity analysis of McMLP to infer the tripartite food-microbe-metabolite interactions, which are then validated using the ground-truth (or literature evidence) for synthetic (or real) data, respectively. The presented tool has the potential to inform the design of microbiota-based personalized dietary strategies to achieve precision nutrition.",,,Unverified,"United States of America,United States of America,United States of America",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
IgGM,Biology,Protein design,"Chinese Academy of Sciences,University of Chinese Academy of Sciences,Tencent","Rubo Wang, Fandi Wu, Xingyu Gao, Jiaxiang Wu, Peilin Zhao, Jianhua Yao",2024-09-22,IgGM: A Generative Model for Functional Antibody and Nanobody Design,https://www.biorxiv.org/content/10.1101/2024.09.19.613838v1.abstract,,,,,,859999999999997600000.00,"1. Hardware setup: 8x NVIDIA A100 SXM4 80GB GPUs, 3.12e14 FLOP/s per GPU

2. Training duration: Directly provided as 10 days = 864,000 seconds

3. Utilization rate: 40%

4. Final calculation:
3.12e14 FLOP/s × 864,000s × 0.4 × 8 GPUs = 8.6e20 FLOPs",,,9200001,"Complexes: 6,448 + 1,907 = 8,355
Amino acids per complex: 450 + 350 + 300 = 1,100
Total amino acids: 8,355 * 1,100 = 9,190,500",,,,,,,,"Immunoglobulins are crucial proteins produced by the immune system to identify and bind to foreign substances, playing an essential role in shielding organisms from infections and diseases. Designing specific antibodies opens new pathways for disease treatment. With the rise of deep learning, AI-driven drug design has become possible, leading to several methods for antibody design. However, many of these approaches require additional conditions that differ from real-world scenarios, making it challenging to incorporate them into existing antibody design processes. Here, we introduce IgGM, generative model that combines a diffusion model and the consistency model for generating antibodies with functional specificity. IgGM produces antibody sequences and structures simultaneously for a given antigen, consisting of three core components: a pre-trained language model for extracting sequence features, a feature learning module for identifying pertinent features, and a prediction module that outputs designed antibody sequences and the predicted complete antibody-antigen complex structure. IgGM has shown effectiveness in both predicting structures and designing novel antibodies and nanobodies, making it relevant in various practical scenarios of antibody and nanobody design.",,,Unverified,"China,China,China",,,,,,"Academia,Academia,Industry",,,"Academia,Academia,Industry",,,,Hardware
ProtENN2,Biology,Protein classification,"European Bioinformatics Institute,University of Cambridge,Google Research","Irina Ponamareva, Antonina Andreeva, Maxwell L Bileschi, Lucy Colwell, Alex Bateman",2024-09-18,Investigation of protein family relationships with deep learning,https://academic.oup.com/bioinformaticsadvances/article/4/1/vbae132/7760237,,,,,,,,,,23910000001,"Total datapoints = 23,910,108,270 residues
= 2.391 × 10¹⁰

Calculation: Single value from training database, no additions or multiplications needed.",,,,,,,,"Motivation
In this article, we propose a method for finding similarities between Pfam families based on the pre-trained neural network ProtENN2. We use the model ProtENN2 per-residue embeddings to produce new high-dimensional per-family embeddings and develop an approach for calculating inter-family similarity scores based on these embeddings, and evaluate its predictions using structure comparison.

Results
We apply our method to Pfam annotation by refining clan membership for Pfam families, suggesting both new members of existing clans and potential new clans for future Pfam releases. We investigate some of the failure modes of our approach, which suggests directions for future improvements. Our method is relatively simple with few parameters and could be applied to other protein family classification models. Overall, our work suggests potential benefits of employing deep learning for improving our understanding of protein family relationships and functions of previously uncharacterized families.

Availability and implementation
github.com/iponamareva/ProtCNNSim, 10.5281/zenodo.10091909.",,,Unverified,"Multinational,United Kingdom of Great Britain and Northern Ireland,Multinational,United States of America",,,,,,"Research collective,Academia,Industry",,,"Research collective,Academia,Industry",,,,
ExSelfRL,Biology,Drug discovery,Soochow University,"Jing Wang, Fei Zhu",2024-09-20,ExSelfRL: An exploration-inspired self-supervised reinforcement learning approach to molecular generation,https://www.sciencedirect.com/science/article/pii/S0957417424022772,,,,,,,,,,,,,,,,,,,"Efficiently searching for novel molecules with specific properties is critical to molecular generation. Some existing works focus on combining deep generative models and reinforcement learning to generate molecules with targeted properties, but there is still the problem of reduced model effectiveness due to sparse rewards. To address the problem, an exploration-inspired self-supervised reinforcement learning (ExSelfRL) method for molecular generation is proposed. By constructing an exploration-inspired reward-shaping framework, ExSelfRL can effectively mine the intrinsic rewards in the molecule generation process based on novelty. Then, driven by intrinsic and primitive sparse rewards, ExSelfRL establishes a self-supervised reinforcement learning agent capable of exploring a broader chemical space to find molecules with better properties. In addition, a dominant set of molecules is defined from the sampled molecules that can further improve their property scores. The experimental results illustrate that ExSelfRL can generate molecules with higher property scores than existing methods.",,,Unverified,Taiwan,,,,,,Academia,,,Academia,,,,
TAWFN,Biology,Protein function prediction,Northeastern University (China),"Lu Meng, Xiaoran Wang",2024-09-23,TAWFN: A Deep Learning Framework for Protein Function Prediction,https://academic.oup.com/bioinformatics/article/40/10/btae571/7766190,,,,,,3500000000000028700.00,"1. Hardware setup: 1x NVIDIA GeForce RTX 3090 (1.60 x 10^14 FLOPs/s per GPU)

2. Training duration: Estimated
   * 71,000 samples, batch size 64 → 1,100 batches/epoch
   * 0.5s/batch → 9 min/epoch
   * 100 epochs → 15 hours (54,000 seconds)

3. Utilization rate: 40%

4. Final calculation:
   1.60 x 10^14 FLOPs/s × 1 GPU × 54,000s × 0.4 = 3.456 x 10^18 FLOPs",,,72001,"PDBset Training = 36,629 * 0.8 = 29,303
AFset Training = 42,427
Total = 29,303 + 42,427 = 71,730",,,,NVIDIA GeForce RTX 3090,1,,,"Motivation
Proteins play pivotal roles in biological systems, and precise prediction of their functions is indispensable for practical applications. Despite the surge in protein sequence data facilitated by high-throughput techniques, unraveling the exact functionalities of proteins still demands considerable time and resources. Currently, numerous methods rely on protein sequences for prediction, while methods targeting protein structures are scarce, often employing convolutional neural networks (CNN) or graph convolutional networks (GCNs) individually.

Results
To address these challenges, our approach starts from protein structures and proposes a method that combines CNN and GCN into a unified framework called the two-model adaptive weight fusion network (TAWFN) for protein function prediction. First, amino acid contact maps and sequences are extracted from the protein structure. Then, the sequence is used to generate one-hot encoded features and deep semantic features. These features, along with the constructed graph, are fed into the adaptive graph convolutional networks (AGCN) module and the multi-layer convolutional neural network (MCNN) module as needed, resulting in preliminary classification outcomes. Finally, the preliminary classification results are inputted into the adaptive weight computation network, where adaptive weights are calculated to fuse the initial predictions from both networks, yielding the final prediction result. To evaluate the effectiveness of our method, experiments were conducted on the PDBset and AFset datasets. For molecular function, biological process, and cellular component tasks, TAWFN achieved area under the precision-recall curve (AUPR) values of 0.718, 0.385, and 0.488 respectively, with corresponding Fmax scores of 0.762, 0.628, and 0.693, and Smin scores of 0.326, 0.483, and 0.454. The experimental results demonstrate that TAWFN exhibits promising performance, outperforming existing methods.

Availability and implementation
The TAWFN source code can be found at: https://github.com/ss0830/TAWFN.",,,Unverified,China,,,,,,Academia,,,Academia,,,771.0511997557896,Hardware
DrugTar,Biology,"Protein property prediction,Drug discovery",Isfahan University of Technology,"Niloofar Borhani, Iman Izadi,  View ORCID ProfileAli Motahharynia, Mahsa Sheikholeslami, Yousof Gheisari",2024-09-24,DrugTar Improves Druggability Prediction by Integrating Large Language Models and Gene Ontologies,https://www.biorxiv.org/content/10.1101/2024.09.21.614218v1.abstract,,,,,,,,,,6317,"ProTar-I: 2,248 proteins
ProTar-II: 4,068 proteins
Final total: 2,248 + 4,068 = 6,316 proteins",,,,,,,,"Target discovery is crucial in drug development, especially for complex chronic diseases. Recent advances in high-throughput technologies and the explosion of biomedical data have highlighted the potential of computational druggability prediction methods. However, most current methods rely on sequence-based features with machine learning, which often face challenges related to hand-crafted features, reproducibility, and accessibility. Moreover, the potential of raw sequence and protein structure has not been fully investigated. Here, we leveraged both protein sequence and structure using deep learning techniques, revealing that protein sequence, especially pre- trained embeddings, is more informative than protein structure. Next, we developed DrugTar, a highl7lperformance deep learning algorithm integrating sequence embeddings from the ESM-2 pre-trained protein language model with protein ontologies to predict druggability. DrugTar achieved areas under the curve and precision-recall curve values above 0.90, outperforming state-of-the-art methods. In conclusion, DrugTar streamlines target discovery as a bottleneck in developing novel therapeutics.",,,Unverified,Iran (Islamic Republic of),,,,,,Academia,,,Academia,,,,
Designing of thermostable proteins with a desired melting temperature,Biology,Protein design,"""Indraprastha Institute of Information Technology
Delhi""","Purva Tijare, Nishant Kumar, Gajendra P. S. Raghava",2024-09-24,Designing of thermostable proteins with a desired melting temperature,https://www.biorxiv.org/content/10.1101/2024.09.21.614294v1.abstract,,,,738000000.00,"""This model comprises 36  layers and has a dimensionality of 1280, amounting to a total of 738 million parameters [28].""",61000000000000.00,738M connections with 13849 training examples for one (assumed) epoch using calculator.,,,13849,"""Training Dataset 13849 sequences"" in Figure 1",,,,,,,,"The stability of proteins at higher temperatures is crucial for its functionality that is measured by their melting temperature (Tm). The Tm is the temperature at which 50% of the protein loses its native structure and activity. Existing methods for predicting Tm have two major limitations: first, they are often trained on redundant proteins, and second, they do not allow users to design proteins with the desired Tm. To address these limitations, we developed a regression method for predicting the Tm value of proteins using 17,312 non-redundant proteins, where no two proteins are more than 40% similar. We used 80% of the data for training and testing; remaining 20% of the data for validation. Initially, we developed a machine learning model using standard features from protein sequences. Our best model, developed using Shannon entropy for all residues, achieved the highest Pearson correlation of 0.80 with an R² of 0.63 between the predicted and actual Tm of proteins on the validation dataset. Next, we fine-tuned large language models (e.g., ProtBert, ProtGPT2, ProtT5) on our training dataset and generated embeddings. These embeddings have been used for developing machine learning models. Our best model, developed using ProtBert embeddings, achieved a maximum correlation of 0.89 with an R² of 0.80 on the validation dataset. Finally, we developed an ensemble method that combines standard protein features and embeddings. One of the aims of the study is to assist the scientific community in the design of targeted melting temperatures. We created a user-friendly web server and a python package for predicting and designing thermostable proteins. Our standalone software can be used to screen thermostable proteins in genomes and metagenomes. We demonstrated the application of PPTstab in identifying thermostable proteins in different organisms from their genomes, the model and data is available at: https://webs.iiitd.edu.in/raghava/pptstab.",,,Unverified,India,,,,,,Academia,,,Academia,,,,Operation counting
ProTeM,Biology,Protein function prediction,"Zhejiang Lab,Zhejiang University,Huazhong University of Science and Technology,Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)","Ming Qin, Xun Li, Yuhao Wang, Zhenping Li, Hongbin Ye, Zongbing Wang, Weihao Gao, Shangsong Liang, Qiang Zhang, Keyan Ding ",2024-09-17,ProTeM: Unifying Protein Function Prediction via Text Matching,https://link.springer.com/chapter/10.1007/978-3-031-72353-7_10,,,,,,,,,,363000001,"Bio-LLM: 2,000,000 documents × 100 tokens = 2.0 × 10^8 tokens
ProTeM: 467,277 pairs × (50 text + 300 protein) tokens = 1.63 × 10^8 tokens
Total: 2.0 × 10^8 + 1.63 × 10^8 = 3.63 × 10^8 tokens",,,,,,,,"The exponential availability of protein sequences has led to the dominance of the pretraining-then-finetuning paradigm for protein function prediction. However, finetuning a pretrained protein language model for diverse downstream tasks requires annotated protein data tailored to each task. To avoid the redundant individual finetuning, we propose a methodology that unifies various Protein function prediction tasks via Text Matching (named ProTeM). This method first transforms simple numeric or category labels from disparate protein datasets into textual descriptions, imbued with rich semantics. We then harness a pretrained large language model, which is proficient in comprehensive language understanding, to capture intrinsic interconnections among varied protein functions and facilitate the alignment between text and protein. During inference, we employ the paradigm of text matching to predict the protein functionalities. Extensive experiments demonstrate that ProTeM achieves performance on par with individually finetuned models, and outshines the model based on conventional multi-task learning. Moreover, ProTeM unveils an enhanced capacity for protein representation, surpassing state-of-the-art PLMs.",,,Unverified,"China,China,China,United Arab Emirates",,,"""During the Bio-LLM training process, we employ the LoRA finetuning technique ([7]), utilizing four NVIDIA V100-32G GPUs. The rank k in LoRA is configured as 8. We use the Adam optimizer with a batch size of 64 and a learning rate of 2e-4. When training ProTeM, we freeze the Bio-LLM and finetune the PLM only. The Adam learning rate is 2e-5 and a batch size of 10.""",,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
Jaeger,Biology,Bacteriophage screening,"University Medicine Greifswald,Utrecht University,Friedrich Schiller University Jena","Yasas Wijesekara, Ling-Yi Wu, Rick Beeloo, Piotr Rozwalak, Ernestina Hauptfeld, Swapnil P. Doijad, Bas E. Dutilh, Lars Kaderali",2024-09-24,Jaeger: an accurate and fast deep-learning tool to detect bacteriophage sequences,https://www.biorxiv.org/content/10.1101/2024.09.24.612722v1.abstract,,,,,,,,,,2112001,"Total fragments = 265,959 + 1,375,939 + 915,817 + 459,660 = 3,017,375
Training set (70%) = 3,017,375 × 0.70 = 2,112,162.5
Final estimate ≈ 2.112 × 10⁶ unique data points",,,,,,,,"Viruses are integral to every biome on Earth, yet we still need a more comprehensive picture of their identity and global distribution. Global metagenomics sequencing efforts revealed the genomic content of tens of thousands of environmental samples, however identifying the viral sequences in these datasets remains challenging due to their vast genomic diversity. Here, we address identifying bacteriophage sequences in unlabeled sequencing data. In a recent benchmarking paper, we observed that existing deep-learning tools show a high true positive rate, but may also produce many false positives when confronted with divergent sequences. To tackle this challenge, we introduce Jaeger, a novel deep-learning method designed specifically for identifying bacteriophage genome fragments. Extensive benchmarking on the IMG/VR database and real-world metagenomes reveals Jaeger’s consistent high sensitivity (0.87) and precision (0.92). Applying Jaeger to over 16,000 metagenomic assemblies from the MGnify database yielded over five million putative phage contigs. On average, Jaeger is around 20 times faster than the other state-of-the-art methods. Jaeger is available at https://github.com/MGXlab/Jaeger.

",,,Unverified,"Germany,Netherlands,Germany",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
Importance of higher-order epistasis in large protein sequence-function relationships,Biology,Protein function prediction,University of Florida,"Palash Sethi a, Juannan Zhou",2024-09-24,Importance of higher-order epistasis in large protein sequence-function relationships,https://pmc.ncbi.nlm.nih.gov/articles/PMC11463489/,,,,,,,,,,23500001,"Initial calculations by dataset:
1. GRB-1: 129,320 * 0.8 * 33 = 3,414,048
2. GRB-3-abundance: 31,936 * 0.8 * 15 = 383,235
3. GRB-3-binding: 25,967 * 0.8 * 15 = 311,610
4. AAV2-Capsid: 42,328 * 0.8 * 28 = 948,136
5. CreiLOV: 165,428 * 0.8 * 15 = 1,985,130
6. cgreGFP: 26,165 * 0.8 * 234 = 4,898,088
7. ppluGFP: 32,260 * 0.8 * 221 = 5,703,568
8. His3-S2: 116,935 * 0.8 * 28 = 2,619,344
9. His3-S5: 92,408 * 0.8 * 31 = 2,291,706
10. His3-S12: 62,305 * 0.8 * 19 = 947,036

Final sum: 3,414,048 + 383,235 + 311,610 + 948,136 + 1,985,130 + 4,898,088 + 5,703,568 + 2,619,344 + 2,291,706 + 947,036 = 23,501,901",,,,NVIDIA A100,1,,,"Epistasis complicates our understanding of protein sequence-function relationships and impedes our ability to build accurate predictive models for novel genotypes. Although pairwise epistasis has been extensively studied in proteins, the significance of higher-order epistasis for protein sequence-function relationships remains contentious, largely due to challenges in fitting higher-order epistatatic interactions for full-length proteins. Here, we introduce a novel transformer-based approach. The key feature of our method is that we can adjust the order of interactions fit by the model by changing the number of attention layers while also accounting for any global nonlinearity induced by the experimental conditions. This allows us to test if inclusion of higher-order interactions leads to enhanced model performance. Applying our method to 10 large protein sequence-function datasets, we found that the importance of higher-order epistasis differs substantially between proteins, accounting for up to 60% of the total variance attributed to epistasis. We also found that including higher-order epistasis is particularly important for generalizing locally sampled fitness data to distant regions of sequence space and for modeling an additional multi-peak fitness landscape derived from combining mutagenesis data from 4 orthologous green fluorescencent proteins. Our findings suggest that higher-order epistasis often does play an important role in protein sequence-function relationships, and thus should be properly incorporated during protein engineering and evolutionary data analysis.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,881.1932743357389,
MTDP,Biology,Protein embedding,,"Jiayu Shang, Cheng Peng, Yongxin Ji, Jiaojiao Guan, Dehan Cai, Xubo Tang, Yanni Sun",2024-09-24,Accurate and efficient protein embedding using multi-teacher distillation learning,https://academic.oup.com/bioinformatics/article/40/9/btae567/7772445,,,,20000000.00,"""In particular, the student model in MTDP has a significantly smaller parameter scale (~20 million)"" from 2.1 Model Structure",600000000000000.00,"6 * 20,000,000 connections * 500,000 training examples * 10 epochs = 6.0e14 FLOP",UniProtKB/Swiss-Prot,"""MTDP is pre-trained on ~500 000 proteins from UniProtKB (Swiss-Prot) provided by the teacher model (Elnaggar et al. 2021)."" from 2.3.1 Pre-training data",500000,"""MTDP is pre-trained on ~500 000 proteins from UniProtKB (Swiss-Prot) provided by the teacher model (Elnaggar et al. 2021)."" from 2.3.1 Pre-training data",10.00,,,NVIDIA GeForce RTX 3080,4,,,"Motivation
Protein embedding, which represents proteins as numerical vectors, is a crucial step in various learning-based protein annotation/classification problems, including gene ontology prediction, protein–protein interaction prediction, and protein structure prediction. However, existing protein embedding methods are often computationally expensive due to their large number of parameters, which can reach millions or even billions. The growing availability of large-scale protein datasets and the need for efficient analysis tools have created a pressing demand for efficient protein embedding methods.

Results
We propose a novel protein embedding approach based on multi-teacher distillation learning, which leverages the knowledge of multiple pre-trained protein embedding models to learn a compact and informative representation of proteins. Our method achieves comparable performance to state-of-the-art methods while significantly reducing computational costs and resource requirements. Specifically, our approach reduces computational time by ∼70% and maintains ±1.5% accuracy as the original large models. This makes our method well-suited for large-scale protein analysis and enables the bioinformatics community to perform protein embedding tasks more efficiently.",,,Unverified,,,,,16,"""We conducted the pre-training process on four RTX 3080 24 GB Nvidia GPUs, with a batch size of 16."" from 2.3.1 Pre-training data",,,,,,,2819.818477874365,Operation counting
AlphaMut,Biology,Mutation prediction,Indian Institute of Science Education and Research,"Prathith Bhargav,  Arnab Mukherjee",2024-09-24,AlphaMut: a deep reinforcement learning model to suggest helix-disrupting mutations,https://www.biorxiv.org/content/10.1101/2024.09.21.614241v1.abstract,,,,,,,,,,358001,"Total Data Points = Helix-only Training Steps + Helix-in-protein Training Steps
168,000 + 190,000 = 358,000 = 3.58e5 datapoints",,,,,,,,"Helices are important secondary structural motifs within proteins and are pivotal in numerous physiological processes. While amino acids (AA) such as alanine and leucine are known to promote helix formation, proline and glycine disfavor it. Helical structure formation, however, also depends on its environment, and hence, prior prediction of a mutational effect on a helical structure is difficult. Here, we employ a reinforcement learning algorithm to develop a predictive model for helix-disrupting mutations. We start with a toy model consisting of helices with only 30 AA and train different models. Our results show that only a few mutations lead to a drastic disruption of the target helix. We further extend our approach to helices in proteins and validate the results using rigorous free energy calculations. Our strategy identifies amino acids crucial for maintaining structural integrity and predicts key mutations that could alter protein function. Through our work, we present a new use case for reinforcement learning in protein structure disruption.",,,Unverified,India,,,,,,Government,,,Government,,,,
Mothra,Biology,Drug discovery,Tokyo Institute of Technology,"Takamasa Suzuki, Dian Ma, Nobuaki Yasuo, Masakazu Sekijima",2024-09-25,Mothra: Multiobjective de novo Molecular Generation Using Monte Carlo Tree Search,,,,,,,37000000000000000000.00,"1. Hardware setup: 4x NVIDIA Tesla P100 GPUs (1.90×10¹³ FLOP/s per GPU)

2. Training duration: 14 days (directly provided) = 1,209,600 seconds

3. Utilization rate: 40%

4. Final calculation:
1.90×10¹³ FLOP/s/GPU × 4 GPUs × 1,209,600 seconds × 0.4 = 3.7×10¹⁹ FLOPs",,,12500001,"250,000 molecules × 50 tokens/molecule = 12,500,000 tokens
Total training datapoints = 12,500,000 tokens (1.25e7)",,,,,,,,"In the field of drug discovery, identifying compounds that satisfy multiple criteria, such as target protein affinity, pharmacokinetics, and membrane permeability, is challenging because of the vast chemical space. Until now, multiobjective optimization via generative models has often involved linear combinations of different reward functions. Linear combinations solve multiobjective optimization problems by turning multiobjective optimization into a single-objective task and causing problems with weighting for each objective. Herein, we propose a scalable multiobjective molecular generative model developed using deep learning techniques. This model integrates the capabilities of recurrent neural networks for molecular generation and Pareto multiobjective Monte Carlo tree search to determine the optimal search direction. Through this integration, our model can generate compounds using enhanced evaluation functions that include important aspects like target protein affinity, drug similarity, and toxicity. The proposed model addresses the limitations of previous linear combination methods, and its effectiveness is demonstrated via extensive experimentation. The improvements achieved in the evaluation metrics underscore the potential utility of our approach toward drug discovery applications. In addition, we provide the source code for our model such that researchers can easily access and use our framework in their own investigations. The source code and pretrained model for Mothra, developed in this study, along with the Docker image for the Pareto front explorer and compound picker, designed to streamline the selection and visualization of optimal chemical compounds, are released under the GNU General Public License v3.0 and available at https://github.com/sekijima-lab/Mothra.",,,Unverified,Japan,,,,,,Academia,,,Academia,,,,Hardware
GeoAB,Biology,Protein design,"Zhejiang University,Westlake University","Haitao Lin, Lirong Wu, Yufei Huang, Yunfan Liu, Odin Zhang, Yuanqing Zhou, Rui Sun, Stan Z. Li",2024-09-25,GeoAB: Towards Realistic Antibody Design and Reliable Affinity Maturation,https://www.biorxiv.org/content/10.1101/2024.05.15.594274v2.abstract,,,,,,1500000000000000300.00,"1. Hardware setup: 1x NVIDIA H100 PCIe GPU (7.56e14 FLOP/s with fp16_tensor)
2. Training duration: 5000 seconds (calculated from 250s/epoch × 20 epochs)
3. Utilization rate: 40%
4. Final calculation: 7.56e14 FLOP/s × 1 GPU × 5000s × 0.4 = 1.512e18 FLOP",,,16500001,"SAbDab: 30,000 structures × 10 CDRs/structure × 15 residues/CDR = 4.5 × 10⁶ residues
SKEMPI2: 3,000 interactions × 3,000 residues/interaction = 9 × 10⁶ residues
PDB-REDO: 30,000 structures × 100 residues/structure = 3 × 10⁶ residues

Total: 4.5 × 10⁶ + 9 × 10⁶ + 3 × 10⁶ = 16.5 × 10⁶ residues

Final estimate: 1.65 × 10⁷ datapoints",,,,,,,,"Increasing works for antibody design are emerging to generate sequences and structures in Complementarity Determining Regions (CDRs), but problems still exist. We focus on two of them: (i) authenticity of the generated structure and (ii) rationality of the affinity maturation, and propose GEOAB as a solution. In specific, GeoABDesigner generates CDR structures with realistic internal geometries, composed of a generative geometry initializer (Geo-Initializer) and a position refiner (Geo-Refiner); GeoAB-Optimizer achieves affinity maturation by accurately predicting both the mutation effects and structures of mutant antibodies with the same network architecture as Geo-Refiner. Experiments show that GEOAB achieves state-of-the-art performance in CDR co-design and mutation effect predictions, and fulfills the discussed tasks effectively.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
ChemNet,Biology,Protein folding prediction,University of Washington,"Ivan Anishchenko, Yakov Kipnis, Indrek Kalvet, Guangfeng Zhou, Rohith Krishna, Samuel J Pellock, Anna Lauko, Gyu Rie Lee, Linna An, Justas Dauparas, Frank DiMaio, David Baker",2024-09-25,Modeling protein-small molecule conformational ensembles with ChemNet ,https://pmc.ncbi.nlm.nih.gov/articles/PMC11463446/,,,,,,,,,,112829,"112,828 data points

Calculation: Single dataset (PDB) with 112,828 examples = 112,828 total data points",,,,,,,,"Modeling the conformational heterogeneity of protein-small molecule systems is an outstanding challenge. We reasoned that while residue level descriptions of biomolecules are efficient for de novo structure prediction, for probing heterogeneity of interactions with small molecules in the folded state an entirely atomic level description could have advantages in speed and generality. We developed a graph neural network called ChemNet trained to recapitulate correct atomic positions from partially corrupted input structures from the Cambridge Structural Database and the Protein Data Bank; the nodes of the graph are the atoms in the system. ChemNet accurately generates structures of diverse organic small molecules given knowledge of their atom composition and bonding, and given a description of the larger protein context, and builds up structures of small molecules and protein side chains for protein-small molecule docking. Because ChemNet is rapid and stochastic, ensembles of predictions can be readily generated to map conformational heterogeneity. In enzyme design efforts described here and elsewhere, we find that using ChemNet to assess the accuracy and pre-organization of the designed active sites results in higher success rates and higher activities; we obtain a preorganized retroaldolase with a kcat/KM of 11000 M−1min−1, considerably higher than any pre-deep learning design for this reaction. We anticipate that ChemNet will be widely useful for rapidly generating conformational ensembles of small molecule and small molecule-protein systems, and for designing higher activity preorganized enzymes.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
PPFlow,Biology,Protein design,"Zhejiang University,Westlake University","Haitao Lin, Odin Zhang, Huifeng Zhao, Dejun Jiang, Lirong Wu, Zicheng Liu, Yufei Huang, Stan Z. Li",2024-09-25,"PPFLOW: Target-aware Peptide Design with Torsional Flow Matching
",https://www.biorxiv.org/content/10.1101/2024.03.07.583831v5.abstract,,,,,,,,,,4850001,"Summary of calculations:
15,593 protein-peptide pairs
Average residues per pair = 300 (protein) + 12 (peptide) = 312
Total datapoints = 15,593 × 312 = 4.85 × 10^6",,,,,,,,"Therapeutic peptides have proven to have great pharmaceutical value and potential in recent decades. However, methods of AI-assisted peptide drug discovery are not fully explored. To fill the gap, we propose a target-aware peptide design method called PPFlow, based on conditional flow matching on torus manifolds, to model the internal geometries of torsion angles for the peptide structure design. Besides, we establish a protein-peptide binding dataset named PPBench2024 to fill the void of massive data for the task of structure-based peptide drug design and to allow the training of deep learning methods. Extensive experiments show that PPFlow reaches state-of-the-art performance in tasks of peptide drug generation and optimization in comparison with baseline models, and can be generalized to other tasks including docking and side-chain packing.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
TxGNN,Biology,Drug discovery,"Harvard Medical School,Harvard-MIT Program in Health Sciences and Technology,The Mount Sinai Hospital (New York),Broad Institute,Harvard Data Science Initiative,Harvard University,Stanford University","Kexin Huang, Payal Chandak, Qianwen Wang, Shreyas Havaldar, Akhil Vaid, Jure Leskovec, Girish N. Nadkarni, Benjamin S. Glicksberg, Nils Gehlenborg, Marinka Zitnik",2024-09-25,A foundation model for clinician-centered drug repurposing ,https://www.nature.com/articles/s41591-024-03233-x,,,,,,,,,,8063001,"Total datapoints = 8,063,026 edges

Calculation: Given directly from knowledge graph details (no addition/multiplication needed)
Final number: 8.063e6",,,,,,,,"Drug repurposing—identifying new therapeutic uses for approved drugs—is often a serendipitous and opportunistic endeavour to expand the use of drugs for new diseases. The clinical utility of drug-repurposing artificial intelligence (AI) models remains limited because these models focus narrowly on diseases for which some drugs already exist. Here we introduce TxGNN, a graph foundation model for zero-shot drug repurposing, identifying therapeutic candidates even for diseases with limited treatment options or no existing drugs. Trained on a medical knowledge graph, TxGNN uses a graph neural network and metric learning module to rank drugs as potential indications and contraindications for 17,080 diseases. When benchmarked against 8 methods, TxGNN improves prediction accuracy for indications by 49.2% and contraindications by 35.1% under stringent zero-shot evaluation. To facilitate model interpretation, TxGNN’s Explainer module offers transparent insights into multi-hop medical knowledge paths that form TxGNN’s predictive rationales. Human evaluation of TxGNN’s Explainer showed that TxGNN’s predictions and explanations perform encouragingly on multiple axes of performance beyond accuracy. Many of TxGNN’s new predictions align well with off-label prescriptions that clinicians previously made in a large healthcare system. TxGNN’s drug-repurposing predictions are accurate, consistent with off-label drug use, and can be investigated by human experts through multi-hop interpretable rationales.",,,Unverified,"United States of America,United States of America,United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,"Academia,Research collective,Research collective,Academia,Academia,Academia",,,"Academia,Research collective,Research collective,Academia,Academia,Academia",,,,
SeaMoon,Biology,Protein folding prediction,"Sorbonne University,Université Grenoble Alpes,Institut Universitaire de France (IUF)","Valentin Lombard, Dan Timsit, Sergei Grudinin, Elodie Laine",2024-09-25,SeaMoon: Prediction of molecular motions based on language models,https://www.biorxiv.org/content/10.1101/2024.09.23.614585v1.abstract,,,,,,,,,,11000001,"7,339 collections × 5 conformations/collection = 36,695 conformations
36,695 conformations × 300 residues/protein = 11,008,500 datapoints
Final estimate: 1.1e7 datapoints",,,,,,,,"How protein move and deform determines their interactions with the environment and is thus of utmost importance for cellular functioning. Following the revolution in single protein 3D structure prediction, researchers have focused on repurposing or developing deep learning models for sampling alternative protein conformations. In this work, we explored whether continuous compact representations of protein motions could be predicted directly from protein sequences, without exploiting nor sampling protein structures. Our approach, called SeaMoon, leverages protein Language Model (pLM) embeddings as input to a lightweight (∼1M trainable parameters) convolutional neural network. SeaMoon achieves a success rate of up to 40% when assessed against ∼1 000 collections of experimental conformations exhibiting a wide range of motions. SeaMoon capture motions not accessible to the normal mode analysis, an unsupervised physics-based method relying solely on a protein structure’s 3D geometry, and generalises to proteins that do not have any detectable sequence similarity to the training set. SeaMoon is easily retrainable with novel or updated pLMs.",,,Unverified,"France,France,France",,,,,,"Academia,Academia,Research collective",,,"Academia,Academia,Research collective",,,,
BetterBodies,Biology,Protein design,"University of Freiburg,Collaborative Research Institute Intelligent Oncology ,BrainLinks-BrainTools","Yannick Vogt, Mehdi Naouar, Maria Kalweit, Christoph Cornelius Miething, Justus Duyster, Joschka Boedecker, Gabriel Kalweit",2024-09-09,BETTERBODIES: Reinforcement Learning Guided Diffusion for Antibody Sequence Design,https://arxiv.org/abs/2409.16298,,,,,,,,,,81621,"Total sequences: 2,500 + 2,753 + 2,167 = 7,420
Data points = 7,420 sequences × 11 residues/sequence = 81,620 data points

Final estimate: 81,620",,,,,,,,"Antibodies offer great potential for the treatment of various diseases. However, the discovery of therapeutic antibodies through traditional wet lab methods is expensive and time-consuming. The use of generative models in designing antibodies therefore holds great promise, as it can reduce the time and resources required. Recently, the class of diffusion models has gained considerable traction for their ability to synthesize diverse and high-quality samples. In their basic form, however, they lack mechanisms to optimize for specific properties, such as binding affinity to an antigen. In contrast, the class of offline Reinforcement Learning (RL) methods has demonstrated strong performance in navigating large search spaces, including scenarios where frequent real-world interaction, such as interaction with a wet lab, is impractical. Our novel method, BetterBodies, which combines Variational Autoencoders (VAEs) with RL guided latent diffusion, is able to generate novel sets of antibody CDRH3 sequences from different data distributions. Using the Absolut! simulator, we demonstrate the improved affinity of our novel sequences to the SARS-CoV spike receptor-binding domain. Furthermore, we reflect biophysical properties in the VAE latent space using a contrastive loss and add a novel Q-function based filtering to enhance the affinity of generated sequences. In conclusion, methods such as ours have the potential to have great implications for real-world biological sequence design, where the generation of novel high-affinity binders is a cost-intensive endeavor.",,,Unverified,"Germany,Germany,Germany",,,,,,"Academia,Research collective,Academia",,,"Academia,Research collective,Academia",,,,
KnoMol,Biology,Molecular property prediction,"Zhejiang University,Jiangsu University of Technology,Zhejiang University School of Medicine","Jian Gao, Zheyuan Shen, Yan Lu, Liteng Shen, Binbin Zhou, Donghang Xu, Haibin Dai, Lei Xu, Jinxin Che, Xiaowu Dong",2024-09-25,KnoMol: A Knowledge-Enhanced Graph Transformer for Molecular Property Prediction,https://pubs.acs.org/doi/abs/10.1021/acs.jcim.4c01092,,,,,,,,,,1160001,"Total datapoints = 1,163,125

Calculation breakdown:
1,163,125 molecules (CHEMBL1163125 dataset)

Final result: 1.16e6",,,,,,,,"Molecular property prediction (MPP) techniques are pivotal in reducing drug development costs by preemptively predicting bioactivity and ADMET properties. Despite the application of numerous deep learning approaches, enhancing the representational capacity of these models remains a significant challenge. This paper presents a novel knowledge-based Transformer framework, KnoMol, designed to improve the understanding of molecular structures. KnoMol integrates expert chemical knowledge into the Transformer, emulating the analytical methods of medicinal chemists. Additionally, the multiperspective attention mechanism provides a more precise way to represent ring systems. In the evaluation experiments, KnoMol achieved state-of-the-art performance on both MoleculeNet and small-scale data sets, surpassing existing models in terms of accuracy and generalization. Further research indicated that the incorporation of knowledge significantly reduces KnoMol’s reliance on data volumes, offering a solution to the challenge of data scarcity. Moreover, KnoMol identified several new inhibitors of HER2 in a case study, demonstrating its value in real-world applications. Overall, this research not only provides a powerful tool for MPP but also serves as a successful precedent for embedding knowledge into Transformers, with positive implications for computer-aided drug discovery and the development of MPP algorithms.",,,Unverified,"China,China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
SPOT,Biology,Protein function prediction,"Heinrich Heine University,Concordia University","Alexander Kroll, Nico Niebuhr, Gregory Butler, Martin J. Lercher",2024-09-26,SPOT: A machine learning model that predicts specific substrates for transport proteins,https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3002807,,,,,,,,,,33163,"Total datapoints = 8,633 + 24,529 = 33,162

Step 1: Positive pairs = 8,633
Step 2: Negative pairs = 24,529
Step 3: Total = 8,633 + 24,529 = 33,162",,,,,,,,"Transport proteins play a crucial role in cellular metabolism and are central to many aspects of molecular biology and medicine. Determining the function of transport proteins experimentally is challenging, as they become unstable when isolated from cell membranes. Machine learning-based predictions could provide an efficient alternative. However, existing methods are limited to predicting a small number of specific substrates or broad transporter classes. These limitations stem partly from using small data sets for model training and a choice of input features that lack sufficient information about the prediction problem. Here, we present SPOT, the first general machine learning model that can successfully predict specific substrates for arbitrary transport proteins, achieving an accuracy above 92% on independent and diverse test data covering widely different transporters and a broad range of metabolites. SPOT uses Transformer Networks to represent transporters and substrates numerically. To overcome the problem of missing negative data for training, it augments a large data set of known transporter-substrate pairs with carefully sampled random molecules as non-substrates. SPOT not only predicts specific transporter-substrate pairs, but also outperforms previously published models designed to predict broad substrate classes for individual transport proteins. We provide a web server and Python function that allows users to explore the substrate scope of arbitrary transporters.",,,Unverified,"Germany,Canada",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
CodonMPNN,Biology,Codon design,"Harvard Medical School,Massachusetts Institute of Technology (MIT)","Hannes Stark, Umesh Padia, Julia Balla, Cameron Diao, George Church",2024-09-25,CodonMPNN for Organism Specific and Codon Optimal Inverse Folding,https://arxiv.org/abs/2409.17265,,,,,,,,,,,,,,,,,,,"Generating protein sequences conditioned on protein structures is an impactful technique for protein engineering. When synthesizing engineered proteins, they are commonly translated into DNA and expressed in an organism such as yeast. One difficulty in this process is that the expression rates can be low due to suboptimal codon sequences for expressing a protein in a host organism. We propose CodonMPNN, which generates a codon sequence conditioned on a protein backbone structure and an organism label. If naturally occurring DNA sequences are close to codon optimality, CodonMPNN could learn to generate codon sequences with higher expression yields than heuristic codon choices for generated amino acid sequences. Experiments show that CodonMPNN retains the performance of previous inverse folding approaches and recovers wild-type codons more frequently than baselines. Furthermore, CodonMPNN has a higher likelihood of generating high-fitness codon sequences than low-fitness codon sequences for the same protein sequence. Code is available at this https URL.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
DFMDock,Biology,Protein-ligand binding affinity prediction,Johns Hopkins University,"Lee-Shin Chu, Sudeep Sarma, Jeffrey J Gray",2024-09-28,Unified Sampling and Ranking for Protein Docking with DFMDock,https://pmc.ncbi.nlm.nih.gov/articles/PMC11463455/,,,,,,,,,,,,,,,,,,,"Diffusion models have shown promise in addressing the protein docking problem. Traditionally, these models are used solely for sampling docked poses, with a separate confidence model for ranking. We introduce DFMDock (Denoising Force Matching Dock), a diffusion model that unifies sampling and ranking within a single framework. DFMDock features two output heads: one for predicting forces and the other for predicting energies. The forces are trained using a denoising force matching objective, while the energy gradients are trained to align with the forces. This design enables our model to sample using the predicted forces and rank poses using the predicted energies, thereby eliminating the need for an additional confidence model. Our approach outperforms the previous diffusion model for protein docking, DiffDock-PP, with a sampling success rate of 44% compared to its 8%, and a Top- 1 ranking success rate of 16% compared to 0% on the Docking Benchmark 5.5 test set. In successful decoy cases, the DFMDock Energy forms a binding funnel similar to the physics-based Rosetta Energy, suggesting that DFMDock can capture the underlying energy landscape.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
ConoDL,Biology,Toxin prediction,"Chongqing University,Ministry of Natural Resources (China)","Menghan Guo, Zengpeng Li, Xuejin Deng, Ding Luo, Jingyi Yang, Yingjun Chen, Weiwei Xue",2024-09-28,ConoDL: A Deep Learning Framework for Rapid Generation and Prediction of Conotoxins,https://www.biorxiv.org/content/10.1101/2024.09.27.614001v1.abstract,,,,1200000000.00,"""ConoGen is a conotoxin generation model that adopts an architecture consistent with the pre-training model ProGen (Figure 2A) 15 . ConoGen is also constructed using a Transformer-based neural network architecture. One major advantage of the Transformer lies in its self-attention mechanism, which enables the model to encode distant signals of information when making sequence predictions, thereby affording it the capability to comprehend complex semantics. Its Transformer architecture consists of 36 layers, each comprising 8 self-attention heads, totaling 1.2 billion trainable parameters to ensure the training and optimization of the model.""",,,,,740001,"ConoGen: 2310 sequences × 40 tokens/sequence = 92,400 tokens
ConoPred: (2310 + 13,941) sequences × 40 tokens/sequence = 650,040 tokens
Total: 92,400 + 650,040 = 742,440 tokens (7.4 × 10⁵)",,,,,,,,"Conotoxins, being small disulfide-rich and bioactive peptides, manifest notable pharmacological potential and find extensive applications. However, the exploration of conotoxins’ vast molecular space using traditional methods is severely limited, necessitating the urgent need of developing novel approaches. Recently, deep learning (DL)-based methods have advanced to the molecular generation of proteins and peptides. Nevertheless, the limited data and the intricate structure of conotoxins constrain the application of deep learning models in the generation of conotoxins. We propose ConoDL, a framework for the generation and prediction of conotoxins, comprising the end-to-end conotoxin generation model (ConoGen) and the conotoxin prediction model (ConoPred). ConoGen employs transfer learning and a large language model (LLM) to tackle the challenges in conotoxin generation. Meanwhile, ConoPred filters artificial conotoxins generated by ConoGen, narrowing down the scope for subsequent research. A comprehensive evaluation of the peptide properties at both sequence and structure levels indicates that the artificial conotoxins generated by ConoDL exhibit a certain degree of similarity to natural conotoxins. Furthermore, ConoDL has generated artificial conotoxins with novel cysteine scaffolds. Therefore, ConoDL may uncover new cysteine scaffolds and conotoxin molecules, facilitating further exploration of the molecular space of conotoxins and the discovery of pharmacologically active variants. The code is available at https://github.com/xueww/ConoDL.The supplementary material and model are available at https://zenodo.org/records/10679280.",,,Unverified,"China,China",,,,,,"Academia,Government",,,"Academia,Government",,,,
DeepREAD,Biology,Protein or nucleotide language model (pLM/nLM),Shape Therapeutics,"Yue Jiang, Lina R. Bagepalli, Bora S. Banjanin, Yiannis A. Savva, Yingxin Cao, Lan Guo, Adrian W. Briggs, Brian Booth, Ronald J. Hause",2024-09-28,Generative Machine Learning of ADAR Substrates for Precise and Efficient RNA Editing,https://www.biorxiv.org/content/10.1101/2024.09.27.613923v1.abstract,,,,,,,,,,162001,"Primary Dataset: 112,000 gRNAs
PolyTarget Library: 50,253 gRNAs
Total = 112,000 + 50,253 = 162,253 gRNAs ≈ 1.62e5",,,,,,,,"Adenosine Deaminase Acting on RNA (ADAR) converts adenosine to inosine within certain double-stranded RNA structures. However, ADAR’s promiscuous editing and poorly understood specificity hinder therapeutic applications. We present an integrated approach combining high-throughput screening (HTS) with generative deep learning to rapidly engineer efficient and specific guide RNAs (gRNAs) to direct ADAR’s activity to any target. Our HTS quantified ADAR-mediated editing across millions of unique gRNA sequences and structures, identifying key determinants of editing outcomes. We leveraged these data to develop DeepREAD (Deep learning for RNA Editing by ADAR Design), a diffusion-based model that elucidates complex design rules to generate novel gRNAs outperforming existing design heuristics. DeepREAD’s gRNAs achieve highly efficient and specific editing, including challenging multi-site edits. We demonstrate DeepREAD’s therapeutic potential by designing gRNAs targeting the MECP2R168X mutation associated with Rett syndrome, achieving both allelic specificity and species cross-reactivity. This approach significantly accelerates the development of ADAR-based RNA therapeutics for diverse genetic diseases.",,,Unverified,United States of America,,,,,,Industry,,,Industry,,,,
PepNet,Biology,"Protein design,Protein or nucleotide language model (pLM/nLM)",Shandong University,"Jiyun Han, Tongxin Kong, Juntao Liu",2024-09-28,PepNet: an interpretable neural network for anti-inflammatory and antimicrobial peptides prediction using a pre-trained protein language model,https://www.nature.com/articles/s42003-024-06911-1,,,,,,,,,,,,,,,,,,,"Identifying anti-inflammatory peptides (AIPs) and antimicrobial peptides (AMPs) is crucial for the discovery of innovative and effective peptide-based therapies targeting inflammation and microbial infections. However, accurate identification of AIPs and AMPs remains a computational challenge mainly due to limited utilization of peptide sequence information. Here, we propose PepNet, an interpretable neural network for predicting both AIPs and AMPs by applying a pre-trained protein language model to fully utilize the peptide sequence information. It first captures the information of residue arrangements and physicochemical properties using a residual dilated convolution block, and then seizes the function-related diverse information by introducing a residual Transformer block to characterize the residue representations generated by a pre-trained protein language model. After training and testing, PepNet demonstrates great superiority over other leading AIP and AMP predictors and shows strong interpretability of its learned peptide representations. A user-friendly web server for PepNet is freely available at http://liulab.top/PepNet/server.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
stFormer,Biology,Spatial Transcriptomics,"Shanghai Jiao Tong University,Chinese Academy of Sciences","Shenghao Cao, Ye Yuan",2024-11-09,"stFormer: a foundation model for spatial transcriptomics
",https://www.biorxiv.org/content/10.1101/2024.09.27.615337v5.abstract,,,,,,,,,,11000000001,"
Minimal datapoint: gene expression value in a cell
Number of genes per cell: 19,264
Number of cells: 580,000
Total datapoints = 580,000 × 19,264 = 11,173,120,000 ≈ 1.1 × 10¹⁰",,,,,,,,"Recent foundation models for single-cell transcriptomics data generate informative, context-aware gene representations. The spatially resolved transcriptomics data offer extra positional insights, yet corresponding gene representation methods that integrate both intracellular and spatial context are still lacking. Here, we introduce a gene representation framework tailored for spatial transcriptomics data. It incorporates ligand genes within the spatial niche into the transformer encoder of single-cell transcriptomics. We further propose a biased cross-attention method to enable the framework to do learning with single-cell resolution on low-resolution, whole-transcriptome Visium data. We implemented our framework on a hybrid Visium dataset derived from two human tissue types with distinct developmental and disease states, and tested on various downstream applications. Compared with the latest foundation model for single-cell transcriptomics, our spatially informed gene representations could identify cell groups and gene functions more accurately, and could predict the perturbation effects of cell-cell ligand-receptor interactions on downstream targets.",,,Unverified,"China,China",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
Loop-Diffusion,Biology,Protein design,University of Washington,"Kevin Borisiak, Gian Marco Visani, Armita Nourmohammad
",2024-09-26,Loop-Diffusion: an equivariant diffusion model for designing and scoring protein loops,https://arxiv.org/abs/2409.18201,,,,,,,,,,433001,"433,000 total datapoints = 433k atomic neighborhoods from 20,000 protein structures
Final result: 4.33e5 datapoints",,,,,,,,"Predicting protein functional characteristics from structure remains a central problem in protein science, with broad implications from understanding the mechanisms of disease to designing novel therapeutics. Unfortunately, current machine learning methods are limited by scarce and biased experimental data, and physics-based methods are either too slow to be useful, or too simplified to be accurate. In this work, we present Loop-Diffusion, an energy based diffusion model which leverages a dataset of general protein loops from the entire protein universe to learn an energy function that generalizes to functional prediction tasks. We evaluate Loop-Diffusion's performance on scoring TCR-pMHC interfaces and demonstrate state-of-the-art results in recognizing binding-enhancing mutations.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
FragLlama,Biology,Drug discovery,YDS Pharmatech,"Jian Shen, Shengmin Zhou, Xing Che",2024-09-30,FragLlama: Next-fragment prediction for molecular design,https://www.biorxiv.org/content/10.1101/2024.09.28.615626v1.abstract,,,,,,,,,,70000000001,"70 billion tokens = 7.0e10 datapoints

Calculation: 70,000,000,000 = 7.0e10",,,,,,,,"The emergence of ChatGPT has drawn significant attention to Large Language Models (LLMs) due to their impressive performance. While LLMs primarily focus on next token/word prediction, we apply this principle to molecular design by reframing the task as predicting the next token/fragment. We present FragLlama, a large language model trained for molecular design, featuring custom tokens that represent molecular fragments and functional groups. The model is for generating molecules given one or two fragments, for application scenarios like general hit-to-lead and lead optimization stage drug design, PROTAC linker design; mapping to commonly used drug design strategies like fragment growing and scaffold hopping. In the pre-training stage, we adapted the Llama 3 architecture to create FragLlama, training it to learn conditional probabilities of these fragment-level tokens. The subsequent alignment stage employed fine-tuning to guide the model towards generating molecules with desired properties. The effectiveness of FragLlama is demonstrated through its applications in designing molecular glue libraries, PROTAC linkers and EGFR binders. FragLlama demonstrates proficiency in reproducing expert-level designs while also exploring novel and promising chemical spaces, highlighting its potential to augment the capabilities of medicinal chemists in drug design.",,,Unverified,United States of America,,,,,,Industry,,,Industry,,,,
EnzymeFlow,Biology,Protein design,"McGill University,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Hong Kong University of Science and Technology,University of Washington,Microsoft Research,DeepMind,Shanghai Jiao Tong University,University of Montreal / Université de Montréal","Chenqing Hua, Yong Liu, Dinghuai Zhang, Odin Zhang, Sitao Luan, Kevin K. Yang, Guy Wolf, Doina Precup, Shuangjia Zheng",2024-10-01,EnzymeFlow: Reaction-conditioned Enzyme Catalytic Pocket Design,https://arxiv.org/abs/2410.00327,,,,,,,,,,53484,"53,483 enzyme-reaction pairs were used to train EnzymeFlow after homology filtering

Initial dataset: 328,192
Raw dataset before filtering: 232,520
Final filtered dataset: 53,483",,,,,,,,"Enzyme design is a critical area in biotechnology, with applications ranging from drug development to synthetic biology. Traditional methods for enzyme function prediction or protein binding pocket design often fall short in capturing the dynamic and complex nature of enzyme-substrate interactions, particularly in catalytic processes. To address the challenges, we introduce EnzymeFlow, a generative model that employs flow matching with hierarchical pre-training and enzyme-reaction co-evolution to generate catalytic pockets for specific substrates and catalytic reactions. Additionally, we introduce a large-scale, curated, and validated dataset of enzyme-reaction pairs, specifically designed for the catalytic pocket generation task, comprising a total of 328, 192 pairs. By incorporating evolutionary dynamics and reaction-specific adaptations, EnzymeFlow becomes a powerful model for designing enzyme pockets, which is capable of catalyzing a wide range of biochemical reactions. Experiments on the new dataset demonstrate the model’s effectiveness in designing high-quality, functional enzyme catalytic pockets, paving the way for advancements in enzyme engineering and synthetic biology. We provide EnzymeFlow code at https://github.com/WillHua127/EnzymeFlow with notebook demonstration at https://github.com/WillHua127/ EnzymeFlow/blob/main/enzymeflow_demo.ipynb.",,,Unverified,"Canada,Canada,Hong Kong,China,United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland,China,Canada",,,,,,"Academia,Academia,Academia,Academia,Industry,Industry,Academia,Academia",,,"Academia,Academia,Academia,Academia,Industry,Industry,Academia,Academia",,,,
PocketFlow,Biology,"Protein design,Drug discovery","University of Science and Technology of China,State Key Laboratory of Cognitive Intelligence,Harvard University","Zaixi Zhang, Marinka Zitnik, Qi Liu",2024-09-29,Generalized Protein Pocket Generation with Prior-Informed Flow Matching,https://arxiv.org/abs/2409.19520,,,,,,10000000000000000000.00,"1. Hardware: 1x Tesla A100 GPU (3.12e14 FLOP/s)
2. Training duration: Estimated ~3 hours (10,940 seconds)
   Calculation: 140k samples / 256 batch size = 547 steps/epoch * 20 epochs * 1s/step
3. Utilization: 100% assumed (no explicit rate mentioned)
4. Total FLOPs = 3.12e14 FLOP/s * 1 GPU * 10,940 seconds = 3.41e18 FLOPs (rounded to 1e19)",,,21000001,"CrossDocked: 100,000 complexes × (100 residues + 50 atoms) = 15,000,000
MOAD: 40,000 pairs × (100 residues + 50 atoms) = 6,000,000
Total: 15,000,000 + 6,000,000 = 21,000,000 datapoints = 2.1 × 10⁷",,,,,,,,"Designing ligand-binding proteins, such as enzymes and biosensors, is essential in bioengineering and protein biology. One critical step in this process involves designing protein pockets, the protein interface binding with the ligand. Current approaches to pocket generation often suffer from time-intensive physical computations or template-based methods, as well as compromised generation quality due to the overlooking of domain knowledge. To tackle these challenges, we propose PocketFlow, a generative model that incorporates protein-ligand interaction priors based on flow matching. During training, PocketFlow learns to model key types of protein-ligand interactions, such as hydrogen bonds. In the sampling, PocketFlow leverages multi-granularity guidance (overall binding affinity and interaction geometry constraints) to facilitate generating high-affinity and valid pockets. Extensive experiments show that PocketFlow outperforms baselines on multiple benchmarks, e.g., achieving an average improvement of 1.29 in Vina Score and 0.05 in scRMSD. Moreover, modeling interactions make PocketFlow a generalized generative model across multiple ligand modalities, including small molecules, peptides, and RNA.",,,Unverified,"China,China,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
FlexSBDD,Biology,Drug discovery,"University of Science and Technology of China,State Key Laboratory of Cognitive Intelligence,Princeton University","Zaixi Zhang, Mengdi Wang, Qi Liu",2024-09-29,FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling,https://arxiv.org/abs/2409.19645,,,,,,16000000000000008000.00,"1. Hardware: 1x NVIDIA A100 PCIe GPU (3.12e14 FP16 Tensor FLOP/s)
2. Training duration: 36 hours (directly provided) = 129,600 seconds
3. Utilization: 40% (assumed)
4. Calculation: 3.12e14 FLOP/s × 1 GPU × 129,600s × 0.4 = 1.6e19 FLOPs",,,1400001,"Original pairs: 40,000 + 100,000 = 140,000
Data augmentation: 140,000 * 10 = 1,400,000
Training iterations: 500,000 * 4 = 2,000,000
Final unique datapoints: 1,400,000",,,,,,,,"Structure-based drug design (SBDD), which aims to generate 3D ligand molecules binding to target proteins, is a fundamental task in drug discovery. Existing SBDD methods typically treat protein as rigid and neglect protein structural change when binding with ligand molecules, leading to a big gap with real-world scenarios and inferior generation qualities (e.g., many steric clashes). To bridge the gap, we propose FlexSBDD, a deep generative model capable of accurately modeling the flexible protein-ligand complex structure for ligand molecule generation. FlexSBDD adopts an efficient flow matching framework and leverages E(3)-equivariant network with scalar-vector dual representation to model dynamic structural changes. Moreover, novel data augmentation schemes based on structure relaxation/sidechain repacking are adopted to boost performance. Extensive experiments demonstrate that FlexSBDD achieves state-of-the-art performance in generating high-affinity molecules and effectively modeling the protein's conformation change to increase favorable protein-ligand interactions (e.g., Hydrogen bonds) and decrease steric clashes.",,,Unverified,"China,China,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,Hardware
BindCraft,Biology,Protein design,"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),University of Zurich,University of Lausanne,Massachusetts Institute of Technology (MIT),Visterra Inc,Swiss Federal Institute of Technology","Martin Pacesa, Lennart Nickel, Joseph Schmidt, Ekaterina Pyatova, Christian Schellhaas, Lucas Kissling, Ana Alcaraz-Serna, Yehlin Cho, Kourosh H. Ghamary, Laura Vinué, Brahm J. Yachnin, Andrew M. Wollacott, Stephen Buckley, Sandrine Georgeon, Casper A. Goverde, Georgios N. Hatzopoulos, Pierre Gönczy, Yannick D. Muller, Gerald Schwank, Sergey Ovchinnikov, Bruno E. Correia",2024-10-01,BindCraft: one-shot design of functional protein binders,https://www.biorxiv.org/content/10.1101/2024.09.30.615802v1.abstract,,,,,,,,,,,,,,,,,,,"Protein–protein interactions (PPIs) are at the core of all key biological processes. However, the complexity of the structural features that determine PPIs makes their design challenging. We present BindCraft, an open-source and automated pipeline for de novo protein binder design with experimental success rates of 10-100%. BindCraft leverages the trained deep learning weights of AlphaFold21 to generate nanomolar binders without the need for high-throughput screening or experimental optimization, even in the absence of known binding sites. We successfully designed binders against a diverse set of challenging targets, including cell-surface receptors, common allergens, de novo designed proteins, and multi-domain nucleases, such as CRISPR-Cas9. We showcase their functional and therapeutic potential by demonstrating that designed binders can reduce IgE binding to birch allergen in patient-derived samples. This work represents a significant advancement towards a “one design-one binder” approach in computational design, with immense potential in therapeutics, diagnostics, and biotechnology.

",,,Unverified,"Switzerland,Switzerland,Switzerland,United States of America,United States of America,Switzerland",,,,,,"Academia,Academia,Academia,Academia,Industry,Academia",,,"Academia,Academia,Academia,Academia,Industry,Academia",,,,
PlasmidGPT,Biology,Plasmid Design,Harvard University,"Bin Shao
",2024-10-01,PlasmidGPT: a generative framework for plasmid design and annotation,https://www.biorxiv.org/content/10.1101/2024.09.30.615762v1.abstract,,,,110000000.00,"""The model consists of 12 230 layers with a dimension of 512, with 8 attention heads and a total of 110M parameters""",100000000000000.00,Arithmetic operation direct count using 110000000 parameters and 153000 training examples.,,,153000,"""We introduce PlasmidGPT, a generative language model pretrained on 153k engineered  8 plasmid sequences from Addgene.""",,,,NVIDIA A100,1,,,"We introduce PlasmidGPT, a generative language model pretrained on 153k engineered plasmid sequences from Addgene. PlasmidGPT generates de novo sequences that share similar characteristics with engineered plasmids but show low sequence identity to the training data. We demonstrate its ability to generate plasmids in a controlled manner based on the input sequence or specific design constraint. Moreover, our model learns informative embeddings of both engineered and natural plasmids, allowing for efficient prediction of a wide range of sequence-related attributes.

",Open weights (unrestricted),,Unverified,United States of America,,,,,,Academia,,,Academia,,,881.1366228024874,Operation counting
Uni-RNA-L-24,Biology,Protein or nucleotide language model (pLM/nLM),DP Technology,"Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen",2023-07-12,Uni-RNA: Universal Pre-Trained Models Revolutionize RNA Research,https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1.abstract,20.00,,,400000000.00,Table 8: Model architecture parameters of different Uni-RNA models,,"""We trained our proposed network on 128 A100""",,,500000000,Table 8: Model architecture parameters of different Uni-RNA models,,,,,,,,"RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of life’s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",,,Unverified,China,,,,,,Industry,,,Industry,,,,
Uni-RNA-L12,Biology,Protein or nucleotide language model (pLM/nLM),DP Technology,"Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen",2023-07-12,UNI-RNA: UNIVERSAL PRE-TRAINED MODELS REVOLUTIONIZE RNA RESEARCH,https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1.abstract,20.00,,,85000000.00,Table 8: Model architecture parameters of different Uni-RNA models,,"""We trained our proposed network on 128 A100""",,,100000000,Table 8: Model architecture parameters of different Uni-RNA models,,,,,,,,"RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of life’s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",,,Unverified,China,,,,,,Industry,,,Industry,,,,
Uni-RNA-L16,Biology,Protein or nucleotide language model (pLM/nLM),DP Technology,"Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen",2023-07-12,UNI-RNA: UNIVERSAL PRE-TRAINED MODELS REVOLUTIONIZE RNA RESEARCH,https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1.abstract,20.00,,,169000000.00,Table 8: Model architecture parameters of different Uni-RNA models,,"""We trained our proposed network on 128 A100""",,,500000000,Table 8: Model architecture parameters of different Uni-RNA models,,,,,,,,"RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of life’s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",,,Unverified,China,,,,,,Industry,,,Industry,,,,
MPNNsol,Biology,Protein generation,"Ecole Polytechnique F´ed´erale de Lausanne (EPFL),University at Buffalo,University of Washington,Massachusetts Institute of Technology (MIT)","Casper A. Goverde, Martin Pacesa, Nicolas Goldbach, Lars J. Dornfeld, Petra E. M. Balbi, Sandrine Georgeon, Stéphane Rosset, Srajan Kapoor, Jagrity Choudhury, Justas Dauparas, Christian Schellhaas, Simon Kozlov, David Baker, Sergey Ovchinnikov, Alex J. Vecchio, Bruno E. Correia",2024-06-19,Computational design of soluble and functional membrane protein analogues,https://www.nature.com/articles/s41586-024-07601-y#Sec30,,,,,,,,PDB (Protein Data Bank),"The MPNNsol model was trained on protein assemblies in the PDB 
(as of 2 August 2021) determined by X-ray crystallography or cryo-EM to a resolution of better than 3.5 Å and with fewer than 10,000 residue",,,,,,,,,,"De novo design of complex protein folds using solely computational means remains a substantial challenge1. Here we use a robust deep learning pipeline to design complex folds and soluble analogues of integral membrane proteins. Unique membrane topologies, such as those from G-protein-coupled receptors2, are not found in the soluble proteome, and we demonstrate that their structural features can be recapitulated in solution. Biophysical analyses demonstrate the high thermal stability of the designs, and experimental structures show remarkable design accuracy. The soluble analogues were functionalized with native structural motifs, as a proof of concept for bringing membrane protein functions to the soluble proteome, potentially enabling new approaches in drug discovery. In summary, we have designed complex protein topologies and enriched them with functionalities from membrane proteins, with high experimental success rates, leading to a de facto expansion of the functional soluble fold space.",Open weights (unrestricted),,Unknown,"Switzerland,United States of America,United States of America,United States of America","ProteinMPNN,AlphaFold 2",,,,,"Academia,Academia,Academia,Academia",Unreleased,"https://github.com/dauparas/ProteinMPNN/tree/main/soluble_model_weights

MIT license","Academia,Academia,Academia,Academia",,,,
BiosimDock,"Medicine,Biology","Drug discovery,Protein-ligand binding affinity prediction",DeepOrigin,"Garik Petrosyan, Garegin Papoian, Natalie Ma, Tigran Abramyan",2024-05-08,We Spill the Beans: Deep Origin's AI- and Physics-Based Models for Drug Discovery,https://www.deeporigin.com/blog/we-spill-the-beans-deep-origins-ai-and-physics-based-models-for-drug-discovery,,,,,,,,PDBbind,,,,,,,,,,,"We outperform other models on accuracy of binding affinity and binding pose prediction
Docking and virtual screening tools are meaningful only if they are able to make fast, accurate, and useful predictions. A good model can filter true binders from a broad pool of potential molecules, including false positives that may be similar in chemical properties. In the hit identification stage of drug discovery it can give drug hunting teams a chemically-diverse set of potential hit molecules to evaluate with experimental assays or further computational analysis, helping them narrow the path to a lead candidate. In contrast, a bad model returns many false positives, costing a team money and effort – and potentially leading drug hunters off-course for months or years.

To benchmark, we tested the BiosimDock model on the PDBbind core dataset and the DEKOIS 2.0 dataset.1, 2, 3, 4 The PDBbind core dataset contains 285 experimental structures of protein-ligand bound complexes across different protein classes. It remains a standard due to widespread use in benchmarking, facilitating comparison between models. It also enables accuracy prediction based on binding poses. DEKOIS 2.0 (Demanding Evaluation Kits for Objective In silico Screening) is an extensively-curated dataset of 81 targets across protein classes, including proteases, kinases, transferases, oxido-reductases, nuclear receptors, and hydrolases. Each target has an accompanying library of true binders and decoys, which have similar physical and chemical properties but do not interact with the target protein. This enables rigorous benchmarking of models for enrichment of true binders over false positives.4, 5, 6 ",Hosted access (no API),,Unknown,"Armenia,United States of America",,,,,,Industry,Unreleased,,Industry,,,,
BaseFold,Biology,Protein folding prediction,Basecamp Research,"Geraldene Munsamy, Tanggis Bohnuud, Philipp Lorenz",2024-03-06,IMPROVING ALPHAFOLD2 PERFORMANCE WITH A GLOBAL METAGENOMIC & BIOLOGICAL DATA SUPPLY CHAIN,https://www.biorxiv.org/content/10.1101/2024.03.06.583325v1,,,,,,,,BRD (Basecamp Research Data),,300000000001,"Number of Sequences (1 x 10⁹) × Average Residues per Sequence (300) = 3 × 10¹¹ datapoints

Final estimate: 3 × 10¹¹",,,,NVIDIA A100,,,,"Scaling laws suggest that more than a trillion species inhabit our planet but only a miniscule and unrepresentative fraction (less than 0.00001%) have been studied or sequenced to date. Deep learning models, including those applied to tasks in the life sciences, depend on the quality and size of training or reference datasets. Given the large knowledge gap we experience when it comes to life on earth, we present a data-centric approach to improving deep learning models in Biology: We built partnerships with nature parks and biodiversity stakeholders across 5 continents covering 50% of global biomes, establishing a global metagenomics and biological data supply chain. With higher protein sequence diversity captured in this dataset compared to existing public data, we apply this data advantage to the protein folding problem by MSA supplementation during inference of AlphaFold2. Our model, BaseFold, exceeds traditional AlphaFold2 performance across targets from the CASP15 and CAMEO, 60% of which show improved pLDDT scores and RMSD values being reduced by up to 80%. On top of this, the improved quality of the predicted structures can yield better docking results. By sharing benefits with the stakeholders this data originates from, we present a way of simultaneously improving deep learning models for biology and incentivising protection of our planet’s biodiversity.",,,Unknown,United Kingdom of Great Britain and Northern Ireland,AlphaFold 2,,,,,,,,,,,,
HiFi - NN,Biology,Enzyme function prediction,"Basecamp Research,Technical University of Munich,Molecular Institute of Biology,Microsoft Research",Gavin Ayres,2023-12-19,Breakthrough in Functional Annotation with HiFi-NN,https://developer.nvidia.com/blog/breakthrough-in-functional-annotation-with-hifi-nn/,,,,3000000.00,"""The model boasts over 3M parameters.""",,,,"""Small representativity, covering only 0.001% of life on earth
No consistent metadata
Lack of stakeholder consent and engagement before data collection
Basecamp opted to develop its proprietary biological data resource through biodiversity partnerships with nature parks across five continents and 23 countries. They sent their scientists on worldwide expeditions to discover new genomes, enzymes, and biological relationships from the most extreme and extraordinary biomes. 

In under two years, they created BaseGraph, the largest knowledge graph of natural biodiversity, containing over 5.5B relationships with a genomic context exceeding 70 kilobases per protein. Their extensive long-read sequencing is complemented by comprehensive metadata collection, enabling them to link proteins of interest to specific reactions and desired process conditions. """,,"""the model retrained with 3M selected, environmentally diverse sequences from Basecamp Research’s BaseGraph.""",,,,NVIDIA A100,8,,,"The accurate computational annotation of protein sequences with enzymatic function, especially those that are part of the functional and taxonomic dark matter, remains a fundamental challenge in bioinformatics. Here, we present HiFi-NN, (Hierarchically-Finetuned Nearest Neighbor search) which annotates protein sequences to the 4th level of EC (enzyme commission) number with greater precision and recall than all existing deep learning methods. HiFi-NN is a hierarchically-finetuned deep learning method based on a combination of semi-supervised representation learning and a nearest neighbours classifier. Furthermore, we show that this method can correctly identify the EC number of a given sequence to identities below 40%, where the current state of the art annotation tool, BLASTp, cannot. We proceed to improve the representations learned by increasing the diversity of the training set, not just in sequence space but also in terms of the environment the sequences have been sampled from. Finally, we use HiFi-NN to annotate a portion of microbial dark matter sequences in the MGnify database.",,,Confident,"United Kingdom of Great Britain and Northern Ireland,Germany,Spain,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,"Academia,Academia,Industry",,,"Academia,Academia,Industry",,,7067.9782862082075,
MoLeR,Biology,Drug discovery,"Microsoft Research,Novartis","Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stiefl, Marwin Segler, Marc Brockschmidt",2024-05-12,Learning to Extend Molecular Scaffolds with Structural Motifs,https://arxiv.org/abs/2103.03864,,,,,,2106259200000000000.00,"training speed 95.2 molecules/sec (Table 1)

1.5*10^6 molecules / 95.2 = 15756 seconds = 4 hours (1 epoch)

""a few GPU days"" - let's assume (!) it was 10 GPU-days 

10*24*3600*8126000000000*0.3 = 2.1062592e+18",GuacaMol,"We use training data from GuacaMol (Brown et al., 2019), which released a curated set of ≈1.5M drug-like molecules, divided into train, validation and test sets.",,,,,"""Training MoLeR requires first preprocessing the data, which takes up to one CPU day for GuacaMol, followed by training itself, which takes up to a few GPU days.""",NVIDIA Tesla K80,,,,"Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. Here, we propose MoLeR, a graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. Our experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. Furthermore, we show the influence of a number of seemingly minor design choices on the overall performance.",Open weights (unrestricted),,Speculative,"United States of America,United Kingdom of Great Britain and Northern Ireland,Switzerland",,,,,,"Industry,Industry",Open source,https://github.com/microsoft/molecule-generation,"Industry,Industry",,,,Hardware
EGNN,Biology,Protein stability prediction,InstaDeep,"Sebastien Boyer, Sam Money-Kyrle, Oliver Bent",2023-05-30,"Predicting protein stability changes under  
multiple amino acid substitutions using equivariant graph neural networks",https://arxiv.org/abs/2305.19801,4.00,,,,,,,,,,,,,,,,,,,,,Unverified,United Kingdom of Great Britain and Northern Ireland,,,,,,Industry,,,Industry,,,,
EPInformer,Biology,Gene expression profile generation,"The University of Hong Kong,Harvard Medical School","Jiecong Lin, Ruibang Luo, Luca Pinello",2024-08-01,EPInformer: a scalable deep learning framework for gene expression prediction by integrating promoter-enhancer sequences with multimodal epigenomic data,https://www.biorxiv.org/content/10.1101/2024.08.01.606099v1,2.00,,,,,,,,,,,,,,,,,,"Transcriptional regulation, critical for cellular differentiation and adaptation to environmental changes, involves coordinated interactions among DNA sequences, regulatory proteins, and chromatin architecture. Despite extensive data from consortia like ENCODE, understanding the dynamics of cis-regulatory elements (CREs) in gene expression remains challenging. Deep learning is a powerful tool for learning gene expression and epigenomic signals from DNA sequences, exhibiting superior performance compared to conventional machine learning approaches. However, even the most advanced deep learning-based methods may fall short in capturing the regulatory effects of distal elements such as enhancers, limiting their predictive accuracy. In addition, these methods may require significant resources to train or to adapt to newly generated data. To address these challenges, we present EPInformer, a scalable deep-learning framework for predicting gene expression by integrating promoter-enhancer interactions with their sequences, epigenomic signals, and chromatin contacts. Our model outperforms existing gene expression prediction models in rigorous cross-chromosome validation, accurately recapitulates enhancer-gene interactions validated by CRISPR perturbation experiments, and identifies crucial transcription factor motifs within regulatory sequences. EPInformer is available as open-source software at https://github.com/pinellolab/EPInformer.",,,Unverified,"Hong Kong,China,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
BTFBS,Biology,Protein-DNA binding prediction,Nanjing Agricultural University,"Bingbing Jin, Song Liang, Xiaoqian Liu, Rui Zhang, Yun Zhu, Yuanyuan Chen, Guangjin Liu, Tao Yang
",2024-09-22,BTFBS: binding-prediction of bacterial transcription factors and binding sites based on deep learning,https://www.biorxiv.org/content/10.1101/2024.09.19.613986v1,,,,,,,,,,,,,,,,,,,"Background The binding of transcription factors (TFs) to TF-binding sites plays a vital role in the process of regulating gene expression and evolution. With the development of machine learning and deep learning, some successes have been achieved in predicting transcription factors and binding sites. Then a natural question arises: for a given transcription factor and a binding site, do they bind? This is the main motivation of this work.
Results In this paper, we develop a model BTFBS, which predicts whether the bacterial transcription factors and binding sites combine or not. The model takes both the amino acid sequences of bacterial transcription factors and the nucleotide sequences of binding sites as inputs, and extracts features through convolutional neural network and MultiheadAttention.
For the model inputs, we use two negative sample sampling methods: RS and EE. On the test dataset of RS, the accuracy, sensitivity, specificity, F1-score and MCC of BTFBS are 0.91446, 0.89746, 0.93134, 0.91264 and 0.82946, respectively. And on the test dataset of EE, the accuracy, sensitivity, specificity, F1-score and MCC of BTFBS are 0.87868, 0.89354, 0.86394, 0.87996 and 0.75796, respectively. Meanwhile, our findings indicate that the optimal approach for obtaining negative samples in the context of bacterial research is to utilize the whole genome sequences of the corresponding bacteria, as opposed to the shuffling method.
Conclusions The above results on the test dataset have shown that the proposed BTFBS model has a good performance in predicting the combination of bacterial transcription factors and their binding sites and provides an experimental guide. BTFBS is publicly available at https://github.com/Vceternal/BTFBS.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
GITIII,Biology,Cell-cell interaction prediction,Yale School of Public Health,Xiao Xiao,2024-08-22,Investigation of pair-wise single-cell interactions by statistically interpreting spatial cell state correlation learned by self-supervised graph inductive bias transformer,https://www.biorxiv.org/content/10.1101/2024.08.21.608964v1,0.00,,,,,,,,,,,,,,,,,,"Image-based spatial transcriptomics (ST) offers spatial gene expression profile at the single-cell resolution and provides information to understand intercellular communication that is critical for maintaining tissue development and organ function. Disruption of normal cell-cell interactions (CCI) can lead to disease onset and progression. Current CCI analysis methods face several limitations, including subjection to the number of measured ligand-receptor genes in image-based spatial transcriptomics, limited graph encoding power, inadequate use of spatial information, and low interpretability. Here, we present GITIII, an interpretable self-supervised graph transformerbased language model that treats cells as words (nodes) and their cell neighborhood as a sentence to explore the communications among cells. Enhanced by multilayer perceptron-based distance scaler, physics-informed attention mechanism, and a state-of-the-art, expressive, and lightweight graph transformer model, GITIII infers CCI by investigating how the state of a cell is influenced by the spatial organization, ligand expression, cell types and states of neighboring cells. With its interpretable architecture, GITIII can be used to understand how the sender cell influences target genes in the receiver cell, visualize the spatial pattern and utility of CCI, identify significant CCI networks, perform CCI-informed cell subtyping, and compare CCI strength between disease groups. Applications to four ST datasets from several species, organs, and platforms, GITIII effectively identified and quantitatively interpreted key CCI patterns driving within-sample heterogeneity and disease progression, thus improving our understanding of brain structures, tumor microenvironments, and the interplay among different cell types responding to neighboring CCIs.",,,Unverified,,,,,,,,,,,,,,
PrePR-CT,Biology,Transcriptomic prediction,"King Abdullah University of Science and Technology (KAUST),Karolinska Institute","Reem Alsulami, Robert Lehmann, Sumeer A. Khan, Vincenzo Lagani, David G´omez-Cabrero, Narsis A. Kiani, Jesper Tegner",2024-07-24,"PrePR-CT: Predicting Perturbation Responses in Unseen Cell Types Using Cell-Type-Specific Graphs
",https://www.biorxiv.org/content/10.1101/2024.07.24.604816v1,0.00,,,,,,,,,,,,,,,,,,"Predicting the transcriptional response of chemical perturbations is crucial to understanding gene function and developing drug candidates, promising a streamlined drug development process. Single-cell sequencing has provided an ideal data basis for training machine learning models for this task. Recent advances in deep learning have led to significant improvements in predictions of chemical as well as genetic perturbations at the single cell level. Experiments have shown that different cell types exhibit distinct transcriptional patterns and responses to perturbation. This poses a fundamental problem for predicting transcriptional responses of drugs or cell types outside the training data. Accordingly, existing methods lack cell-type-specific modeling or do not explicitly provide an interpretable mechanism for the gene features. In this study, we introduce a novel approach that employs a network representation of various cell types as an inductive bias, improving prediction performance in scenarios with limited data while acknowledging cellular differences. We applied our framework to four small-scale single-cell perturbation datasets and one large-scale screening experiment, demonstrating that this representation can inherently generalize to previously unseen cell types. Furthermore, our method outperforms the state-of-the-art methods in predicting the post-perturbation response in unobserved cell types.",,,Unverified,"Saudi Arabia,Sweden",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
sgRNAGen,Biology,RNA design,Beijing Institute of Technology,"Yan Xia, Zeyu Liang, Xiaowen Du, Dengtian Cao, Jing Li, Lichao Sun, Yi-Xin Huo, Shuyuan Guo",2024-05-31,Design nonrepetitive and diverse activity single-guide RNA by deep learning,https://www.biorxiv.org/content/10.1101/2024.05.30.596019v1,,,,,,,,,,,,,,,,,,,"Multiplex and precise control of the gene expression based on CRISPR/Cas9 is important to metabolic regulation in synthetic biology. However, employing single guide RNAs (sgRNAs) that possess repetitive DNA sequences and exhibit uniform activity could detrimentally affect the editing process, undermining both its stability and regulatory potential. In this study, we developed a deep generative model based on a decoder-only Transformer architecture (sgRNAGen) for the de novo generation of a series of nonrepetitive and diverse sgRNAs with activity. To assess the quality of sgRNAs generated by sgRNAGen, we evaluated their activity by targeting essential genes, with the results indicating that 98% of the generated sgRNAs were active in Bacillus subtilis. The generated sgRNAs were further validated for applications in single-gene editing, large fragment knockouts, and multiplex editing. Notably, the efficiency of knocking out long fragments up to 169.5 kb reached 100%, and targeting multiple sites allowed for the creation of strains with various combinations of mutations in a single editing. Furthermore, we developed a CRISPRi system utilizing the designed sgRNAs to regulate gene expression with desired strength and high precision. SgRNAGen offers a method for devising nonrepetitive and diverse activity sgRNAs, enhancing metabolic control and advancing applications within synthetic biology.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
DRGN-AI,Biology,Cryo-EM image reconstruction,"Stanford University,SLAC National Laboratory,Princeton University,Columbia University","Axel Levy, Michal Grzadkowski, Frédéric Poitevin, Francesca Vallese, Oliver Biggs Clarke, Gordon Wetzstein, Ellen D. Zhong",2024-06-02,Revealing biomolecular structure and motion with neural ab initio cryo-EM reconstruction,https://www.biorxiv.org/content/10.1101/2024.05.30.596729v1,,,,,,,,,,,,,,,,,,,"Proteins and other biomolecules form dynamic macromolecular machines that are tightly orchestrated to move, bind, and perform chemistry. Cryo-electron microscopy (cryo-EM) can access the intrinsic heterogeneity of these complexes and is therefore a key tool for understanding mechanism and function. However, 3D reconstruction of the resulting imaging data presents a challenging computational problem, especially without any starting information, a setting termed ab initio reconstruction. Here, we introduce a method, DRGN-AI, for ab initio heterogeneous cryo-EM reconstruction. With a two-step hybrid approach combining search and gradient-based optimization, DRGN-AI can reconstruct dynamic protein complexes from scratch without input poses or initial models. Using DRGN-AI, we reconstruct the compositional and conformational variability contained in a variety of benchmark datasets, process an unfiltered dataset of the DSL1/SNARE complex fully ab initio, and reveal a new “supercomplex” state of the human erythrocyte ankyrin-1 complex. With this expressive and scalable model for structure determination, we hope to unlock the full potential of cryo-EM as a high-throughput tool for structural biology and discovery.",,,Unverified,"United States of America,United States of America,United States of America",,,,,,"Academia,Academia,Academia",,,"Academia,Academia,Academia",,,,
DualNetGO,Biology,Protein function prediction,Hong Kong University of Science and Technology,"Zhuoyang Chen,  Qiong Luo",2024-07-03,DualNetGO: A Dual Network Model for Protein Function Prediction via Effective Feature Selection,https://www.biorxiv.org/content/10.1101/2023.11.29.569192v2,0.00,,,,,,,,,,,,,,,,,,"Motivation Protein-protein Interaction (PPI) networks are crucial for automatically annotating protein functions. As multiple PPI networks exist for the same set of proteins that capture properties from different aspects, it is a challenging task to effectively utilize these heterogeneous networks. Recently, several deep learning models have combined PPI networks from all evidence, or concatenated all graph embeddings for protein function prediction. However, the lack of a judicious selection procedure prevents the effective harness of information from different PPI networks, as these networks vary in densities, structures, and noise levels. Consequently, combining protein features indiscriminately could increase the noise level, leading to decreased model performance.

Results We develop DualNetGO, a dual network model comprised of a classifier and a selector, to predict protein functions by effectively selecting features from different sources including graph embeddings of PPI networks, protein domain and subcellular location information. Evaluation of DualNetGO on human and mouse datasets in comparison with other network-based models show at least 4.5%, 6.2% and 14.2% improvement on Fmax in BP, MF and CC Gene Ontology categories respectively for human, and 3.3%, 10.6% and 7.7% improvement on Fmax for mouse. We demonstrate the generalization capability of our model by training and testing on the CAFA3 data, and show its versatility by incorporating Esm2 embeddings. We further show that our model is insensitive to the choice of graph embedding method and is time- and memory-saving. These results demonstrate that combining a subset of features including PPI networks and protein attributes selected by our model is more effective in utilizing PPI network information than only using one kind of or concatenating graph embeddings from all kinds of PPI networks.

Availability and implementation The source code of DualNetGO and some of the experiment data are available at: https://github.com/georgedashen/DualNetGO.",,,Unverified,"Hong Kong,China",,,,,,Academia,,,Academia,,,,
EvoMIL,Biology,Virus-host association prediction,"University of Glasgow,Cancer Research UK Beatson Institute","Dan Liu, Francesca Young, David L Robertson, Ke Yuan",2023-04-08,Prediction of virus-host association using protein language models and multiple instance learning,https://www.biorxiv.org/content/10.1101/2023.04.07.536023v1,2.00,,,,,,,,,,,,,,,,,,"Predicting virus-host association is essential to understand how viruses interact with host species, and discovering new therapeutics for viral diseases across humans and animals. Currently, the host of the majority of viruses is unknown. Here, we introduce EvoMIL, a deep learning method that predicts virus-host association at the species level from viral sequence only. The method combines a pre-trained large protein language model and attention-based multiple instance learning to allow protein-orientated predictions. Our results show that protein embeddings capture stronger predictive signals than traditional handcrafted features, including amino acids and DNA k-mers, and physio-chemical properties. EvoMIL binary classifiers achieve AUC values of over 0.95 for all prokaryotic and nearly 0.8 for almost all eukaryotic hosts. In multi-host prediction tasks, EvoMIL achieved median performance improvements of 8.6% in prokaryotic hosts and 1.8% in eukaryotic hosts. Furthermore, EvoMIL estimates the importance of single proteins in the prediction and maps them to an embedding landscape of all viral proteins, where proteins with similar functions are distinctly clustered together.

Author summary Being able to predict which viruses can infect which hosts, and identifying the specific proteins that are involved in these interactions, is crucial for understanding viral diseases and developing more effective treatments. Traditional methods for predicting these interactions rely on handcrafted common features among proteins, overlooking the importance of single proteins. We have developed a new method that combines a protein language model and multiple instance learning to allow host prediction directly from protein sequences, without the need to extract handcrafted features. This method significantly improved multiple host association accuracy and revealed the key proteins involved in virus-host interactions.",,,Unverified,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,,,Academia,,,Academia,,,,
CLR_ESP,Biology,Enzyme substrate pair prediction,,"Zhenjiao Du, Weiming Fu, Xiaolong Guo, Doina Caragea, Yonghui Li",2024-08-16,CLR_ESP: Improved enzyme-substrate pair prediction using contrastive learning,https://www.biorxiv.org/content/10.1101/2024.08.13.607829v1,,,,,,,,,,,,,,,,,,,"To reduce the cost of experimental characterization of the potential substrates for enzymes, machine learning prediction model offer an alternative solution. Pretrained language models, as powerful approaches for protein and molecule representation, have been employed in the development of enzyme-substrate prediction models, achieving promising performance. In addition to continuing improvements in language models, effectively fusing encoders to handle multimodal prediction tasks is critical for further enhancing model performance using available representation methods. Here, we present CLR_ESP, a multimodal classifier that integrates protein and chemistry language models with a newly designed contrastive learning strategy for predicting enzyme-substrate pairs. Our best model achieved SOTA performance with an accuracy of 94.70% on independent test data while requiring fewer computational resources and training data. It also confirmed our hypothesis that embeddings of positive pairs are closer to each other in high-dimension space, while negative pairs exhibit the opposite trend. The proposed architecture is expected to be further applied to enhance performance in additional multimodality prediction tasks in biology. A user-friendly web server of CLR_ESP is established and freely accessible at https://78k6imn5wp.us-east-1.awsapprunner.com/.",,,Unverified,,,,,,,,,,,,,,
SignalP 6.0,Biology,Signal peptide prediction,"Technical University of Denmark,ETH Zurich,University of Copenhagen,Stanford University,Stockholm University,European Bioinformatics Institute","Felix Teufel, José Juan Almagro Armenteros, Alexander Rosenberg Johansen, Magnús Halldór Gíslason, Silas Irby Pihl, Konstantinos D. Tsirigos, Ole Winther, Søren Brunak, Gunnar von Heijne & Henrik Nielsen",2022-01-03,SignalP 6.0 predicts all five types of signal peptides using protein language models,https://www.nature.com/articles/s41587-021-01156-3,,,,,,,,,,,,,,,,,,,"Signal peptides (SPs) are short amino acid sequences that control protein secretion and translocation in all living organisms. SPs can be predicted from sequence data, but existing algorithms are unable to detect all known types of SPs. We introduce SignalP 6.0, a machine learning model that detects all five SP types and is applicable to metagenomic data.",,,Unverified,"Denmark,Switzerland,Denmark,United States of America,Sweden,Multinational",,,,,,"Academia,Academia,Academia,Academia,Academia,Research collective",,,"Academia,Academia,Academia,Academia,Academia,Research collective",,,,
PARM,Biology,"Protein or nucleotide language model (pLM/nLM),Nucleotide generation","Oncode Institute,UMC Utrecht,Netherlands Cancer Institute,University of Groningen,Radboud University Medical Center","Lucía Barbadilla-Martínez, Noud Klaassen, Vinícius H. Franceschini-Santos, Jérémie Breda, Miguel Hernandez-Quiles, Tijs van Lieshout, Carlos G. Urzua Traslaviña, Hatice Yücel, Minh Chau Luong Boi, Celia Hermana-Garcia-Agullo, Sebastian Gregoricchio, Wilbert Zwart, Emile Voest, Lude Franke, Michiel Vermeulen, Jeroen de Ridder, Bas van Steensel",2024-07-15,The regulatory grammar of human promoters uncovered by MPRA-trained deep learning,https://www.biorxiv.org/content/10.1101/2024.07.09.602649v2,,,,,,,,,,,,,,,,,,,"One of the major challenges in genomics is to build computational models that accurately predict genome-wide gene expression from the sequences of regulatory elements. At the heart of gene regulation are promoters, yet their regulatory logic is still incompletely understood. Here, we report PARM, a cell-type specific deep learning model trained on specially designed massively parallel reporter assays that query human promoter sequences. PARM requires ∼1,000 times less computational power than state-of-the-art technology, and reliably predicts autonomous promoter activity throughout the genome from DNA sequence alone, in multiple cell types. PARM can even design purely synthetic strong promoters. We leveraged PARM to systematically identify binding sites of transcription factors (TFs) that are likely to contribute to the activity of each natural human promoter. We uncovered and experimentally confirmed striking positional preferences of TFs that differ between activating and repressive regulatory functions, as well as a complex grammar of motif-motif interactions. For example, many, but not all, TFs act as repressors when their binding motif is located near or just downstream of the transcription start site. Our approach lays the foundation towards a deep understanding of the regulation of human promoters by TFs.",,,Unverified,Netherlands,,,,,,Academia,,,Academia,,,,
FABind,Biology,Protein-ligand contact prediction,"Renmin University of China,Huazhong University of Science and Technology,Microsoft Research AI for Science,University of Science and Technology of China","Qizhi Pei, Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Yingce Xia, Shufang Xie, Tao Qin, Kun He, Tie-Yan Liu, Rui Yan",2024-01-09,FABind: Fast and Accurate Protein-Ligand Binding,https://arxiv.org/abs/2310.06763v5,,,,,,,,,,,,,,,,,,,"Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose FABind, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. FABind incorporates a unique ligand-informed pocket prediction module, which is also leveraged for docking pose estimation. The model further enhances the docking process by incrementally integrating the predicted pocket to optimize protein-ligand binding, reducing discrepancies between training and inference. Through extensive experiments on benchmark datasets, our proposed FABind demonstrates strong advantages in terms of effectiveness and efficiency compared to existing methods. Our code is available at this https URL",,,Unverified,"China,China,United States of America,United Kingdom of Great Britain and Northern Ireland,China,Netherlands,Germany,China",,,,,,"Academia,Academia,Industry,Academia",,,"Academia,Academia,Industry,Academia",,,,
TransFew,Biology,Protein function prediction,University of Missouri,"Frimpong Boadu, Jianlin Cheng",2024-08-17,Improving protein function prediction by learning and integrating representations of protein sequences and function labels,https://academic.oup.com/bioinformaticsadvances/article/4/1/vbae120/7735316,,,,,,,,,,,,,,,,,,,"Motivation
As fewer than 1% of proteins have protein function information determined experimentally, computationally predicting the function of proteins is critical for obtaining functional information for most proteins and has been a major challenge in protein bioinformatics. Despite the significant progress made in protein function prediction by the community in the last decade, the general accuracy of protein function prediction is still not high, particularly for rare function terms associated with few proteins in the protein function annotation database such as the UniProt.

Results
We introduce TransFew, a new transformer model, to learn the representations of both protein sequences and function labels [Gene Ontology (GO) terms] to predict the function of proteins. TransFew leverages a large pre-trained protein language model (ESM2-t48) to learn function-relevant representations of proteins from raw protein sequences and uses a biological natural language model (BioBert) and a graph convolutional neural network-based autoencoder to generate semantic representations of GO terms from their textual definition and hierarchical relationships, which are combined together to predict protein function via the cross-attention. Integrating the protein sequence and label representations not only enhances overall function prediction accuracy, but delivers a robust performance of predicting rare function terms with limited annotations by facilitating annotation transfer between GO terms.

Availability and implementation
https://github.com/BioinfoMachineLearning/TransFew.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage,Biology,Transcriptomic prediction,"Sun Yat-sen University,University of California Irvine,Guangdong Provincial People's Hospital,Guangdong Academy of Medical Sciences","Zhiwei Huang, Songhao Luo, Zihao Wang, Zhenquan Zhang, Benyuan Jiang, Qing Nie, Jiajun Zhang",2024-07-12,Deep learning linking mechanistic models to single-cell transcriptomics data reveals transcriptional bursting in response to DNA damage,https://www.biorxiv.org/content/10.1101/2024.07.10.602845v1,,,,,,,,,,,,,,,,,,,"Cells must adopt flexible regulatory strategies to make decisions regarding their fate, including differentiation, apoptosis, or survival in the face of various external stimuli. One key cellular strategy that enables these functions is stochastic gene expression programs. However, understanding how transcriptional bursting, and consequently, cell fate, responds to DNA damage on a genome-wide scale poses a challenge. In this study, we propose an interpretable and scalable inference framework, DeepTX, that leverages deep learning methods to connect mechanistic models and scRNA-seq data, thereby revealing genome-wide transcriptional burst kinetics. This framework enables rapid and accurate solutions to transcription models and the inference of transcriptional burst kinetics from scRNA-seq data. Applying this framework to several scRNA-seq datasets of DNA-damaging drug treatments, we observed that fluctuations in transcriptional bursting induced by different drugs could lead to distinct fate decisions: IdU treatment induces differentiation in mouse embryonic stem cells by increasing the burst size of gene expression, while 5FU treatment with low and high dose increases the burst frequency of gene expression to induce cell apoptosis and survival in human colon cancer cells. Together, these results show that DeepTX can be used to analyze single-cell transcriptomics data and can provide mechanistic insights into cell fate decisions.",,,Unverified,China,,,,,,Academia,,,Academia,,,,
CryoChains,Biology,Cryo-EM image reconstruction,"University of California Santa Barbara (UCSB),Stanford University","Bongjin Koo, Julien Martel, Ariana Peck, Axel Levy, Frédéric Poitevin, Nina Miolane",2023-07-15,CryoChains: Heterogeneous Reconstruction of Molecular Assembly of Semi-flexible Chains from Cryo-EM Images,https://arxiv.org/abs/2306.07274,,,,,,,,,,,,,,,,,,,"Cryogenic electron microscopy (cryo-EM) has transformed structural biology by allowing to reconstruct 3D biomolecular structures up to near-atomic resolution. However, the 3D reconstruction process remains challenging, as the 3D structures may exhibit substantial shape variations, while the 2D image acquisition suffers from a low signal-to-noise ratio, requiring to acquire very large datasets that are time-consuming to process. Current reconstruction methods are precise but computationally expensive, or faster but lack a physically-plausible model of large molecular shape variations. To fill this gap, we propose CryoChains that encodes large deformations of biomolecules via rigid body transformation of their chains, while representing their finer shape variations with the normal mode analysis framework of biophysics. Our synthetic data experiments on the human GABA\textsubscript{B} and heat shock protein show that CryoChains gives a biophysically-grounded quantification of the heterogeneous conformations of biomolecules, while reconstructing their 3D molecular structures at an improved resolution compared to the current fastest, interpretable deep learning method.",,,Unverified,"United States of America,United States of America",,,,,,"Academia,Academia",,,"Academia,Academia",,,,
CryoDRGN,Biology,Cryo-EM image reconstruction,Massachusetts Institute of Technology (MIT),"Ellen D. Zhong, Tristan Bepler, Bonnie Berger, Joseph H. Davis",2021-02-04,CryoDRGN: reconstruction of heterogeneous cryo-EM structures using neural networks,https://www.nature.com/articles/s41592-020-01049-4,,,,,,,,,,,,,,,,,,,"Cryo-electron microscopy (cryo-EM) single-particle analysis has proven powerful in determining the structures of rigid macromolecules. However, many imaged protein complexes exhibit conformational and compositional heterogeneity that poses a major challenge to existing three-dimensional reconstruction methods. Here, we present cryoDRGN, an algorithm that leverages the representation power of deep neural networks to directly reconstruct continuous distributions of 3D density maps and map per-particle heterogeneity of single-particle cryo-EM datasets. Using cryoDRGN, we uncovered residual heterogeneity in high-resolution datasets of the 80S ribosome and the RAG complex, revealed a new structural state of the assembling 50S ribosome, and visualized large-scale continuous motions of a spliceosome complex. CryoDRGN contains interactive tools to visualize a dataset’s distribution of per-particle variability, generate density maps for exploratory analysis, extract particle subsets for use with other tools and generate trajectories to visualize molecular motions. CryoDRGN is open-source software freely available at http://cryodrgn.csail.mit.edu.",,,Unverified,United States of America,,,,,,Academia,,,Academia,,,,
AMPLIFY,Biology,Protein or nucleotide language model (pLM/nLM),"Chandar Research Lab,Mila - Quebec AI (originally Montreal Institute for Learning Algorithms),Amgen,Polytechnique Montreal,CIFAR AI Research","Quentin Fournier, Robert M. Vernon, Almer van der Sloot, Benjamin Schulz, Sarath Chandar,  Christopher James Langmead",2024-09-23,Protein Language Models: Is Scaling Necessary?,https://www.biorxiv.org/content/10.1101/2024.09.23.614603v1,3.00,,,,,1.1000000000000008e+22,"1. Hardware: A100 GPUs with 3.12×10¹⁴ FLOP/s per GPU (bf16/fp16)

2. Duration: Directly provided - 1,014 GPU days = 8.75×10⁷ seconds

3. Utilization: 40%

4. Calculation:
3.12×10¹⁴ FLOP/s × 8.75×10⁷ s × 0.40 = 1.09×10²² FLOPs",,,200000000001,"Dataset: 391,000,000 sequences
Sequence length: 512 tokens/sequence
Total tokens = 391,000,000 × 512 = 2.00 × 10¹¹ tokens

Steps processed = 1,000,000 × 4,096 = 4,096,000,000 sequences
Number of epochs = 4,096,000,000 / 391,000,000 ≈ 10.47

Final result: 2.00 × 10¹¹ tokens",,,,,,,,"Public protein sequence databases contain samples from the fitness landscape explored by nature. Protein language models (pLMs) pre-trained on these sequences aim to capture this landscape for tasks like property prediction and protein design. Following the same trend as in natural language processing, pLMs have continuously been scaled up. However, the premise that scale leads to better performance assumes that source databases provide accurate representation of the underlying fitness landscape, which is likely false. By developing an efficient codebase, designing a modern architecture, and addressing data quality concerns such as sample bias, we introduce AMPLIFY, a best-in-class pLM that is orders of magnitude less expensive to train and deploy than previous models. Furthermore, to support the scientific community and democratize the training of pLMs, we have open-sourced AMPLIFY’s pre-training codebase, data, and model checkpoints.",,,Unverified,"Canada,United States of America,Canada,Canada",,,,,,"Academia,Industry,Academia,Research collective",,,"Academia,Industry,Academia,Research collective",,,,Hardware
Evo 2 40B,Biology,Protein or nucleotide language model (pLM/nLM),"Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco","Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher Ré, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie",2025-02-19,Genome modeling and design across all domains of life with Evo 2,https://arcinstitute.org/manuscripts/Evo2,,,,40300000000.00,Table 1 lists 40.3B paramters as model size.,2.2500000000000004e+24,"40.3e9 parameters * 9.3e12 training datapoints * 6 = 2.25e24.
Same FLOPS estimate given by authors in Table 1.",OpenGenome 2,,9300000000000,"""We trained two versions of Evo 2: a smaller version at 7B parameters trained on 2.4 trillion tokens and a full version at 40B parameters trained on 9.3 trillion tokens.""",,,,,,,,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",Open weights (unrestricted),,Unverified,"United States of America,United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,"Academia,Industry,Industry,Academia,Academia,Academia",,,"Academia,Industry,Industry,Academia,Academia,Academia",checked,,,
Evo 2 7B,Biology,Protein or nucleotide language model (pLM/nLM),"Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco","Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher Ré, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie",2025-02-19,Genome modeling and design across all domains of life with Evo 2,https://arcinstitute.org/manuscripts/Evo2,,,,7000000000.00,,1.008e+23,7e9 parameters *2.4e12 training datapoints*6=1.008e23,OpenGenome 2,,2400000000000,"""We trained two versions of Evo 2: a smaller version at 7B parameters trained on 2.4 trillion tokens and a full version at 40B parameters trained on 9.3 trillion tokens.""",,,,,,,,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.",Open weights (unrestricted),,Unverified,"United States of America,United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,"Academia,Industry,Industry,Academia,Academia,Academia",,,"Academia,Industry,Industry,Academia,Academia,Academia",checked,,,